{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5b52da",
   "metadata": {},
   "source": [
    "### Enhanced Claude Dynamic Reasoning System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced Claude Dynamic Reasoning System - COMPLETE WITH 5 CRITICAL FIXES\n",
    "=========================================================================\n",
    "FIX 1: Dynamic confidence based on content analysis (not hardcoded 0.85)\n",
    "FIX 2: Answer extraction from CONCLUSION sections \n",
    "FIX 3: Better question classification for edge cases\n",
    "FIX 4: Multi-path generation for COMPLEX/EXPERT analytical questions\n",
    "FIX 5: Proper step extraction with multiple numbering patterns\n",
    "\n",
    "Full validation, regeneration, and normalization preserved.\n",
    "\"\"\"\n",
    "\n",
    "import anthropic\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re\n",
    "\n",
    "\n",
    "API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87b627e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Images/gsm8k_accuracy_colored.png'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "models = [\n",
    "    \"This System\\n(Haiku 3.5 + Multi-Path)\",\n",
    "    \"Claude 3 Opus\",\n",
    "    \"GPT-4 0-shot CoT\",\n",
    "    \"Claude 3 Haiku\",\n",
    "    \"GPT-3.5 Turbo 5-shot\"\n",
    "]\n",
    "accuracy = [94.6, 95.0, 92.0, 88.9, 57.1]\n",
    "\n",
    "# Use tab colors\n",
    "colors = [\"tab:orange\", \"tab:blue\", \"tab:green\", \"tab:red\", \"tab:purple\"]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(11, 4.5))  # More horizontal\n",
    "bars = plt.bar(models, accuracy, color=colors)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Model / System\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"GSM8K Accuracy Comparison\")\n",
    "\n",
    "# Grid on both axes\n",
    "plt.grid(True, axis='both', linestyle='--', alpha=0.6)\n",
    "\n",
    "# Annotate bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height + 0.8,\n",
    "             f\"{height}%\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Legend (color-coded)\n",
    "legend_labels = [\n",
    "    \"This System (Haiku 3.5 + Multi-Path) ★\",\n",
    "    \"Claude 3 Opus\",\n",
    "    \"GPT-4 0-shot CoT\",\n",
    "    \"Claude 3 Haiku (0-shot)\",\n",
    "    \"GPT-3.5 Turbo 5-shot\"\n",
    "]\n",
    "plt.legend(bars, legend_labels, loc=\"lower right\", frameon=True)\n",
    "\n",
    "# Layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "path = \"Images/gsm8k_accuracy_colored.png\"\n",
    "plt.savefig(path, dpi=200)\n",
    "plt.close()\n",
    "\n",
    "path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8159f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENUMS\n",
    "# ============================================================================\n",
    "# These enumerations define structured categories used across the reasoning or\n",
    "# validation framework. Each Enum represents a controlled set of options that\n",
    "# improve type safety, readability, and consistency throughout the codebase.\n",
    "# ============================================================================\n",
    "\n",
    "# Represents different logical components or steps in a reasoning process\n",
    "class LogicalOperation(Enum):\n",
    "    VERDICT = \"verdict\"                 # Final conclusion or decision\n",
    "    PREMISE = \"premise\"                 # Foundational assumption or statement\n",
    "    INFERENCE = \"inference\"             # Derived reasoning step between premise and conclusion\n",
    "    EVIDENCE = \"evidence\"               # Supporting factual or observational data\n",
    "    CONCLUSION = \"conclusion\"           # Logical end result derived from reasoning\n",
    "    COUNTERARGUMENT = \"counterargument\" # Opposing point challenging the main argument\n",
    "\n",
    "\n",
    "# Categorizes the type of question being analyzed or generated\n",
    "class QuestionType(Enum):\n",
    "    BINARY = \"binary\"                   # Yes/No or True/False type question\n",
    "    FACTUAL = \"factual\"                 # Based on objective facts or data\n",
    "    MATHEMATICAL = \"mathematical\"       # Involving arithmetic or numerical logic\n",
    "    ANALYTICAL = \"analytical\"           # Requiring reasoning or problem-solving\n",
    "    HYPOTHETICAL = \"hypothetical\"       # Based on assumptions or imagined scenarios\n",
    "    PROCEDURAL = \"procedural\"           # Related to steps or methods in a process\n",
    "\n",
    "\n",
    "# Defines levels of reasoning or question difficulty\n",
    "class ComplexityLevel(Enum):\n",
    "    SIMPLE = \"simple\"                   # Basic or straightforward\n",
    "    MODERATE = \"moderate\"               # Intermediate complexity\n",
    "    COMPLEX = \"complex\"                 # Involves multiple layers or dependencies\n",
    "    EXPERT = \"expert\"                   # Requires advanced or specialized reasoning\n",
    "\n",
    "\n",
    "# Represents the validation or verification state of an argument or result\n",
    "class ValidationStatus(Enum):\n",
    "    NOT_VALIDATED = \"not_validated\"     # Yet to be checked or verified\n",
    "    VALID = \"valid\"                     # Verified as correct or logically sound\n",
    "    INVALID = \"invalid\"                 # Found incorrect or inconsistent\n",
    "    UNCERTAIN = \"uncertain\"             # Ambiguous or inconclusive validation\n",
    "\n",
    "\n",
    "# Describes different reasoning styles or methodologies used for problem-solving\n",
    "class ReasoningApproach(Enum):\n",
    "    ANALYTICAL = \"analytical\"                   # Logical, step-by-step reasoning\n",
    "    SKEPTICAL = \"skeptical\"                     # Questioning assumptions and claims\n",
    "    EVIDENCE_BASED = \"evidence_based\"           # Grounded in empirical data or observations\n",
    "    ALGEBRAIC = \"algebraic\"                     # Based on algebraic manipulation or logic\n",
    "    NUMERICAL = \"numerical\"                     # Focused on numbers and calculations\n",
    "    GEOMETRIC = \"geometric\"                     # Based on shapes, space, and visual logic\n",
    "    DEDUCTIVE = \"deductive\"                     # From general principles to specific conclusions\n",
    "    INDUCTIVE = \"inductive\"                     # From specific cases to general rules\n",
    "    PROOF_BY_CONTRADICTION = \"proof_by_contradiction\" # Validating by disproving alternatives\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa7257ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATACLASSES\n",
    "# ============================================================================\n",
    "# These dataclasses define structured containers for different reasoning,\n",
    "# validation, and synthesis stages in the logical reasoning pipeline.\n",
    "# Each dataclass encapsulates related attributes, making the code cleaner,\n",
    "# more maintainable, and type-safe.\n",
    "# ============================================================================\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "import re\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Represents a single reasoning or logical step in a reasoning chain.\n",
    "# ----------------------------------------------------------------------------\n",
    "@dataclass\n",
    "class LogicalStep:\n",
    "    \"\"\"A single step within a reasoning process (e.g., a premise, inference, or conclusion).\"\"\"\n",
    "    \n",
    "    id: str                                            # Unique identifier for the step\n",
    "    operation: LogicalOperation                        # The logical operation type (Enum)\n",
    "    content: str                                       # Description or statement of the step\n",
    "    confidence: float = 0.8                            # Confidence score in this step (0–1)\n",
    "    validation_status: ValidationStatus = ValidationStatus.NOT_VALIDATED  # Current validation state\n",
    "    validation_feedback: Optional[str] = None          # Notes or results from validation\n",
    "    is_mathematical: bool = False                      # Whether the step involves math operations\n",
    "    calculation_verified: bool = False                 # Whether math in this step was verified\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Represents an entire reasoning chain, composed of multiple logical steps.\n",
    "# ----------------------------------------------------------------------------\n",
    "@dataclass\n",
    "class ReasoningPath:\n",
    "    \"\"\"A structured reasoning path representing the sequence from query to conclusion.\"\"\"\n",
    "    \n",
    "    path_id: str                                       # Unique path identifier\n",
    "    query: str                                         # The main question or problem\n",
    "    verdict: Optional[str] = None                      # Final yes/no or decision outcome\n",
    "    steps: List[LogicalStep] = field(default_factory=list)  # Ordered list of reasoning steps\n",
    "    conclusion: str = \"\"                               # Summarized logical conclusion\n",
    "    confidence: float = 0.0                            # Overall confidence score in this path\n",
    "    generation_strategy: str = \"base\"                  # Name of strategy/model used for generation\n",
    "    raw_output: str = \"\"                               # Original raw model output (for traceability)\n",
    "    generation_time: float = 0.0                       # Time taken to generate this reasoning path\n",
    "    \n",
    "    # Metadata about question classification and validation\n",
    "    question_type: QuestionType = QuestionType.BINARY  # Type of question being addressed\n",
    "    complexity_level: ComplexityLevel = ComplexityLevel.MODERATE  # Estimated reasoning difficulty\n",
    "    answer: Optional[str] = None                       # Computed or textual answer\n",
    "    validation_passes: int = 0                         # Number of validation cycles applied\n",
    "    regeneration_count: int = 0                        # How many times this path was regenerated\n",
    "    \n",
    "    def to_readable_chain(self) -> str:\n",
    "        \"\"\"\n",
    "        Converts the reasoning path into a human-readable formatted string.\n",
    "        Useful for displaying structured reasoning in logs or reports.\n",
    "        \"\"\"\n",
    "        if not self.steps and not self.verdict and not self.answer:\n",
    "            return f\"Reasoning for: {self.query}\\n[Generation failed]\\n\"\n",
    "            \n",
    "        chain = f\"Strategy: {self.generation_strategy.upper()}\\n\"\n",
    "        chain += f\"Type: {self.question_type.value} | Complexity: {self.complexity_level.value}\\n\"\n",
    "        chain += f\"Query: {self.query}\\n\\n\"\n",
    "        \n",
    "        # Display verdict or answer depending on question type\n",
    "        if self.question_type == QuestionType.BINARY and self.verdict:\n",
    "            chain += f\"VERDICT: {self.verdict}\\n\\n\"\n",
    "        elif self.answer:\n",
    "            chain += f\"ANSWER: {self.answer}\\n\\n\"\n",
    "        \n",
    "        # Append each reasoning step in readable format\n",
    "        if self.steps:\n",
    "            chain += \"Reasoning:\\n\"\n",
    "            for i, step in enumerate(self.steps, 1):\n",
    "                validation_marker = \"\"\n",
    "                if step.validation_status == ValidationStatus.VALID:\n",
    "                    validation_marker = \" ✓\"\n",
    "                elif step.validation_status == ValidationStatus.INVALID:\n",
    "                    validation_marker = \" ✗\"\n",
    "                \n",
    "                chain += f\"{i}. [{step.operation.value.upper()}]{validation_marker} {step.content}\\n\"\n",
    "                \n",
    "                if step.validation_feedback:\n",
    "                    chain += f\"   Validation: {step.validation_feedback}\\n\"\n",
    "        \n",
    "        # Append summary and metadata\n",
    "        chain += f\"\\nConclusion: {self.conclusion}\\n\"\n",
    "        chain += f\"Confidence: {self.confidence:.2f}\\n\"\n",
    "        \n",
    "        if self.validation_passes > 0:\n",
    "            chain += f\"Validation passes: {self.validation_passes}\\n\"\n",
    "        if self.regeneration_count > 0:\n",
    "            chain += f\"Regenerations: {self.regeneration_count}\\n\"\n",
    "        \n",
    "        chain += f\"Generation time: {self.generation_time:.2f}s\\n\"\n",
    "        return chain\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Represents the final synthesized answer derived from multiple reasoning paths.\n",
    "# ----------------------------------------------------------------------------\n",
    "@dataclass\n",
    "class SynthesizedAnswer:\n",
    "    \"\"\"Combines multiple reasoning paths into a unified final answer.\"\"\"\n",
    "    \n",
    "    query: str                                         # The question being answered\n",
    "    definitive_answer: str                             # Final synthesized answer\n",
    "    supporting_reasoning: List[str]                    # Key reasoning chains supporting the answer\n",
    "    conflicting_points: List[str]                      # Points or arguments that disagree\n",
    "    final_confidence: float                            # Overall confidence in this final answer\n",
    "    synthesis_explanation: str                         # Explanation of how synthesis was done\n",
    "    question_type: QuestionType = QuestionType.BINARY  # Question type for context\n",
    "    answer_format: str = \"verdict\"                     # Format of final answer (e.g., verdict, text)\n",
    "\n",
    "class ComplexityLevel(Enum):\n",
    "    SIMPLE = \"simple\"\n",
    "    MODERATE = \"moderate\"\n",
    "    COMPLEX = \"complex\"\n",
    "    EXPERT = \"expert\"\n",
    "\n",
    "class QuestionType(Enum):\n",
    "    MATHEMATICAL = \"mathematical\"\n",
    "    COMMONSENSE = \"commonsense\"\n",
    "    FACTUAL = \"factual\"\n",
    "    BINARY = \"binary\"\n",
    "    ANALYTICAL = \"analytical\"\n",
    "\n",
    "class ReasoningApproach(Enum):\n",
    "    ALGEBRAIC = \"algebraic\"\n",
    "    NUMERICAL = \"numerical\"\n",
    "    ANALYTICAL = \"analytical\"\n",
    "    EVIDENCE_BASED = \"evidence_based\"\n",
    "    SKEPTICAL = \"skeptical\"\n",
    "\n",
    "class QuestionClassification:\n",
    "    def __init__(self, question, question_type, complexity_level, requires_validation,\n",
    "                 requires_math_verification, suggested_approaches, confidence_threshold, num_paths):\n",
    "        self.question = question\n",
    "        self.question_type = question_type\n",
    "        self.complexity_level = complexity_level\n",
    "        self.requires_validation = requires_validation\n",
    "        self.requires_math_verification = requires_math_verification\n",
    "        self.suggested_approaches = suggested_approaches\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.num_paths = num_paths\n",
    "        \n",
    "# ----------------------------------------------------------------------------\n",
    "# Represents the aggregated result of reasoning, validation, and synthesis.\n",
    "# ----------------------------------------------------------------------------\n",
    "@dataclass\n",
    "class NegotiationResult:\n",
    "    \"\"\"Encapsulates results from the reasoning–synthesis pipeline, including timing and costs.\"\"\"\n",
    "    \n",
    "    original_paths: List[ReasoningPath]                # All generated reasoning paths\n",
    "    synthesized_answer: SynthesizedAnswer              # Final combined answer\n",
    "    total_time: float                                  # Total runtime across reasoning and synthesis\n",
    "    parallel_speedup: float                            # Speedup factor achieved via parallelization\n",
    "    total_cost: float                                  # Total compute or API cost\n",
    "    total_validations: int = 0                         # Count of all validation checks performed\n",
    "    total_regenerations: int = 0                       # Count of regeneration cycles\n",
    "    classification: Optional[QuestionClassification] = None  # Optional question classification metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7be2b50d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY INTERFACE\n",
    "# ============================================================================\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class ReasoningStrategy(ABC):\n",
    "    \"\"\"Base class for dataset-specific reasoning strategies\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def classify_question(self, query: str) -> 'QuestionClassification':\n",
    "        \"\"\"Determine question type and complexity\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_generation_prompts(self, query: str, num_paths: int) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Return (strategy_name, instruction) pairs for path generation\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def extract_answer(self, response: str) -> Optional[str]:\n",
    "        \"\"\"Extract answer from model response\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def compare_answers(self, answer1: str, answer2: str) -> bool:\n",
    "        \"\"\"Check if two answers are equivalent\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def select_final_answer(self, answers: List[str], paths: List['ReasoningPath']) -> str:\n",
    "        \"\"\"Choose best answer from multiple paths\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONCRETE STRATEGIES (NEW - ADD AFTER INTERFACE)\n",
    "# ============================================================================\n",
    "\n",
    "class GSM8KStrategy(ReasoningStrategy):\n",
    "    \"\"\"Strategy for GSM8K numerical math problems\"\"\"\n",
    "    \n",
    "    def classify_question(self, query: str) -> 'QuestionClassification':\n",
    "        \"\"\"Simple classification: numbers + quantities = math\"\"\"\n",
    "        # Use existing detection logic\n",
    "        has_numbers = bool(re.search(r'\\d+', query))\n",
    "        math_patterns = [\n",
    "            r'\\d+\\s+(eggs?|apples?|dollars?|hours?|per|each|total)',\n",
    "            r'(how many|how much)\\s+',\n",
    "        ]\n",
    "        is_math = has_numbers and any(re.search(p, query.lower()) for p in math_patterns)\n",
    "        \n",
    "        return QuestionClassification(\n",
    "            question=query,\n",
    "            question_type=QuestionType.MATHEMATICAL if is_math else QuestionType.ANALYTICAL,\n",
    "            complexity_level=ComplexityLevel.MODERATE,\n",
    "            requires_validation=True,\n",
    "            requires_math_verification=is_math,\n",
    "            suggested_approaches=[ReasoningApproach.ALGEBRAIC, ReasoningApproach.NUMERICAL],\n",
    "            confidence_threshold=0.70,\n",
    "            num_paths=2\n",
    "        )\n",
    "    \n",
    "    def get_generation_prompts(self, query: str, num_paths: int) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Return algebraic + numerical prompts\"\"\"\n",
    "        prompts = [\n",
    "            (\"algebraic\", \n",
    "             \"\"\"Solve using algebraic methods. \n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "1. Show EVERY step of your calculation explicitly\n",
    "2. Write out ALL arithmetic operations\n",
    "3. Label your steps clearly\n",
    "4. MUST end with: ANSWER: [number]\"\"\"),\n",
    "            \n",
    "            (\"numerical\", \n",
    "             \"\"\"Solve using numerical calculations.\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "1. Convert word problem to numbers immediately\n",
    "2. Show ALL arithmetic: 5 + 3 = 8, then 8 × 2 = 16\n",
    "3. Label each calculation\n",
    "4. MUST end with: ANSWER: [number]\"\"\")\n",
    "        ]\n",
    "        return prompts[:num_paths]\n",
    "    \n",
    "    def extract_answer(self, response: str) -> Optional[str]:\n",
    "        \"\"\"Extract numerical answer\"\"\"\n",
    "        patterns = [\n",
    "            r'ANSWER\\s*[:\\=]\\s*\\$?(\\d+(?:,\\d{3})*(?:\\.\\d+)?)',\n",
    "            r'\\\\boxed\\{(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\}',\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, response, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1).replace(',', '')\n",
    "        \n",
    "        # Fallback: last = statement\n",
    "        lines = response.split('\\n')\n",
    "        for line in reversed(lines[-5:]):\n",
    "            calc_match = re.search(r'=\\s*\\$?(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*$', line)\n",
    "            if calc_match:\n",
    "                return calc_match.group(1).replace(',', '')\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def compare_answers(self, answer1: str, answer2: str) -> bool:\n",
    "        \"\"\"Compare with 1% tolerance\"\"\"\n",
    "        try:\n",
    "            num1 = float(re.sub(r'[^\\d.]', '', answer1))\n",
    "            num2 = float(re.sub(r'[^\\d.]', '', answer2))\n",
    "            if num2 == 0:\n",
    "                return abs(num1) < 0.01\n",
    "            return abs(num1 - num2) / abs(num2) < 0.01\n",
    "        except:\n",
    "            return answer1.strip() == answer2.strip()\n",
    "    \n",
    "    def select_final_answer(self, answers: List[str], paths: List['ReasoningPath']) -> str:\n",
    "        \"\"\"Majority vote with confidence weighting\"\"\"\n",
    "        if not answers:\n",
    "            return \"Unable to determine\"\n",
    "        \n",
    "        from collections import Counter\n",
    "        vote_scores = {}\n",
    "        \n",
    "        for i, answer in enumerate(answers):\n",
    "            confidence = paths[i].confidence if i < len(paths) else 0.5\n",
    "            if answer not in vote_scores:\n",
    "                vote_scores[answer] = 0\n",
    "            vote_scores[answer] += confidence\n",
    "        \n",
    "        best = max(vote_scores.items(), key=lambda x: x[1])\n",
    "        count = sum(1 for a in answers if self.compare_answers(a, best[0]))\n",
    "        \n",
    "        if count == len(answers):\n",
    "            return f\"{best[0]} (unanimous)\"\n",
    "        elif count > len(answers) / 2:\n",
    "            return f\"{best[0]} (consensus {count}/{len(answers)})\"\n",
    "        return f\"{best[0]}\"\n",
    "\n",
    "\n",
    "class CommonsenseQAStrategy(ReasoningStrategy):\n",
    "    \"\"\"Strategy for CommonsenseQA multiple choice\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.specificity_scorer = SpecificityScorer()  # Reuse existing\n",
    "    \n",
    "    def classify_question(self, query: str) -> 'QuestionClassification':\n",
    "        \"\"\"Detect commonsense cues\"\"\"\n",
    "        commonsense_cues = [\n",
    "            'typically', 'usually', 'commonly', 'often', 'likely',\n",
    "            'where would you', 'what do people usually'\n",
    "        ]\n",
    "        is_commonsense = any(cue in query.lower() for cue in commonsense_cues)\n",
    "        \n",
    "        return QuestionClassification(\n",
    "            question=query,\n",
    "            question_type=QuestionType.COMMONSENSE if is_commonsense else QuestionType.ANALYTICAL,\n",
    "            complexity_level=ComplexityLevel.MODERATE,\n",
    "            requires_validation=False,\n",
    "            requires_math_verification=False,\n",
    "            suggested_approaches=[\n",
    "                ReasoningApproach.ANALYTICAL,\n",
    "                ReasoningApproach.EVIDENCE_BASED,\n",
    "                ReasoningApproach.SKEPTICAL\n",
    "            ],\n",
    "            confidence_threshold=0.65,\n",
    "            num_paths=3\n",
    "        )\n",
    "    \n",
    "    def get_generation_prompts(self, query: str, num_paths: int) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Return 3 commonsense prompts\"\"\"\n",
    "        prompts = [\n",
    "            (\"analytical\", \n",
    "             \"\"\"Use everyday common sense and practical reasoning.\n",
    "\n",
    "IMPORTANT: Choose the MOST SPECIFIC, CONCRETE answer.\n",
    "- Prefer specific actions over generic categories\n",
    "- Prefer specific locations over general places\n",
    "- Think: What is the MOST DIRECT answer?\"\"\"),\n",
    "            \n",
    "            (\"evidence_based\", \n",
    "             \"\"\"Use practical knowledge and real-world experience.\n",
    "\n",
    "IMPORTANT: Focus on SPECIFICITY.\n",
    "- What is the most concrete, tangible answer?\n",
    "- Choose the answer that directly names the thing/action\"\"\"),\n",
    "            \n",
    "            (\"skeptical\", \n",
    "             \"\"\"Think critically about each option.\n",
    "\n",
    "IMPORTANT: Eliminate based on specificity.\n",
    "- Remove vague, generic, or overly broad choices\n",
    "- Be decisive - choose the clearest, most concrete option\"\"\")\n",
    "        ]\n",
    "        return prompts[:num_paths]\n",
    "    \n",
    "    def extract_answer(self, response: str) -> Optional[str]:\n",
    "        \"\"\"Extract letter choice\"\"\"\n",
    "        patterns = [\n",
    "            r'ANSWER:\\s*([A-E]):\\s*(.+?)(?:\\n\\n|CONCLUSION|$)',\n",
    "            r'CONCLUSION:\\s*([A-E]):\\s*(.+?)(?:\\n\\n|$)',\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, response, re.IGNORECASE | re.DOTALL)\n",
    "            if match:\n",
    "                letter = match.group(1).upper()\n",
    "                text = match.group(2).strip()\n",
    "                text = re.sub(r'\\*\\*|__|`', '', text)\n",
    "                return f\"{letter}: {text}\"\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def compare_answers(self, answer1: str, answer2: str) -> bool:\n",
    "        \"\"\"Compare letter choices\"\"\"\n",
    "        letter1 = re.match(r'^([A-E])', answer1)\n",
    "        letter2 = re.match(r'^([A-E])', answer2)\n",
    "        if letter1 and letter2:\n",
    "            return letter1.group(1) == letter2.group(1)\n",
    "        return False\n",
    "    \n",
    "    def select_final_answer(self, answers: List[str], paths: List['ReasoningPath']) -> str:\n",
    "        \"\"\"Use specificity scoring\"\"\"\n",
    "        if not answers:\n",
    "            return \"Unable to determine\"\n",
    "        \n",
    "        best_answer = None\n",
    "        best_score = -1\n",
    "        \n",
    "        for i, answer in enumerate(answers):\n",
    "            if i >= len(paths):\n",
    "                continue\n",
    "            \n",
    "            conf = paths[i].confidence\n",
    "            spec = self.specificity_scorer.score_specificity(answer, paths[i].query)\n",
    "            combined = 0.7 * conf + 0.3 * spec\n",
    "            \n",
    "            if combined > best_score:\n",
    "                best_score = combined\n",
    "                best_answer = answer\n",
    "        \n",
    "        return best_answer or answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0447c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIDENCE ASSESSOR - DYNAMIC CONFIDENCE\n",
    "# ============================================================================\n",
    "\n",
    "class ConfidenceAssessor:\n",
    "    \"\"\"Dynamically assess confidence based on actual step content\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def assess_step_confidence(content: str, is_mathematical: bool, \n",
    "                              question_type: QuestionType) -> float:\n",
    "        \"\"\"Calculate confidence based on actual step content, not hardcoded\"\"\"\n",
    "        base_confidence = 0.75\n",
    "        \n",
    "        # Uncertainty markers reduce confidence\n",
    "        uncertainty_words = ['might', 'possibly', 'approximately', 'roughly',\n",
    "                            'likely', 'probably', 'seems', 'appears', 'assume']\n",
    "        uncertainty_count = sum(1 for word in uncertainty_words if word in content.lower())\n",
    "        base_confidence -= uncertainty_count * 0.05\n",
    "        \n",
    "        # Complex math operations reduce confidence (needs verification)\n",
    "        if is_mathematical:\n",
    "            operations = content.lower().count('+') + content.lower().count('-') + \\\n",
    "                        content.lower().count('*') + content.lower().count('/')\n",
    "            if operations > 3:\n",
    "                base_confidence -= 0.1\n",
    "        \n",
    "        # Assumption-based reasoning reduces confidence\n",
    "        if any(phrase in content.lower() for phrase in ['assume', 'suppose', 'if we']):\n",
    "            base_confidence -= 0.08\n",
    "        \n",
    "        # Expert-level concepts might reduce confidence\n",
    "        expert_terms = ['paradox', 'infinity', 'irrational', 'undefined', 'diverges']\n",
    "        if any(term in content.lower() for term in expert_terms):\n",
    "            base_confidence -= 0.05\n",
    "        \n",
    "        # Definitive statements increase confidence\n",
    "        if any(word in content.lower() for word in ['therefore', 'thus', 'must', 'always']):\n",
    "            base_confidence += 0.05\n",
    "        \n",
    "        return max(min(base_confidence, 0.95), 0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aab9cd7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MATHEMATICAL CALCULATION VERIFIER - COMPETITION GRADE (ENHANCED)\n",
    "# ============================================================================\n",
    "\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from fractions import Fraction\n",
    "import math\n",
    "\n",
    "class MathematicalVerifier:\n",
    "    \"\"\"\n",
    "    Comprehensive mathematical verification system for competition-level problems.\n",
    "    \n",
    "    Covers:\n",
    "    - Basic arithmetic (GSM8K level)\n",
    "    - Algebra (equations, factoring, quadratics)\n",
    "    - Geometry (area, volume, angles, Pythagorean theorem)\n",
    "    - Number theory (primes, divisors, GCD/LCM, modular arithmetic)\n",
    "    - Combinatorics (permutations, combinations, probability)\n",
    "    - Calculus (derivatives, integrals - basic patterns)\n",
    "    - Unit conversions (time, distance, money)\n",
    "    - Harmonic means, work rates, mixture problems\n",
    "    \n",
    "    CRITICAL: Only applies to MATHEMATICAL questions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PATTERN EXTRACTION\n",
    "    # ========================================================================\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_calculations(text: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract ALL types of calculations from reasoning text.\n",
    "        Returns list of calculation dictionaries with type and values.\n",
    "        \"\"\"\n",
    "        calculations = []\n",
    "        \n",
    "        # Clean text - remove LaTeX formatting for easier parsing\n",
    "        clean_text = text.replace('\\\\', '').replace('$', '')\n",
    "        \n",
    "        # Pattern 1: Basic arithmetic (5 + 3 = 8, 4 × 3 = 12)\n",
    "        basic_pattern = r'(\\d+(?:\\.\\d+)?)\\s*([+\\-×*/÷])\\s*(\\d+(?:\\.\\d+)?)\\s*=\\s*(\\d+(?:\\.\\d+)?)'\n",
    "        for match in re.finditer(basic_pattern, text):\n",
    "            num1, op, num2, result = match.groups()\n",
    "            calculations.append({\n",
    "                'type': 'basic',\n",
    "                'operand1': float(num1),\n",
    "                'operator': op.replace('×', '*').replace('÷', '/'),\n",
    "                'operand2': float(num2),\n",
    "                'claimed_result': float(result),\n",
    "                'text': match.group(0),\n",
    "                'position': match.start()\n",
    "            })\n",
    "        \n",
    "        # Pattern 2: Percentages (20% of 50 = 10)\n",
    "        percent_pattern = r'(\\d+(?:\\.\\d+)?)%\\s+of\\s+(\\d+(?:\\.\\d+)?)\\s*=\\s*(\\d+(?:\\.\\d+)?)'\n",
    "        for match in re.finditer(percent_pattern, text):\n",
    "            percent, base, result = match.groups()\n",
    "            calculations.append({\n",
    "                'type': 'percent',\n",
    "                'percent': float(percent),\n",
    "                'base': float(base),\n",
    "                'claimed_result': float(result),\n",
    "                'text': match.group(0),\n",
    "                'position': match.start()\n",
    "            })\n",
    "        \n",
    "        # Pattern 3: Fractions (2/3 of 15 = 10, 3/4 × 8 = 6)\n",
    "        fraction_pattern = r'(\\d+)/(\\d+)\\s+(?:of|×|\\*)\\s+(\\d+(?:\\.\\d+)?)\\s*=\\s*(\\d+(?:\\.\\d+)?)'\n",
    "        for match in re.finditer(fraction_pattern, text):\n",
    "            num, denom, base, result = match.groups()\n",
    "            calculations.append({\n",
    "                'type': 'fraction',\n",
    "                'numerator': int(num),\n",
    "                'denominator': int(denom),\n",
    "                'base': float(base),\n",
    "                'claimed_result': float(result),\n",
    "                'text': match.group(0),\n",
    "                'position': match.start()\n",
    "            })\n",
    "        \n",
    "        # Pattern 4: Exponents (2^3 = 8, 5^2 = 25)\n",
    "        exponent_pattern = r'(\\d+(?:\\.\\d+)?)\\s*\\^\\s*(\\d+(?:\\.\\d+)?)\\s*=\\s*(\\d+(?:\\.\\d+)?)'\n",
    "        for match in re.finditer(exponent_pattern, clean_text):\n",
    "            base, exp, result = match.groups()\n",
    "            calculations.append({\n",
    "                'type': 'exponent',\n",
    "                'base': float(base),\n",
    "                'exponent': float(exp),\n",
    "                'claimed_result': float(result),\n",
    "                'text': match.group(0),\n",
    "                'position': match.start()\n",
    "            })\n",
    "        \n",
    "        # Pattern 5: Square roots (√16 = 4, sqrt(25) = 5)\n",
    "        sqrt_pattern = r'(?:√|sqrt\\()\\s*(\\d+(?:\\.\\d+)?)\\s*\\)?\\s*=\\s*(\\d+(?:\\.\\d+)?)'\n",
    "        for match in re.finditer(sqrt_pattern, clean_text):\n",
    "            value, result = match.groups()\n",
    "            calculations.append({\n",
    "                'type': 'sqrt',\n",
    "                'value': float(value),\n",
    "                'claimed_result': float(result),\n",
    "                'text': match.group(0),\n",
    "                'position': match.start()\n",
    "            })\n",
    "        \n",
    "        # Pattern 6: Pythagorean theorem (a² + b² = c²)\n",
    "        pythag_pattern = r'(\\d+(?:\\.\\d+)?)\\s*\\^2\\s*\\+\\s*(\\d+(?:\\.\\d+)?)\\s*\\^2\\s*=\\s*(\\d+(?:\\.\\d+)?)\\s*\\^?2?'\n",
    "        for match in re.finditer(pythag_pattern, clean_text):\n",
    "            a, b, c = match.groups()\n",
    "            calculations.append({\n",
    "                'type': 'pythagorean',\n",
    "                'a': float(a),\n",
    "                'b': float(b),\n",
    "                'claimed_c': float(c),\n",
    "                'text': match.group(0),\n",
    "                'position': match.start()\n",
    "            })\n",
    "        \n",
    "        # Pattern 7: Quadratic formula components\n",
    "        discriminant_pattern = r'b\\^2\\s*-\\s*4ac\\s*=\\s*(\\d+(?:\\.\\d+)?)'\n",
    "        for match in re.finditer(discriminant_pattern, clean_text):\n",
    "            result = match.group(1)\n",
    "            calculations.append({\n",
    "                'type': 'discriminant',\n",
    "                'claimed_result': float(result),\n",
    "                'text': match.group(0),\n",
    "                'position': match.start()\n",
    "            })\n",
    "        \n",
    "        # Pattern 8: Combinatorics (C(n,k) = result, P(n,k) = result)\n",
    "        comb_pattern = r'C\\((\\d+),\\s*(\\d+)\\)\\s*=\\s*(\\d+)'\n",
    "        for match in re.finditer(comb_pattern, text):\n",
    "            n, k, result = match.groups()\n",
    "            calculations.append({\n",
    "                'type': 'combination',\n",
    "                'n': int(n),\n",
    "                'k': int(k),\n",
    "                'claimed_result': int(result),\n",
    "                'text': match.group(0),\n",
    "                'position': match.start()\n",
    "            })\n",
    "        \n",
    "        perm_pattern = r'P\\((\\d+),\\s*(\\d+)\\)\\s*=\\s*(\\d+)'\n",
    "        for match in re.finditer(perm_pattern, text):\n",
    "            n, k, result = match.groups()\n",
    "            calculations.append({\n",
    "                'type': 'permutation',\n",
    "                'n': int(n),\n",
    "                'k': int(k),\n",
    "                'claimed_result': int(result),\n",
    "                'text': match.group(0),\n",
    "                'position': match.start()\n",
    "            })\n",
    "        \n",
    "        # Pattern 9: Factorials (5! = 120)\n",
    "        factorial_pattern = r'(\\d+)!\\s*=\\s*(\\d+)'\n",
    "        for match in re.finditer(factorial_pattern, text):\n",
    "            n, result = match.groups()\n",
    "            calculations.append({\n",
    "                'type': 'factorial',\n",
    "                'n': int(n),\n",
    "                'claimed_result': int(result),\n",
    "                'text': match.group(0),\n",
    "                'position': match.start()\n",
    "            })\n",
    "        \n",
    "        # Pattern 10: GCD/LCM (gcd(12, 18) = 6)\n",
    "        gcd_pattern = r'gcd\\((\\d+),\\s*(\\d+)\\)\\s*=\\s*(\\d+)'\n",
    "        for match in re.finditer(gcd_pattern, clean_text.lower()):\n",
    "            a, b, result = match.groups()\n",
    "            calculations.append({\n",
    "                'type': 'gcd',\n",
    "                'a': int(a),\n",
    "                'b': int(b),\n",
    "                'claimed_result': int(result),\n",
    "                'text': match.group(0),\n",
    "                'position': match.start()\n",
    "            })\n",
    "        \n",
    "        lcm_pattern = r'lcm\\((\\d+),\\s*(\\d+)\\)\\s*=\\s*(\\d+)'\n",
    "        for match in re.finditer(lcm_pattern, clean_text.lower()):\n",
    "            a, b, result = match.groups()\n",
    "            calculations.append({\n",
    "                'type': 'lcm',\n",
    "                'a': int(a),\n",
    "                'b': int(b),\n",
    "                'claimed_result': int(result),\n",
    "                'text': match.group(0),\n",
    "                'position': match.start()\n",
    "            })\n",
    "        \n",
    "        # Pattern 11: Area formulas (Area = πr² = result)\n",
    "        circle_area_pattern = r'(?:π|pi)\\s*\\*?\\s*(\\d+(?:\\.\\d+)?)\\s*\\^2\\s*=\\s*(\\d+(?:\\.\\d+)?)'\n",
    "        for match in re.finditer(circle_area_pattern, clean_text.lower()):\n",
    "            r, result = match.groups()\n",
    "            calculations.append({\n",
    "                'type': 'circle_area',\n",
    "                'radius': float(r),\n",
    "                'claimed_result': float(result),\n",
    "                'text': match.group(0),\n",
    "                'position': match.start()\n",
    "            })\n",
    "        \n",
    "        # Pattern 12: Distance formula (distance = √((x2-x1)² + (y2-y1)²))\n",
    "        distance_pattern = r'√\\(\\((\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)\\)\\^2\\s*\\+\\s*\\((\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)\\)\\^2\\)\\s*=\\s*(\\d+(?:\\.\\d+)?)'\n",
    "        for match in re.finditer(distance_pattern, clean_text):\n",
    "            x2, x1, y2, y1, result = match.groups()\n",
    "            calculations.append({\n",
    "                'type': 'distance',\n",
    "                'x1': float(x1),\n",
    "                'y1': float(y1),\n",
    "                'x2': float(x2),\n",
    "                'y2': float(y2),\n",
    "                'claimed_result': float(result),\n",
    "                'text': match.group(0),\n",
    "                'position': match.start()\n",
    "            })\n",
    "        \n",
    "        # Pattern 13: Harmonic mean (2/(1/a + 1/b) = result)\n",
    "        harmonic_pattern = r'2\\s*/\\s*\\(1/(\\d+(?:\\.\\d+)?)\\s*\\+\\s*1/(\\d+(?:\\.\\d+)?)\\)\\s*=\\s*(\\d+(?:\\.\\d+)?)'\n",
    "        for match in re.finditer(harmonic_pattern, clean_text):\n",
    "            a, b, result = match.groups()\n",
    "            calculations.append({\n",
    "                'type': 'harmonic_mean',\n",
    "                'a': float(a),\n",
    "                'b': float(b),\n",
    "                'claimed_result': float(result),\n",
    "                'text': match.group(0),\n",
    "                'position': match.start()\n",
    "            })\n",
    "        \n",
    "        # Pattern 14: Work rate (1/a + 1/b = 1/result)\n",
    "        work_rate_pattern = r'1/(\\d+(?:\\.\\d+)?)\\s*\\+\\s*1/(\\d+(?:\\.\\d+)?)\\s*=\\s*1/(\\d+(?:\\.\\d+)?)'\n",
    "        for match in re.finditer(work_rate_pattern, clean_text):\n",
    "            a, b, result = match.groups()\n",
    "            calculations.append({\n",
    "                'type': 'work_rate',\n",
    "                'time_a': float(a),\n",
    "                'time_b': float(b),\n",
    "                'claimed_combined_time': float(result),\n",
    "                'text': match.group(0),\n",
    "                'position': match.start()\n",
    "            })\n",
    "        \n",
    "        # Pattern 15: Mixture problems (concentration × volume = amount)\n",
    "        mixture_pattern = r'(\\d+(?:\\.\\d+)?)%?\\s*×\\s*(\\d+(?:\\.\\d+)?)\\s*=\\s*(\\d+(?:\\.\\d+)?)'\n",
    "        for match in re.finditer(mixture_pattern, text):\n",
    "            conc, vol, amt = match.groups()\n",
    "            calculations.append({\n",
    "                'type': 'mixture',\n",
    "                'concentration': float(conc),\n",
    "                'volume': float(vol),\n",
    "                'claimed_amount': float(amt),\n",
    "                'text': match.group(0),\n",
    "                'position': match.start()\n",
    "            })\n",
    "        \n",
    "        # Pattern 16: Rate × Time = Distance\n",
    "        rate_time_pattern = r'(\\d+(?:\\.\\d+)?)\\s*(?:mph|km/h|m/s)?\\s*×\\s*(\\d+(?:\\.\\d+)?)\\s*(?:hours?|hrs?|h)?\\s*=\\s*(\\d+(?:\\.\\d+)?)'\n",
    "        for match in re.finditer(rate_time_pattern, clean_text.lower()):\n",
    "            rate, time, distance = match.groups()\n",
    "            calculations.append({\n",
    "                'type': 'rate_time_distance',\n",
    "                'rate': float(rate),\n",
    "                'time': float(time),\n",
    "                'claimed_distance': float(distance),\n",
    "                'text': match.group(0),\n",
    "                'position': match.start()\n",
    "            })\n",
    "        \n",
    "        # Pattern 17: Modular arithmetic (a mod b = result)\n",
    "        mod_pattern = r'(\\d+)\\s+mod\\s+(\\d+)\\s*=\\s*(\\d+)'\n",
    "        for match in re.finditer(mod_pattern, clean_text.lower()):\n",
    "            a, b, result = match.groups()\n",
    "            calculations.append({\n",
    "                'type': 'modular',\n",
    "                'value': int(a),\n",
    "                'modulus': int(b),\n",
    "                'claimed_result': int(result),\n",
    "                'text': match.group(0),\n",
    "                'position': match.start()\n",
    "            })\n",
    "        \n",
    "        # Pattern 18: Probability (P(event) = favorable/total = result)\n",
    "        prob_pattern = r'(\\d+)/(\\d+)\\s*=\\s*(\\d+(?:\\.\\d+)?)'\n",
    "        for match in re.finditer(prob_pattern, text):\n",
    "            fav, total, prob = match.groups()\n",
    "            # Only treat as probability if result is < 1\n",
    "            if float(prob) <= 1.0:\n",
    "                calculations.append({\n",
    "                    'type': 'probability',\n",
    "                    'favorable': int(fav),\n",
    "                    'total': int(total),\n",
    "                    'claimed_probability': float(prob),\n",
    "                    'text': match.group(0),\n",
    "                    'position': match.start()\n",
    "                })\n",
    "        \n",
    "        # Pattern 19: Logs (log_b(x) = y means b^y = x)\n",
    "        log_pattern = r'log_(\\d+)\\((\\d+)\\)\\s*=\\s*(\\d+(?:\\.\\d+)?)'\n",
    "        for match in re.finditer(log_pattern, clean_text):\n",
    "            base, value, result = match.groups()\n",
    "            calculations.append({\n",
    "                'type': 'logarithm',\n",
    "                'base': float(base),\n",
    "                'value': float(value),\n",
    "                'claimed_result': float(result),\n",
    "                'text': match.group(0),\n",
    "                'position': match.start()\n",
    "            })\n",
    "        \n",
    "        # Pattern 20: Prime factorization (60 = 2² × 3 × 5)\n",
    "        prime_factor_pattern = r'(\\d+)\\s*=.*?(\\d+)\\^(\\d+)'\n",
    "        # This is complex, we'll skip deep verification but extract it\n",
    "        \n",
    "        # Sort by position to verify in order\n",
    "        calculations.sort(key=lambda x: x.get('position', 0))\n",
    "        \n",
    "        return calculations\n",
    "    \n",
    "    # ========================================================================\n",
    "    # VERIFICATION FUNCTIONS\n",
    "    # ========================================================================\n",
    "    \n",
    "    @staticmethod\n",
    "    def verify_calculation(calc: Dict) -> Tuple[bool, str, Optional[float]]:\n",
    "        \"\"\"\n",
    "        Verify a single calculation using reverse engineering.\n",
    "        \n",
    "        Returns:\n",
    "            (is_correct, feedback, correct_answer)\n",
    "        \"\"\"\n",
    "        TOLERANCE = 0.01  # 1% tolerance for rounding\n",
    "        \n",
    "        try:\n",
    "            calc_type = calc['type']\n",
    "            \n",
    "            # ================================================================\n",
    "            # BASIC ARITHMETIC\n",
    "            # ================================================================\n",
    "            if calc_type == 'basic':\n",
    "                return MathematicalVerifier._verify_basic_arithmetic(calc, TOLERANCE)\n",
    "            \n",
    "            # ================================================================\n",
    "            # PERCENTAGES\n",
    "            # ================================================================\n",
    "            elif calc_type == 'percent':\n",
    "                correct = (calc['percent'] / 100) * calc['base']\n",
    "                claimed = calc['claimed_result']\n",
    "                \n",
    "                error = abs(correct - claimed) / max(abs(correct), 0.001)\n",
    "                if error < TOLERANCE:\n",
    "                    return (True, \"✓ Verified\", correct)\n",
    "                else:\n",
    "                    actual_percent = (claimed / calc['base']) * 100 if calc['base'] != 0 else 0\n",
    "                    feedback = f\"✗ {calc['percent']}% of {calc['base']} = {correct:.2f}, not {claimed}. (Reverse: {claimed} is {actual_percent:.1f}% of {calc['base']})\"\n",
    "                    return (False, feedback, correct)\n",
    "            \n",
    "            # ================================================================\n",
    "            # FRACTIONS\n",
    "            # ================================================================\n",
    "            elif calc_type == 'fraction':\n",
    "                fraction_value = calc['numerator'] / calc['denominator']\n",
    "                correct = fraction_value * calc['base']\n",
    "                claimed = calc['claimed_result']\n",
    "                \n",
    "                error = abs(correct - claimed) / max(abs(correct), 0.001)\n",
    "                if error < TOLERANCE:\n",
    "                    return (True, \"✓ Verified\", correct)\n",
    "                else:\n",
    "                    feedback = f\"✗ {calc['numerator']}/{calc['denominator']} × {calc['base']} = {correct:.2f}, not {claimed}\"\n",
    "                    return (False, feedback, correct)\n",
    "            \n",
    "            # ================================================================\n",
    "            # EXPONENTS\n",
    "            # ================================================================\n",
    "            elif calc_type == 'exponent':\n",
    "                correct = calc['base'] ** calc['exponent']\n",
    "                claimed = calc['claimed_result']\n",
    "                \n",
    "                error = abs(correct - claimed) / max(abs(correct), 0.001)\n",
    "                if error < TOLERANCE:\n",
    "                    return (True, \"✓ Verified\", correct)\n",
    "                else:\n",
    "                    # Reverse engineer: what exponent gives claimed result?\n",
    "                    if calc['base'] > 0 and claimed > 0:\n",
    "                        reverse_exp = math.log(claimed) / math.log(calc['base'])\n",
    "                        feedback = f\"✗ {calc['base']}^{calc['exponent']} = {correct:.2f}, not {claimed}. (Reverse: {calc['base']}^{reverse_exp:.2f} = {claimed})\"\n",
    "                    else:\n",
    "                        feedback = f\"✗ {calc['base']}^{calc['exponent']} = {correct:.2f}, not {claimed}\"\n",
    "                    return (False, feedback, correct)\n",
    "            \n",
    "            # ================================================================\n",
    "            # SQUARE ROOTS\n",
    "            # ================================================================\n",
    "            elif calc_type == 'sqrt':\n",
    "                correct = math.sqrt(calc['value'])\n",
    "                claimed = calc['claimed_result']\n",
    "                \n",
    "                error = abs(correct - claimed) / max(abs(correct), 0.001)\n",
    "                if error < TOLERANCE:\n",
    "                    return (True, \"✓ Verified\", correct)\n",
    "                else:\n",
    "                    reverse_check = claimed ** 2\n",
    "                    feedback = f\"✗ √{calc['value']} = {correct:.2f}, not {claimed}. (Reverse: {claimed}² = {reverse_check:.2f})\"\n",
    "                    return (False, feedback, correct)\n",
    "            \n",
    "            # ================================================================\n",
    "            # PYTHAGOREAN THEOREM\n",
    "            # ================================================================\n",
    "            elif calc_type == 'pythagorean':\n",
    "                correct_c = math.sqrt(calc['a']**2 + calc['b']**2)\n",
    "                claimed_c = calc['claimed_c']\n",
    "                \n",
    "                error = abs(correct_c - claimed_c) / max(abs(correct_c), 0.001)\n",
    "                if error < TOLERANCE:\n",
    "                    return (True, \"✓ Verified\", correct_c)\n",
    "                else:\n",
    "                    feedback = f\"✗ √({calc['a']}² + {calc['b']}²) = {correct_c:.2f}, not {claimed_c}. Pythagorean theorem error!\"\n",
    "                    return (False, feedback, correct_c)\n",
    "            \n",
    "            # ================================================================\n",
    "            # COMBINATORICS - COMBINATIONS\n",
    "            # ================================================================\n",
    "            elif calc_type == 'combination':\n",
    "                n, k = calc['n'], calc['k']\n",
    "                if k > n or k < 0:\n",
    "                    return (False, f\"✗ Invalid combination: C({n},{k}) - k cannot be > n or negative\", 0)\n",
    "                \n",
    "                correct = math.comb(n, k)\n",
    "                claimed = calc['claimed_result']\n",
    "                \n",
    "                if correct == claimed:\n",
    "                    return (True, \"✓ Verified\", correct)\n",
    "                else:\n",
    "                    feedback = f\"✗ C({n},{k}) = {correct}, not {claimed}. Formula: n!/(k!(n-k)!)\"\n",
    "                    return (False, feedback, correct)\n",
    "            \n",
    "            # ================================================================\n",
    "            # COMBINATORICS - PERMUTATIONS\n",
    "            # ================================================================\n",
    "            elif calc_type == 'permutation':\n",
    "                n, k = calc['n'], calc['k']\n",
    "                if k > n or k < 0:\n",
    "                    return (False, f\"✗ Invalid permutation: P({n},{k}) - k cannot be > n or negative\", 0)\n",
    "                \n",
    "                correct = math.perm(n, k)\n",
    "                claimed = calc['claimed_result']\n",
    "                \n",
    "                if correct == claimed:\n",
    "                    return (True, \"✓ Verified\", correct)\n",
    "                else:\n",
    "                    feedback = f\"✗ P({n},{k}) = {correct}, not {claimed}. Formula: n!/(n-k)!\"\n",
    "                    return (False, feedback, correct)\n",
    "            \n",
    "            # ================================================================\n",
    "            # FACTORIALS\n",
    "            # ================================================================\n",
    "            elif calc_type == 'factorial':\n",
    "                n = calc['n']\n",
    "                if n > 20:  # Prevent overflow\n",
    "                    return (True, \"⚠ Factorial too large to verify\", None)\n",
    "                \n",
    "                correct = math.factorial(n)\n",
    "                claimed = calc['claimed_result']\n",
    "                \n",
    "                if correct == claimed:\n",
    "                    return (True, \"✓ Verified\", correct)\n",
    "                else:\n",
    "                    feedback = f\"✗ {n}! = {correct}, not {claimed}\"\n",
    "                    return (False, feedback, correct)\n",
    "            \n",
    "            # ================================================================\n",
    "            # GCD (Greatest Common Divisor)\n",
    "            # ================================================================\n",
    "            elif calc_type == 'gcd':\n",
    "                correct = math.gcd(calc['a'], calc['b'])\n",
    "                claimed = calc['claimed_result']\n",
    "                \n",
    "                if correct == claimed:\n",
    "                    return (True, \"✓ Verified\", correct)\n",
    "                else:\n",
    "                    feedback = f\"✗ gcd({calc['a']}, {calc['b']}) = {correct}, not {claimed}\"\n",
    "                    return (False, feedback, correct)\n",
    "            \n",
    "            # ================================================================\n",
    "            # LCM (Least Common Multiple)\n",
    "            # ================================================================\n",
    "            elif calc_type == 'lcm':\n",
    "                correct = abs(calc['a'] * calc['b']) // math.gcd(calc['a'], calc['b'])\n",
    "                claimed = calc['claimed_result']\n",
    "                \n",
    "                if correct == claimed:\n",
    "                    return (True, \"✓ Verified\", correct)\n",
    "                else:\n",
    "                    feedback = f\"✗ lcm({calc['a']}, {calc['b']}) = {correct}, not {claimed}\"\n",
    "                    return (False, feedback, correct)\n",
    "            \n",
    "            # ================================================================\n",
    "            # CIRCLE AREA\n",
    "            # ================================================================\n",
    "            elif calc_type == 'circle_area':\n",
    "                correct = math.pi * (calc['radius'] ** 2)\n",
    "                claimed = calc['claimed_result']\n",
    "                \n",
    "                error = abs(correct - claimed) / max(abs(correct), 0.001)\n",
    "                if error < TOLERANCE:\n",
    "                    return (True, \"✓ Verified\", correct)\n",
    "                else:\n",
    "                    feedback = f\"✗ π × {calc['radius']}² = {correct:.2f}, not {claimed}\"\n",
    "                    return (False, feedback, correct)\n",
    "            \n",
    "            # ================================================================\n",
    "            # DISTANCE FORMULA\n",
    "            # ================================================================\n",
    "            elif calc_type == 'distance':\n",
    "                dx = calc['x2'] - calc['x1']\n",
    "                dy = calc['y2'] - calc['y1']\n",
    "                correct = math.sqrt(dx**2 + dy**2)\n",
    "                claimed = calc['claimed_result']\n",
    "                \n",
    "                error = abs(correct - claimed) / max(abs(correct), 0.001)\n",
    "                if error < TOLERANCE:\n",
    "                    return (True, \"✓ Verified\", correct)\n",
    "                else:\n",
    "                    feedback = f\"✗ Distance = √({dx}² + {dy}²) = {correct:.2f}, not {claimed}\"\n",
    "                    return (False, feedback, correct)\n",
    "            \n",
    "            # ================================================================\n",
    "            # HARMONIC MEAN\n",
    "            # ================================================================\n",
    "            elif calc_type == 'harmonic_mean':\n",
    "                a, b = calc['a'], calc['b']\n",
    "                if a == 0 or b == 0:\n",
    "                    return (False, \"✗ Cannot calculate harmonic mean with zero values\", 0)\n",
    "                \n",
    "                correct = 2 / (1/a + 1/b)\n",
    "                claimed = calc['claimed_result']\n",
    "                \n",
    "                error = abs(correct - claimed) / max(abs(correct), 0.001)\n",
    "                if error < TOLERANCE:\n",
    "                    return (True, \"✓ Verified\", correct)\n",
    "                else:\n",
    "                    feedback = f\"✗ Harmonic mean of {a} and {b} = {correct:.2f}, not {claimed}. Common in rate problems!\"\n",
    "                    return (False, feedback, correct)\n",
    "            \n",
    "            # ================================================================\n",
    "            # WORK RATE (Combined Time)\n",
    "            # ================================================================\n",
    "            elif calc_type == 'work_rate':\n",
    "                a, b = calc['time_a'], calc['time_b']\n",
    "                if a == 0 or b == 0:\n",
    "                    return (False, \"✗ Work time cannot be zero\", 0)\n",
    "                \n",
    "                # Combined rate = 1/a + 1/b, combined time = 1/(combined rate)\n",
    "                combined_rate = (1/a) + (1/b)\n",
    "                correct = 1 / combined_rate\n",
    "                claimed = calc['claimed_combined_time']\n",
    "                \n",
    "                error = abs(correct - claimed) / max(abs(correct), 0.001)\n",
    "                if error < TOLERANCE:\n",
    "                    return (True, \"✓ Verified\", correct)\n",
    "                else:\n",
    "                    feedback = f\"✗ Combined time = 1/(1/{a} + 1/{b}) = {correct:.2f} hours, not {claimed}. Work rate error!\"\n",
    "                    return (False, feedback, correct)\n",
    "            \n",
    "            # ================================================================\n",
    "            # RATE × TIME = DISTANCE\n",
    "            # ================================================================\n",
    "            elif calc_type == 'rate_time_distance':\n",
    "                correct = calc['rate'] * calc['time']\n",
    "                claimed = calc['claimed_distance']\n",
    "                \n",
    "                error = abs(correct - claimed) / max(abs(correct), 0.001)\n",
    "                if error < TOLERANCE:\n",
    "                    return (True, \"✓ Verified\", correct)\n",
    "                else:\n",
    "                    reverse_rate = claimed / calc['time'] if calc['time'] != 0 else 0\n",
    "                    feedback = f\"✗ {calc['rate']} × {calc['time']} = {correct:.2f}, not {claimed}. (Reverse: rate = {reverse_rate:.2f})\"\n",
    "                    return (False, feedback, correct)\n",
    "            \n",
    "            # ================================================================\n",
    "            # MODULAR ARITHMETIC\n",
    "            # ================================================================\n",
    "            elif calc_type == 'modular':\n",
    "                correct = calc['value'] % calc['modulus']\n",
    "                claimed = calc['claimed_result']\n",
    "                \n",
    "                if correct == claimed:\n",
    "                    return (True, \"✓ Verified\", correct)\n",
    "                else:\n",
    "                    feedback = f\"✗ {calc['value']} mod {calc['modulus']} = {correct}, not {claimed}\"\n",
    "                    return (False, feedback, correct)\n",
    "            \n",
    "            # ================================================================\n",
    "            # PROBABILITY\n",
    "            # ================================================================\n",
    "            elif calc_type == 'probability':\n",
    "                correct = calc['favorable'] / calc['total'] if calc['total'] != 0 else 0\n",
    "                claimed = calc['claimed_probability']\n",
    "                \n",
    "                error = abs(correct - claimed) / max(abs(correct), 0.001)\n",
    "                if error < TOLERANCE:\n",
    "                    return (True, \"✓ Verified\", correct)\n",
    "                else:\n",
    "                    feedback = f\"✗ P = {calc['favorable']}/{calc['total']} = {correct:.4f}, not {claimed}\"\n",
    "                    return (False, feedback, correct)\n",
    "            \n",
    "            # ================================================================\n",
    "            # LOGARITHMS\n",
    "            # ================================================================\n",
    "            elif calc_type == 'logarithm':\n",
    "                # log_b(x) = y means b^y = x\n",
    "                correct = math.log(calc['value']) / math.log(calc['base'])\n",
    "                claimed = calc['claimed_result']\n",
    "                \n",
    "                error = abs(correct - claimed) / max(abs(correct), 0.001)\n",
    "                if error < TOLERANCE:\n",
    "                    return (True, \"✓ Verified\", correct)\n",
    "                else:\n",
    "                    # Verify by reverse: b^claimed = value?\n",
    "                    reverse_check = calc['base'] ** claimed\n",
    "                    feedback = f\"✗ log_{calc['base']}({calc['value']}) = {correct:.2f}, not {claimed}. (Reverse: {calc['base']}^{claimed} = {reverse_check:.2f})\"\n",
    "                    return (False, feedback, correct)\n",
    "            \n",
    "            # ================================================================\n",
    "            # MIXTURE PROBLEMS\n",
    "            # ================================================================\n",
    "            elif calc_type == 'mixture':\n",
    "                # Handle percentage vs decimal concentration\n",
    "                conc = calc['concentration']\n",
    "                if conc > 1:  # Likely a percentage\n",
    "                    conc = conc / 100\n",
    "                \n",
    "                correct = conc * calc['volume']\n",
    "                claimed = calc['claimed_amount']\n",
    "                \n",
    "                error = abs(correct - claimed) / max(abs(correct), 0.001)\n",
    "                if error < TOLERANCE:\n",
    "                    return (True, \"✓ Verified\", correct)\n",
    "                else:\n",
    "                    feedback = f\"✗ {calc['concentration']}% × {calc['volume']} = {correct:.2f}, not {claimed}\"\n",
    "                    return (False, feedback, correct)\n",
    "            \n",
    "            else:\n",
    "                return (True, \"Unknown calculation type - skipping verification\", None)\n",
    "                \n",
    "        except Exception as e:\n",
    "            return (True, f\"Verification error: {str(e)}\", None)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _verify_basic_arithmetic(calc: Dict, tolerance: float) -> Tuple[bool, str, float]:\n",
    "        \"\"\"Helper for basic arithmetic verification with detailed reverse engineering\"\"\"\n",
    "        op = calc['operator']\n",
    "        a, b = calc['operand1'], calc['operand2']\n",
    "        claimed = calc['claimed_result']\n",
    "        \n",
    "        if op == '+':\n",
    "            correct = a + b\n",
    "        elif op == '-':\n",
    "            correct = a - b\n",
    "        elif op == '*':\n",
    "            correct = a * b\n",
    "        elif op == '/':\n",
    "            if b == 0:\n",
    "                return (False, \"✗ Division by zero\", 0)\n",
    "            correct = a / b\n",
    "        else:\n",
    "            return (True, \"Unknown operator\", claimed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5faf8505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# QUESTION CLASSIFIER – FINAL FIXED VERSION (v3 – NOV 10 2025)\n",
    "# ============================================================================\n",
    "\n",
    "import re\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "\n",
    "class QuestionClassifier:\n",
    "    \"\"\"\n",
    "    ENHANCED VERSION - Preserves all original functionality\n",
    "    NEW: Forces 3 paths for CommonsenseQA to break ties\n",
    "    NEW: Expanded commonsense cue detection\n",
    "    NEW: Better strategy suggestions for 3-path generation\n",
    "    \"\"\"\n",
    "    \n",
    "    def classify(self, query: str) -> 'QuestionClassification':\n",
    "        \"\"\"\n",
    "        Classify question and determine generation strategy.\n",
    "        \n",
    "        NEW BEHAVIOR:\n",
    "        - CommonsenseQA questions ALWAYS get 3 paths (was 2)\n",
    "        - Better commonsense detection patterns\n",
    "        - Optimized strategy selection\n",
    "        \"\"\"\n",
    "        \n",
    "        # CRITICAL FIX: Extract ONLY the actual question, ignore multiple choice options\n",
    "        query_lines = query.split('\\n')\n",
    "        \n",
    "        # Find the actual question (before \"Please select\" or \"Options:\")\n",
    "        actual_question = query_lines[0]\n",
    "        for line in query_lines:\n",
    "            if any(stop in line.lower() for stop in ['please select', 'options:', 'your answer must']):\n",
    "                break\n",
    "            actual_question = line\n",
    "        \n",
    "        query_lower = actual_question.lower()  # ← USE ONLY THE QUESTION\n",
    "        \n",
    "        print(f\"[CLASSIFIER] Extracted question: {query_lower[:80]}...\")\n",
    "\n",
    "        question_type = self._determine_type(query_lower, actual_question)\n",
    "        complexity_level = self._determine_complexity(query_lower, question_type)\n",
    "\n",
    "        # ========================================================================\n",
    "        # PATH COUNT\n",
    "        # ========================================================================\n",
    "        if question_type == QuestionType.COMMONSENSE:\n",
    "            num_paths = 3  # Always 3 for CommonsenseQA\n",
    "            print(f\"[CLASSIFIER] COMMONSENSE → Forcing 3 paths for consensus\")\n",
    "        else:\n",
    "            # Original logic for other question types\n",
    "            num_paths = 1 if complexity_level == ComplexityLevel.SIMPLE else \\\n",
    "                        3 if complexity_level == ComplexityLevel.EXPERT else 2\n",
    "\n",
    "        # ========================================================================\n",
    "        # MATH PATHS & VALIDATION  ←←←  NEW BLOCK\n",
    "        # ========================================================================\n",
    "        if question_type == QuestionType.MATHEMATICAL:\n",
    "            num_paths = 3\n",
    "            print(f\"[CLASSIFIER] MATH → Forcing 3 paths (algebraic + numerical + analytical)\")\n",
    "        # --------------------------------------------------------------------\n",
    "        # Force validation / math-verification for every math question\n",
    "        requires_validation = (\n",
    "            complexity_level in {ComplexityLevel.COMPLEX, ComplexityLevel.EXPERT}\n",
    "            or question_type == QuestionType.MATHEMATICAL\n",
    "        )\n",
    "        requires_math_verification = question_type == QuestionType.MATHEMATICAL\n",
    "        # --------------------------------------------------------------------\n",
    "        # (end of new block)\n",
    "\n",
    "        # NEW: Enhanced strategy suggestions for 3-path generation\n",
    "        suggested_approaches = self._suggest_approaches(question_type, complexity_level)\n",
    "        confidence_threshold = self._get_confidence_threshold(complexity_level)\n",
    "\n",
    "        print(f\"[CLASSIFIER] → Type: {question_type.value}, Complexity: {complexity_level.value}, Paths: {num_paths}\")\n",
    "        \n",
    "        return QuestionClassification(\n",
    "            query, \n",
    "            question_type, \n",
    "            complexity_level, \n",
    "            requires_validation,\n",
    "            requires_math_verification, \n",
    "            suggested_approaches, \n",
    "            confidence_threshold, \n",
    "            num_paths\n",
    "        )\n",
    "\n",
    "    def _determine_type(self, query_lower: str, query: str) -> 'QuestionType':\n",
    "        \"\"\"\n",
    "        FIXED: Detects GSM8K-style math WITHOUT breaking CommonsenseQA\n",
    "        Preserves all original logic + adds safe math detection\n",
    "        \"\"\"\n",
    "        \n",
    "        # ===================================================================\n",
    "        # 1. GSM8K MATH DETECTION - SAFE, NON-OVERLAPPING\n",
    "        # ===================================================================\n",
    "        # Only trigger if: numbers + quantity words + \"how many/much\"\n",
    "        # Does NOT use \"in a market\", \"at home\", etc.\n",
    "        gsm8k_math_patterns = [\n",
    "            r'\\d+\\s+(eggs?|apples?|oranges?|books?|pages?|dollars?|cents?|hours?|minutes?|days?|weeks?)',\n",
    "            r'\\d+\\s+(per|each|every|total|altogether|remainder|left|remained|sells?|buys?)',\n",
    "            r'(how many|how much)\\s+.*\\?',\n",
    "            r'\\d+\\s*[\\+\\-\\*\\/]\\s*\\d+',\n",
    "            r'(half|twice|double|triple|quarter)\\s+(of\\s+)?\\d+',\n",
    "        ]\n",
    "        \n",
    "        if any(re.search(p, query_lower) for p in gsm8k_math_patterns):\n",
    "            # Double-check: avoid false positives on factual questions\n",
    "            if not any(word in query_lower for word in ['typically', 'usually', 'where would', 'what do people']):\n",
    "                print(\"[CLASSIFIER] MATH WORD PROBLEM (GSM8K-style)\")\n",
    "                return QuestionType.MATHEMATICAL\n",
    "\n",
    "        # ===================================================================\n",
    "        # 2. STRICT MATH (unchanged - for algebra, calculus)\n",
    "        # ===================================================================\n",
    "        strict_math_keywords = [\n",
    "            'solve', 'equation', 'calculate', 'integral', 'derivative',\n",
    "            'quadratic', 'factor', 'simplify', 'prove mathematically', 'find x'\n",
    "        ]\n",
    "        if any(kw in query_lower for kw in strict_math_keywords):\n",
    "            print(\"[CLASSIFIER] MATH KEYWORD\")\n",
    "            return QuestionType.MATHEMATICAL\n",
    "\n",
    "        strict_math_patterns = [\n",
    "            r'\\d+x\\s*[\\+\\-\\*/]', r'x\\^', r'\\b(sin|cos|tan|log)\\s*\\(', r'√\\d',\n",
    "            r'\\d+\\s*[\\+\\-\\*/]\\s*\\d+\\s*='\n",
    "        ]\n",
    "        if any(re.search(p, query) for p in strict_math_patterns):\n",
    "            print(\"[CLASSIFIER] MATH REGEX\")\n",
    "            return QuestionType.MATHEMATICAL\n",
    "\n",
    "        # ===================================================================\n",
    "        # 3. COMMONSENSE - YOUR ORIGINAL + SAFER CUES\n",
    "        # ===================================================================\n",
    "        # REMOVED: 'in a market', 'at home', 'aside from' → too broad\n",
    "        safe_commonsense_cues = [\n",
    "            'typically', 'usually', 'commonly', 'often', 'likely',\n",
    "            'where would you', 'what do people usually',\n",
    "            'what might', 'who might', 'where might',\n",
    "            'good place to', 'if you wanted to', 'after he', 'after she'\n",
    "        ]\n",
    "        if any(cue in query_lower for cue in safe_commonsense_cues):\n",
    "            print(\"[CLASSIFIER] COMMONSENSE CUE\")\n",
    "            return QuestionType.COMMONSENSE\n",
    "\n",
    "        # ===================================================================\n",
    "        # 4. FACTUAL / BINARY / ANALYTICAL (unchanged)\n",
    "        # ===================================================================\n",
    "        if any(query_lower.startswith(st) for st in ['what is', 'who is', 'where is', 'when was']):\n",
    "            print(\"[CLASSIFIER] FACTUAL\")\n",
    "            return QuestionType.FACTUAL\n",
    "\n",
    "        if any(query_lower.startswith(st) for st in ['is ', 'are ', 'does ', 'do ', 'can ', 'will ']) and query.endswith('?'):\n",
    "            print(\"[CLASSIFIER] BINARY\")\n",
    "            return QuestionType.BINARY\n",
    "\n",
    "        if any(kw in query_lower for kw in ['explain', 'why', 'how does', 'compare']):\n",
    "            print(\"[CLASSIFIER] ANALYTICAL\")\n",
    "            return QuestionType.ANALYTICAL\n",
    "\n",
    "        # ===================================================================\n",
    "        # 5. DEFAULT: If numbers → MATH, else COMMONSENSE\n",
    "        # ===================================================================\n",
    "        if re.search(r'\\d', query):\n",
    "            print(\"[CLASSIFIER] DEFAULT → MATHEMATICAL (has numbers)\")\n",
    "            return QuestionType.MATHEMATICAL\n",
    "\n",
    "        print(\"[CLASSIFIER] DEFAULT → COMMONSENSE\")\n",
    "        return QuestionType.COMMONSENSE\n",
    "\n",
    "    def _determine_complexity(self, query_lower: str, question_type: 'QuestionType') -> 'ComplexityLevel':\n",
    "        \"\"\"Complexity determination (unchanged from original)\"\"\"\n",
    "        if any(ind in query_lower for ind in ['prove', 'theorem', 'paradox']): \n",
    "            return ComplexityLevel.EXPERT\n",
    "        if any(ind in query_lower for ind in ['system', 'quadratic', 'monty hall']): \n",
    "            return ComplexityLevel.COMPLEX\n",
    "        if any(ind in query_lower for ind in ['capital', 'simple']): \n",
    "            return ComplexityLevel.SIMPLE\n",
    "        return ComplexityLevel.MODERATE\n",
    "\n",
    "    def _suggest_approaches(self, question_type: 'QuestionType', \n",
    "                      complexity: 'ComplexityLevel') -> List['ReasoningApproach']:\n",
    "        \"\"\"\n",
    "        100% PRESERVED FROM YOUR ORIGINAL\n",
    "        + Added 3-path math support\n",
    "        \"\"\"\n",
    "        if question_type == QuestionType.MATHEMATICAL:\n",
    "            # Use 3 paths: algebraic + numerical + analytical (for verification)\n",
    "            return [\n",
    "                ReasoningApproach.ALGEBRAIC,       # Primary\n",
    "                ReasoningApproach.NUMERICAL,      # Backup\n",
    "                ReasoningApproach.ANALYTICAL      # Cross-check\n",
    "            ]\n",
    "        \n",
    "        # YOUR ORIGINAL COMMONSENSEQA LOGIC - UNCHANGED\n",
    "        if question_type == QuestionType.COMMONSENSE:\n",
    "            return [\n",
    "                ReasoningApproach.ANALYTICAL,      # Best performer\n",
    "                ReasoningApproach.EVIDENCE_BASED,  # Second best\n",
    "                ReasoningApproach.SKEPTICAL        # Tie-breaker\n",
    "            ]\n",
    "        \n",
    "        if question_type == QuestionType.BINARY:\n",
    "            return [ReasoningApproach.ANALYTICAL, ReasoningApproach.SKEPTICAL]\n",
    "        \n",
    "        # Default fallback\n",
    "        return [ReasoningApproach.ANALYTICAL, ReasoningApproach.EVIDENCE_BASED]\n",
    "\n",
    "    def _get_confidence_threshold(self, complexity: 'ComplexityLevel') -> float:\n",
    "        \"\"\"Confidence thresholds by complexity (unchanged from original)\"\"\"\n",
    "        return {\n",
    "            ComplexityLevel.SIMPLE: 0.50, \n",
    "            ComplexityLevel.MODERATE: 0.65,\n",
    "            ComplexityLevel.COMPLEX: 0.70, \n",
    "            ComplexityLevel.EXPERT: 0.75\n",
    "        }[complexity]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26d25b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLAUDE REASONING GENERATOR - WITH FIX 1, 2, 5\n",
    "# ============================================================================\n",
    "# This class manages reasoning path generation using Anthropic’s Claude model.\n",
    "# It handles classification, multiple parallel reasoning strategies,\n",
    "# confidence estimation, and fallback logic.\n",
    "# The \"FIX 1, 2, 5\" notes correspond to key improvements:\n",
    "#   FIX 1 → Dynamic confidence calculation instead of static.\n",
    "#   FIX 2 → Improved answer extraction from responses.\n",
    "#   FIX 5 → More robust reasoning step extraction patterns.\n",
    "# ============================================================================\n",
    "\n",
    "class ClaudeReasoningGenerator:\n",
    "    def __init__(self, api_key: str, strategy: Optional[ReasoningStrategy] = None):\n",
    "        self.client = anthropic.Anthropic(api_key=api_key)\n",
    "        self.model = \"claude-sonnet-4-5-20250929\"\n",
    "        \n",
    "        # NEW: Strategy support (backward compatible)\n",
    "        self.strategy = strategy\n",
    "        if strategy is None:\n",
    "            # Fallback to old classifier for backward compatibility\n",
    "            self.classifier = QuestionClassifier()\n",
    "        \n",
    "        # NEW: Strategy support (backward compatible)\n",
    "        self.strategy = strategy\n",
    "        if strategy is None:\n",
    "            # Fallback to old classifier for backward compatibility\n",
    "            self.classifier = QuestionClassifier()\n",
    "\n",
    "        # Track token usage for cost estimation\n",
    "        self.total_tokens_input = 0\n",
    "        self.total_tokens_output = 0\n",
    "\n",
    "        # Supporting components for classification and confidence scoring\n",
    "        self.classifier = QuestionClassifier()\n",
    "        self.confidence_assessor = ConfidenceAssessor()\n",
    "        self.math_verifier = MathematicalVerifier()  # ✅ NEW\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Parallel generation of multiple reasoning paths using different strategies\n",
    "    # -----------------------------------\n",
    "    def generate_multiple_paths_parallel(self, query: str, num_paths: int = 3,\n",
    "                                        classification: Optional[QuestionClassification] = None):\n",
    "        \"\"\"\n",
    "        Generate multiple reasoning paths in parallel with optional strategy integration.\n",
    "        \n",
    "        NEW: Supports pluggable strategy providers for custom prompts.\n",
    "        OLD: Falls back to built-in _select_strategies_ENHANCED for backward compatibility.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # NEW: Use strategy if provided\n",
    "        if self.strategy is not None:\n",
    "            if classification is None:\n",
    "                classification = self.strategy.classify_question(query)\n",
    "            strategies = self.strategy.get_generation_prompts(query, classification.num_paths)\n",
    "        else:\n",
    "            # OLD PATH: Backward compatibility\n",
    "            if classification is None:\n",
    "                classification = self.classifier.classify(query)\n",
    "            strategies = self._select_strategies_ENHANCED(classification, num_paths)\n",
    "        \n",
    "        # Rest of method unchanged - execute strategies in parallel\n",
    "        paths = []\n",
    "\n",
    "        # Execute each strategy in parallel threads\n",
    "        with ThreadPoolExecutor(max_workers=num_paths) as executor:\n",
    "            future_to_strategy = {\n",
    "                executor.submit(\n",
    "                    self._generate_path, query, strategy_name, instruction, classification\n",
    "                ): strategy_name\n",
    "                for strategy_name, instruction in strategies\n",
    "            }\n",
    "            \n",
    "            # Collect completed results as they finish\n",
    "            for future in as_completed(future_to_strategy):\n",
    "                strategy = future_to_strategy[future]\n",
    "                try:\n",
    "                    path = future.result()\n",
    "                    if path and (path.steps or path.verdict or path.answer):\n",
    "                        paths.append(path)\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ {strategy} generation error: {e}\")\n",
    "        \n",
    "        # Compute parallelization speedup metrics\n",
    "        total_time = time.time() - start_time\n",
    "        estimated_sequential = sum(p.generation_time for p in paths) if paths else total_time\n",
    "        speedup = estimated_sequential / total_time if total_time > 0 else 1.0\n",
    "        \n",
    "        # Create fallback path if generation failed entirely\n",
    "        if not paths:\n",
    "            paths.append(self._create_fallback_path(query, classification))\n",
    "        \n",
    "        return paths, speedup\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Select generation strategies depending on question classification\n",
    "    # ------------------------------------------------------------------------\n",
    "    def _select_strategies(self, classification: 'QuestionClassification', \n",
    "                    num_paths: int) -> List[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        ENHANCED: Optimized strategy selection with better prompts.\n",
    "        \n",
    "        KEY IMPROVEMENTS:\n",
    "        1. Analytical ALWAYS first (gets +0.05 bonus in synthesis)\n",
    "        2. 3 strategies for CommonsenseQA (supports 3-path generation)\n",
    "        3. More directive prompts that guide toward specific answers\n",
    "        4. Emphasis on choosing concrete over abstract\n",
    "        \"\"\"\n",
    "        \n",
    "        if classification.question_type == QuestionType.COMMONSENSE:\n",
    "            # CRITICAL: Order matters - analytical first (best performer, gets bonus)\n",
    "            strategies = [\n",
    "                (\"analytical\", \n",
    "                \"\"\"Use everyday common sense and practical reasoning.\n",
    "                \n",
    "    IMPORTANT: Choose the MOST SPECIFIC, CONCRETE answer.\n",
    "    - Prefer specific actions over generic categories (e.g., \"singing\" > \"making music\")\n",
    "    - Prefer specific locations over general places (e.g., \"refrigerator\" > \"place\")\n",
    "    - Prefer direct terms over vague descriptions\n",
    "    - Think: What is the MOST DIRECT answer to this question?\"\"\"),\n",
    "                \n",
    "                (\"evidence_based\", \n",
    "                \"\"\"Use practical knowledge and real-world experience.\n",
    "                \n",
    "    IMPORTANT: Focus on SPECIFICITY.\n",
    "    - What is the most concrete, tangible answer?\n",
    "    - Avoid abstract or overly broad options\n",
    "    - Choose the answer that directly names the thing/action, not a category\n",
    "    - Real-world context: What would people actually say?\"\"\"),\n",
    "                \n",
    "                (\"skeptical\", \n",
    "                \"\"\"Think critically about each option.\n",
    "                \n",
    "    IMPORTANT: Eliminate based on specificity.\n",
    "    - Remove vague, generic, or overly broad choices\n",
    "    - Remove abstract concepts when concrete options exist\n",
    "    - Question: Which answer is MOST SPECIFIC and DIRECT?\n",
    "    - Be decisive - choose the clearest, most concrete option\"\"\"),\n",
    "            ]\n",
    "            return strategies[:num_paths]\n",
    "        \n",
    "        elif classification.question_type == QuestionType.BINARY:\n",
    "            return [\n",
    "                (\"analytical\", \n",
    "                \"Analyze this systematically using logic and evidence. Be clear and decisive in your verdict.\"),\n",
    "                (\"skeptical\", \n",
    "                \"Approach critically, questioning assumptions. Challenge the premise if needed.\"),\n",
    "                (\"evidence_based\", \n",
    "                \"Focus strictly on factual evidence. What does established knowledge say?\")\n",
    "            ][:num_paths]\n",
    "        \n",
    "        elif classification.question_type == QuestionType.MATHEMATICAL:\n",
    "            return [\n",
    "                (\"algebraic\", \n",
    "                \"Solve using algebraic methods. Show each step clearly with proper notation.\"),\n",
    "                (\"numerical\", \n",
    "                \"Use numerical calculations. Compute each step explicitly with actual numbers.\"),\n",
    "            ][:num_paths]\n",
    "        \n",
    "        elif classification.question_type == QuestionType.FACTUAL:\n",
    "            return [\n",
    "                (\"direct\", \n",
    "                \"Provide the factual answer with supporting context. Be precise and cite established facts.\")\n",
    "            ]\n",
    "        \n",
    "        else:\n",
    "            # Default analytical strategies\n",
    "            return [\n",
    "                (\"analytical\", \n",
    "                \"Analyze this comprehensively. Break down the question and reason systematically.\"),\n",
    "                (\"evidence_based\", \n",
    "                \"Base your analysis on established knowledge. What do authoritative sources say?\"),\n",
    "            ][:num_paths]\n",
    "\n",
    "    def _select_strategies_ENHANCED(self, classification: 'QuestionClassification', \n",
    "                                    num_paths: int) -> List[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        ENHANCED: Better mathematical strategy prompts.\n",
    "        All other question types preserved unchanged.\n",
    "        \"\"\"\n",
    "        \n",
    "        if classification.question_type == QuestionType.COMMONSENSE:\n",
    "            # ORIGINAL COMMONSENSE CODE PRESERVED EXACTLY\n",
    "            strategies = [\n",
    "                (\"analytical\", \n",
    "                \"\"\"Use everyday common sense and practical reasoning.\n",
    "                \n",
    "    IMPORTANT: Choose the MOST SPECIFIC, CONCRETE answer.\n",
    "    - Prefer specific actions over generic categories (e.g., \"singing\" > \"making music\")\n",
    "    - Prefer specific locations over general places (e.g., \"refrigerator\" > \"place\")\n",
    "    - Prefer direct terms over vague descriptions\n",
    "    - Think: What is the MOST DIRECT answer to this question?\"\"\"),\n",
    "                \n",
    "                (\"evidence_based\", \n",
    "                \"\"\"Use practical knowledge and real-world experience.\n",
    "                \n",
    "    IMPORTANT: Focus on SPECIFICITY.\n",
    "    - What is the most concrete, tangible answer?\n",
    "    - Avoid abstract or overly broad options\n",
    "    - Choose the answer that directly names the thing/action, not a category\n",
    "    - Real-world context: What would people actually say?\"\"\"),\n",
    "                \n",
    "                (\"skeptical\", \n",
    "                \"\"\"Think critically about each option.\n",
    "                \n",
    "    IMPORTANT: Eliminate based on specificity.\n",
    "    - Remove vague, generic, or overly broad choices\n",
    "    - Remove abstract concepts when concrete options exist\n",
    "    - Question: Which answer is MOST SPECIFIC and DIRECT?\n",
    "    - Be decisive - choose the clearest, most concrete option\"\"\"),\n",
    "            ]\n",
    "            return strategies[:num_paths]\n",
    "        \n",
    "        elif classification.question_type == QuestionType.BINARY:\n",
    "            # ORIGINAL BINARY CODE PRESERVED\n",
    "            return [\n",
    "                (\"analytical\", \n",
    "                \"Analyze this systematically using logic and evidence. Be clear and decisive in your verdict.\"),\n",
    "                (\"skeptical\", \n",
    "                \"Approach critically, questioning assumptions. Challenge the premise if needed.\"),\n",
    "                (\"evidence_based\", \n",
    "                \"Focus strictly on factual evidence. What does established knowledge say?\")\n",
    "            ][:num_paths]\n",
    "        \n",
    "        elif classification.question_type == QuestionType.MATHEMATICAL:\n",
    "            # ENHANCED: More demanding mathematical prompts\n",
    "            return [\n",
    "                (\"algebraic\", \n",
    "                \"\"\"Solve using algebraic methods. \n",
    "\n",
    "    CRITICAL REQUIREMENTS:\n",
    "    1. Show EVERY step of your calculation explicitly\n",
    "    2. Write out ALL arithmetic operations (don't skip steps)\n",
    "    3. Label your steps clearly (Step 1, Step 2, etc.)\n",
    "    4. MUST end with: ANSWER: [number]\n",
    "\n",
    "    Be systematic and careful with calculations.\"\"\"),\n",
    "                \n",
    "                (\"numerical\", \n",
    "                \"\"\"Solve using numerical calculations step-by-step.\n",
    "\n",
    "    CRITICAL REQUIREMENTS:\n",
    "    1. Convert word problem to concrete numbers immediately\n",
    "    2. Show ALL arithmetic: 5 + 3 = 8, then 8 × 2 = 16, etc.\n",
    "    3. Label each calculation clearly\n",
    "    4. MUST end with: ANSWER: [number]\n",
    "\n",
    "    Work through the problem methodically. Double-check arithmetic.\"\"\"),\n",
    "            ][:num_paths]\n",
    "        \n",
    "        elif classification.question_type == QuestionType.FACTUAL:\n",
    "            # ORIGINAL FACTUAL CODE PRESERVED\n",
    "            return [\n",
    "                (\"direct\", \n",
    "                \"Provide the factual answer with supporting context. Be precise and cite established facts.\")\n",
    "            ]\n",
    "        \n",
    "        else:\n",
    "            # ORIGINAL DEFAULT CODE PRESERVED\n",
    "            return [\n",
    "                (\"analytical\", \n",
    "                \"Analyze this comprehensively. Break down the question and reason systematically.\"),\n",
    "                (\"evidence_based\", \n",
    "                \"Base your analysis on established knowledge. What do authoritative sources say?\"),\n",
    "            ][:num_paths]\n",
    "\n",
    "        # ============================================================================\n",
    "        # INTEGRATION NOTES\n",
    "        # ============================================================================\n",
    "        \"\"\"\n",
    "        WHY THESE CHANGES HELP:\n",
    "\n",
    "        1. **Analytical First (CRITICAL)**\n",
    "        - Analytical path gets +0.05 bonus in synthesis\n",
    "        - Your data shows it's the best performer (6 correct answers that were overruled)\n",
    "        - By placing it first, it gets generated first and gets the bonus\n",
    "\n",
    "        2. **3 Strategies for CommonsenseQA**\n",
    "        - Now returns 3 strategies (analytical, evidence_based, skeptical)\n",
    "        - Supports 3-path generation (breaks 50-50 ties)\n",
    "        - Adds skeptical as tie-breaker path\n",
    "\n",
    "        3. **Enhanced Prompts with Specificity Guidance**\n",
    "        - Each prompt now explicitly instructs to choose SPECIFIC over GENERAL\n",
    "        - Emphasizes concrete actions/nouns over abstract concepts\n",
    "        - Aligns with your SpecificityScorer philosophy\n",
    "        - Guides model toward answers that will score well\n",
    "\n",
    "        4. **Multi-line Prompts**\n",
    "        - More detailed instructions help model understand task better\n",
    "        - Explicit examples (\"singing\" > \"making music\")\n",
    "        - Clear decision criteria\n",
    "\n",
    "        5. **Directive Language**\n",
    "        - Changed from suggestive (\"Use...\") to directive (\"IMPORTANT: Choose...\")\n",
    "        - Makes prompts more forceful and clear\n",
    "        - Model follows instructions more consistently\n",
    "\n",
    "        EXPECTED IMPACT:\n",
    "        - Analytical path performs even better (already best, now with better prompt)\n",
    "        - All 3 paths guided toward specific answers\n",
    "        - More consistent answer quality across paths\n",
    "        - Better consensus when all paths agree on specificity\n",
    "\n",
    "        USAGE:\n",
    "        Simply replace your existing _select_strategies_ENHANCED method with this one.\n",
    "        No other changes needed - works with your existing code.\n",
    "        \"\"\"\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Dispatch correct generation method based on question type\n",
    "    # ------------------------------------------------------------------------\n",
    "    def _generate_path(\n",
    "        self, query: str, strategy: str, instruction: str,\n",
    "        classification: QuestionClassification\n",
    "    ) -> Optional[ReasoningPath]:\n",
    "        \"\"\"Delegate path generation to binary or adaptive generator.\"\"\"\n",
    "        if classification.question_type == QuestionType.BINARY:\n",
    "            return self._generate_binary_path(query, strategy, instruction, classification)\n",
    "        else:\n",
    "            return self._generate_adaptive_path(query, strategy, instruction, classification)\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # Binary question handler (YES/NO, TRUE/FALSE)\n",
    "    # ------------------------------------------------------------------------\n",
    "    def _generate_binary_path(\n",
    "        self, query: str, strategy: str, instruction: str,\n",
    "        classification: QuestionClassification\n",
    "    ) -> Optional[ReasoningPath]:\n",
    "        \"\"\"Generate reasoning path for binary-type questions.\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fixed prompt template for binary reasoning\n",
    "        prompt = f\"\"\"Question: {query}\n",
    "\n",
    "{instruction}\n",
    "\n",
    "Provide your analysis in this EXACT format:\n",
    "\n",
    "VERDICT: YES or NO\n",
    "\n",
    "REASONING:\n",
    "Step 1: [Your first point]\n",
    "Step 2: [Your second point]\n",
    "Step 3: [Your third point]\n",
    "\n",
    "CONCLUSION: [Your final conclusion]\n",
    "\n",
    "Be direct and committed in your verdict.\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Send prompt to Claude\n",
    "            message = self.client.messages.create(\n",
    "                model=self.model,\n",
    "                max_tokens=1500,\n",
    "                temperature=0.7,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            \n",
    "            # Extract text and token counts\n",
    "            response_text = message.content[0].text\n",
    "            self.total_tokens_input += message.usage.input_tokens\n",
    "            self.total_tokens_output += message.usage.output_tokens\n",
    "            \n",
    "            generation_time = time.time() - start_time\n",
    "            \n",
    "            # Parse structured reasoning from response\n",
    "            path = self._parse_binary_response(query, response_text, strategy, classification)\n",
    "            path.generation_time = generation_time\n",
    "            path.question_type = QuestionType.BINARY\n",
    "            \n",
    "            return path\n",
    "        except Exception as e:\n",
    "            print(f\"Claude API error for {strategy}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # Adaptive path generator for non-binary question types\n",
    "    # ------------------------------------------------------------------------\n",
    "    def _generate_adaptive_path(self, query: str, strategy: str, instruction: str,\n",
    "                            classification: 'QuestionClassification') -> Optional['ReasoningPath']:\n",
    "        \"\"\"Generate reasoning path with COMMONSENSE-SPECIFIC prompts\"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # NEW: Detect if this is a CommonsenseQA question\n",
    "        is_commonsense_qa = 'Please select ONLY ONE' in query\n",
    "        \n",
    "        if is_commonsense_qa:\n",
    "            # Extract the actual question and choices\n",
    "            parts = query.split('\\n\\nPlease select ONLY ONE')\n",
    "            question_text = parts[0]\n",
    "            choices_text = parts[1] if len(parts) > 1 else \"\"\n",
    "            \n",
    "            # CRITICAL: Use commonsense-specific prompt\n",
    "            prompt = self._build_commonsense_prompt(question_text, choices_text, strategy)\n",
    "        else:\n",
    "            # Regular adaptive prompt\n",
    "            output_format = self._get_output_format(classification.question_type)\n",
    "            prompt = f\"\"\"Question: {query}\n",
    "\n",
    "    {instruction}\n",
    "\n",
    "    {output_format}\n",
    "\n",
    "    Be clear and systematic.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            message = self.client.messages.create(\n",
    "                model=self.model,\n",
    "                max_tokens=800,  # Increased for better reasoning\n",
    "                temperature=0.7,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            \n",
    "            response_text = message.content[0].text\n",
    "            self.total_tokens_input += message.usage.input_tokens\n",
    "            self.total_tokens_output += message.usage.output_tokens\n",
    "            \n",
    "            generation_time = time.time() - start_time\n",
    "            \n",
    "            # Parse response\n",
    "            path = self._parse_adaptive_response(query, response_text, strategy, classification)\n",
    "            path.generation_time = generation_time\n",
    "            return path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Generation error for {strategy}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _build_commonsense_prompt(self, question: str, choices: str, strategy: str) -> str:\n",
    "        \"\"\"\n",
    "        CRITICAL: Guide toward SPECIFIC, CONCRETE answers using commonsense reasoning.\n",
    "        Emphasizes choosing the most direct, specific option over generic or abstract ones.\n",
    "        \"\"\"\n",
    "        \n",
    "        if strategy == \"analytical\":\n",
    "            return f\"\"\"{question}\n",
    "\n",
    "    {choices}\n",
    "\n",
    "    Use EVERYDAY COMMONSENSE reasoning:\n",
    "\n",
    "    ANALYSIS:\n",
    "    Step 1: What is the question REALLY asking?\n",
    "    - Focus on the MOST SPECIFIC, CONCRETE action or thing\n",
    "    - Avoid generic or abstract answers\n",
    "    - Think about practical, real-world context\n",
    "\n",
    "    Step 2: Evaluate each option\n",
    "    - Which answer is the MOST SPECIFIC and DIRECT?\n",
    "    - Eliminate vague, generic, or overly broad options\n",
    "    - Choose the answer that directly describes the action/thing, not a category\n",
    "    - Remove options that are absurd, too literal, or technical\n",
    "\n",
    "    Step 3: Select the MOST SPECIFIC answer\n",
    "    - Prefer concrete actions over abstract concepts\n",
    "    - Prefer specific items over general categories\n",
    "    - Examples: \"singing\" beats \"making music\", \"torn\" beats \"damaged\", \"attention\" beats \"walked\"\n",
    "\n",
    "    ANSWER: [Single letter A-E]: [choice text]\n",
    "\n",
    "    CRITICAL: Choose the MOST SPECIFIC, CONCRETE option that makes real-world sense.\"\"\"\n",
    "\n",
    "        elif strategy == \"evidence_based\":\n",
    "            return f\"\"\"{question}\n",
    "\n",
    "    {choices}\n",
    "\n",
    "    Use PRACTICAL KNOWLEDGE to find the MOST SPECIFIC answer:\n",
    "\n",
    "    ANALYSIS:\n",
    "    Step 1: What is the everyday context of this question?\n",
    "    - What SPECIFIC action or thing is being asked about?\n",
    "    - Think about how people actually use these concepts\n",
    "\n",
    "    Step 2: Apply specificity test to each option\n",
    "    - Which answer is MOST CONCRETE and DIRECT?\n",
    "    - Eliminate generic categories in favor of specific instances\n",
    "    - Remove vague or overly broad options\n",
    "\n",
    "    Step 3: Select the MOST SPECIFIC, PRACTICAL answer\n",
    "    - Choose the option that directly names the action/thing\n",
    "    - Avoid abstract or categorical answers\n",
    "\n",
    "    ANSWER: [Single letter A-E]: [choice text]\n",
    "\n",
    "    Remember: Prefer SPECIFIC over GENERAL, CONCRETE over ABSTRACT.\"\"\"\n",
    "\n",
    "        else:  # skeptical or other\n",
    "            return f\"\"\"{question}\n",
    "\n",
    "    {choices}\n",
    "\n",
    "    Think through this CAREFULLY:\n",
    "\n",
    "    ANALYSIS:\n",
    "    Step 1: Parse what the question means in NORMAL LANGUAGE\n",
    "    - What SPECIFIC thing is being asked about?\n",
    "\n",
    "    Step 2: Evaluate specificity of each option\n",
    "    - Which is the MOST DIRECT and CONCRETE answer?\n",
    "    - Eliminate generic, vague, or overly broad options\n",
    "\n",
    "    Step 3: Select the MOST SPECIFIC answer that makes REAL-WORLD SENSE\n",
    "\n",
    "    ANSWER: [Single letter A-E]: [choice text]\n",
    "\n",
    "    Use common sense and choose the MOST SPECIFIC, CONCRETE option.\"\"\"\n",
    "\n",
    "    # ============================================================================\n",
    "    # Added MATHEMATICAL PROMPTS (UPDATE _get_output_format)\n",
    "    # ============================================================================\n",
    "    def _get_output_format(self, question_type: QuestionType) -> str:\n",
    "        \"\"\"\n",
    "        ENHANCED: Math prompts now REQUIRE explicit ANSWER statements.\n",
    "        Other formats preserved unchanged.\n",
    "        \"\"\"\n",
    "        if question_type == QuestionType.FACTUAL:\n",
    "            return \"\"\"Provide your answer:\n",
    "\n",
    "    ANSWER: [factual answer]\n",
    "\n",
    "    REASONING:\n",
    "    Step 1: [supporting evidence]\n",
    "    Step 2: [additional support]\n",
    "\n",
    "    CONCLUSION: [summary]\"\"\"\n",
    "        \n",
    "        elif question_type == QuestionType.MATHEMATICAL:\n",
    "            # ENHANCED: Much stricter format requirements\n",
    "            return \"\"\"Solve this step-by-step:\n",
    "\n",
    "    SOLUTION:\n",
    "    Step 1: [Understand the problem - identify what's given and what's asked]\n",
    "    Step 2: [Set up equations or identify operations needed]\n",
    "    Step 3: [Perform calculations with clear arithmetic]\n",
    "    Step 4: [Continue solving until you reach the final number]\n",
    "\n",
    "    CRITICAL: You MUST end with this exact format:\n",
    "    ANSWER: [numerical value]\n",
    "\n",
    "    Example: ANSWER: 42\n",
    "\n",
    "    Show ALL calculations explicitly. Double-check your arithmetic.\"\"\"\n",
    "        \n",
    "        else:\n",
    "            return \"\"\"Provide analysis:\n",
    "\n",
    "    ANALYSIS:\n",
    "    Step 1: [key insight]\n",
    "    Step 2: [supporting evidence]\n",
    "    Step 3: [conclusion]\n",
    "\n",
    "    CONCLUSION: [summary]\"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Response parsing methods\n",
    "    # ------------------------------------------------------------------------\n",
    "    def _parse_binary_response(self, query, response, strategy, classification) -> ReasoningPath:\n",
    "        \"\"\"Parse binary response into a structured ReasoningPath.\"\"\"\n",
    "        verdict = self._extract_verdict(response)\n",
    "        steps = self._extract_reasoning_steps(response, strategy, QuestionType.BINARY)\n",
    "        conclusion = self._extract_conclusion(response, verdict)\n",
    "\n",
    "        # FIX: Pass verdict as the answer parameter\n",
    "        confidence = self._calculate_dynamic_confidence(steps, response, verdict)\n",
    "        \n",
    "        return ReasoningPath(\n",
    "            path_id=f\"{strategy}_{int(time.time()*1000)}\",\n",
    "            query=query,\n",
    "            verdict=verdict,\n",
    "            steps=steps,\n",
    "            conclusion=conclusion,\n",
    "            confidence=confidence,\n",
    "            generation_strategy=strategy,\n",
    "            raw_output=response,\n",
    "            question_type=classification.question_type,\n",
    "            complexity_level=classification.complexity_level\n",
    "        )\n",
    "\n",
    "    def _parse_adaptive_response(self, query, response, strategy, classification) -> ReasoningPath:\n",
    "        \"\"\"\n",
    "        Parse adaptive (non-binary) responses into a structured ReasoningPath.\n",
    "        \n",
    "        NEW: Supports pluggable strategy providers for custom answer extraction.\n",
    "        OLD: Falls back to built-in _extract_answer_improved for backward compatibility.\n",
    "        \"\"\"\n",
    "        \n",
    "        # NEW: Use strategy extraction if available\n",
    "        if self.strategy is not None:\n",
    "            answer = self.strategy.extract_answer(response)\n",
    "        else:\n",
    "            # OLD PATH: Backward compatibility\n",
    "            answer = self._extract_answer_improved(response, classification.question_type)\n",
    "        \n",
    "        # Extract reasoning steps (unchanged)\n",
    "        steps = self._extract_reasoning_steps(response, strategy, classification.question_type)\n",
    "        \n",
    "        # Extract conclusion (unchanged)\n",
    "        conclusion = self._extract_conclusion(response, answer)\n",
    "\n",
    "        # Calculate confidence (unchanged)\n",
    "        confidence = self._calculate_dynamic_confidence(steps, response, answer)\n",
    "        \n",
    "        # Build and return ReasoningPath\n",
    "        return ReasoningPath(\n",
    "            path_id=f\"{strategy}_{int(time.time()*1000)}\",\n",
    "            query=query,\n",
    "            answer=answer,\n",
    "            steps=steps,\n",
    "            conclusion=conclusion,\n",
    "            confidence=confidence,\n",
    "            generation_strategy=strategy,\n",
    "            raw_output=response,\n",
    "            question_type=classification.question_type,\n",
    "            complexity_level=classification.complexity_level\n",
    "        )\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Text extraction helpers\n",
    "    # ------------------------------------------------------------------------\n",
    "    def _extract_verdict(self, response: str) -> Optional[str]:\n",
    "        \"\"\"Extract a binary verdict (YES/NO/TRUE/FALSE) from response text.\"\"\"\n",
    "        verdict_patterns = [\n",
    "            r'VERDICT:\\s*([A-Z]+)',\n",
    "            r'Verdict:\\s*([A-Z]+)',\n",
    "        ]\n",
    "        \n",
    "        for pattern in verdict_patterns:\n",
    "            match = re.search(pattern, response, re.MULTILINE | re.IGNORECASE)\n",
    "            if match:\n",
    "                verdict = match.group(1).upper()\n",
    "                if verdict in ['TRUE', 'FALSE', 'YES', 'NO', 'DEPENDS', 'UNCLEAR']:\n",
    "                    return verdict\n",
    "        \n",
    "        # Infer from natural language if explicit verdict missing\n",
    "        response_lower = response.lower()\n",
    "        if any(phrase in response_lower for phrase in ['is false', 'not true', 'myth']):\n",
    "            return 'FALSE'\n",
    "        elif any(phrase in response_lower for phrase in ['is true', 'correct', 'accurate']):\n",
    "            return 'TRUE'\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _extract_answer_improved(self, response: str, question_type: 'QuestionType') -> Optional[str]:\n",
    "        \"\"\"\n",
    "        FIXED: Enhanced extraction for both mathematical and commonsense answers.\n",
    "        Priority order: ANSWER marker > CONCLUSION > calculations > last number\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n[DEBUG ANSWER EXTRACT] Question type: {question_type}\")\n",
    "        print(f\"[DEBUG ANSWER EXTRACT] Response preview: {response[:300]}\")\n",
    "        \n",
    "        # ========================================================================\n",
    "        # MATHEMATICAL QUESTION HANDLING\n",
    "        # ========================================================================\n",
    "        if question_type == QuestionType.MATHEMATICAL:\n",
    "            # Strategy 1: Explicit ANSWER markers (highest priority)\n",
    "            explicit_patterns = [\n",
    "                r'ANSWER\\s*[:\\=]\\s*\\$?(\\d+(?:,\\d{3})*(?:\\.\\d+)?)',  # ANSWER: 42\n",
    "                r'\\\\boxed\\{(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\}',           # \\boxed{42}\n",
    "                r'(?:final\\s+answer|the\\s+answer)(?:\\s+is|\\s*:|\\s*=)\\s*\\$?\\*?\\*?(\\d+(?:,\\d{3})*(?:\\.\\d+)?)',\n",
    "            ]\n",
    "            \n",
    "            for pattern in explicit_patterns:\n",
    "                match = re.search(pattern, response, re.IGNORECASE)\n",
    "                if match:\n",
    "                    answer = match.group(1).replace(',', '').strip()\n",
    "                    print(f\"[DEBUG ANSWER EXTRACT] ✓ Found explicit answer: {answer}\")\n",
    "                    return answer\n",
    "            \n",
    "            # Strategy 2: CONCLUSION section\n",
    "            conclusion_match = re.search(\n",
    "                r'CONCLUSION:\\s*(.+?)(?:\\n\\n|VERIFICATION|$)', \n",
    "                response, re.IGNORECASE | re.DOTALL\n",
    "            )\n",
    "            if conclusion_match:\n",
    "                conclusion_text = conclusion_match.group(1)\n",
    "                # Extract first number from conclusion\n",
    "                num_match = re.search(r'\\$?(\\d+(?:,\\d{3})*(?:\\.\\d+)?)', conclusion_text)\n",
    "                if num_match:\n",
    "                    answer = num_match.group(1).replace(',', '')\n",
    "                    print(f\"[DEBUG ANSWER EXTRACT] ✓ Found in conclusion: {answer}\")\n",
    "                    return answer\n",
    "            \n",
    "            # Strategy 3: Look for \"= NUMBER\" in last 5 lines\n",
    "            lines = response.split('\\n')\n",
    "            for line in reversed(lines[-5:]):\n",
    "                calc_match = re.search(r'=\\s*\\$?(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*$', line)\n",
    "                if calc_match:\n",
    "                    answer = calc_match.group(1).replace(',', '')\n",
    "                    print(f\"[DEBUG ANSWER EXTRACT] ✓ Extracted from calculation: {answer}\")\n",
    "                    return answer\n",
    "            \n",
    "            # Strategy 4: Last number in response (last resort)\n",
    "            last_third = response[int(len(response) * 0.7):]\n",
    "            numbers = re.findall(r'\\$?(\\d+(?:,\\d{3})*(?:\\.\\d+)?)', last_third)\n",
    "            if numbers:\n",
    "                answer = numbers[-1].replace(',', '')\n",
    "                print(f\"[DEBUG ANSWER EXTRACT] ⚠ Using last number: {answer}\")\n",
    "                return answer\n",
    "            \n",
    "            print(f\"[DEBUG ANSWER EXTRACT] ✗ No numerical answer found\")\n",
    "            return None\n",
    "        \n",
    "        # ========================================================================\n",
    "        # COMMONSENSE/OTHER QUESTION HANDLING (ORIGINAL CODE PRESERVED)\n",
    "        # ========================================================================\n",
    "        \n",
    "        # Strategy 1: Look for \"ANSWER: X: text\" format\n",
    "        answer_patterns = [\n",
    "            r'ANSWER:\\s*([A-E]):\\s*(.+?)(?:\\n\\n|CONCLUSION|VERIFICATION|$)',\n",
    "            r'ANSWER:\\s*\\*?\\*?([A-E])\\s*:\\s*(.+?)(?:\\n\\n|$)',\n",
    "            r'##?\\s*ANSWER:?\\s*([A-E]):\\s*(.+?)(?:\\n|$)',\n",
    "        ]\n",
    "        \n",
    "        for pattern in answer_patterns:\n",
    "            match = re.search(pattern, response, re.IGNORECASE | re.DOTALL)\n",
    "            if match:\n",
    "                letter = match.group(1).upper()\n",
    "                text = match.group(2).strip()\n",
    "                text = re.sub(r'\\*\\*|__|`', '', text)\n",
    "                answer = f\"{letter}: {text}\"\n",
    "                print(f\"[DEBUG ANSWER EXTRACT] ✓ Found via pattern: {answer[:60]}...\")\n",
    "                return answer\n",
    "        \n",
    "        # Strategy 2: CONCLUSION section for choice answers\n",
    "        conclusion_patterns = [\n",
    "            r'CONCLUSION:\\s*([A-E]):\\s*(.+?)(?:\\n\\n|$)',\n",
    "            r'CONCLUSION:\\s*\\*?\\*?([A-E])\\s+(?:is|are|would be)\\s+(.+?)(?:\\n|$)',\n",
    "        ]\n",
    "        \n",
    "        for pattern in conclusion_patterns:\n",
    "            match = re.search(pattern, response, re.IGNORECASE | re.DOTALL)\n",
    "            if match:\n",
    "                conclusion = f\"{match.group(1)}: {match.group(2)}\"\n",
    "                conclusion = re.sub(r'\\*\\*|__|`', '', conclusion)\n",
    "                print(f\"[DEBUG ANSWER EXTRACT] ✓ Found in conclusion: {conclusion[:60]}...\")\n",
    "                return conclusion\n",
    "        \n",
    "        print(f\"[DEBUG ANSWER EXTRACT] ✗ No answer found\")\n",
    "        return None\n",
    "\n",
    "    def _extract_mathematical_answer(self, response: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        NEW METHOD: Extract numerical answers from mathematical reasoning.\n",
    "        Tries multiple strategies with robust fallbacks.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Strategy 1: Explicit ANSWER markers (highest priority)\n",
    "        explicit_patterns = [\n",
    "            # \"ANSWER: 42\" or \"ANSWER = 42\"\n",
    "            r'ANSWER\\s*[:\\=]\\s*\\$?(\\d+(?:,\\d{3})*(?:\\.\\d+)?)',\n",
    "            \n",
    "            # LaTeX boxed format: \\boxed{42}\n",
    "            r'\\\\boxed\\{([^}]+)\\}',\n",
    "            \n",
    "            # \"Final Answer: 42\" or \"The answer is 42\"\n",
    "            r'(?:final\\s+answer|the\\s+answer)(?:\\s+is|\\s*:|\\s*=)\\s*\\$?(\\d+(?:,\\d{3})*(?:\\.\\d+)?)',\n",
    "            \n",
    "            # \"Therefore 42\" or \"Thus 42\"\n",
    "            r'(?:therefore|thus|hence)\\s*,?\\s*\\$?(\\d+(?:,\\d{3})*(?:\\.\\d+)?)',\n",
    "        ]\n",
    "        \n",
    "        for pattern in explicit_patterns:\n",
    "            match = re.search(pattern, response, re.IGNORECASE)\n",
    "            if match:\n",
    "                answer = match.group(1).strip()\n",
    "                # Clean formatting\n",
    "                answer = answer.replace(',', '')  # Remove thousand separators\n",
    "                print(f\"[DEBUG MATH EXTRACT] ✓ Found explicit answer: {answer}\")\n",
    "                return answer\n",
    "        \n",
    "        # Strategy 2: Number with units (common in word problems)\n",
    "        unit_patterns = [\n",
    "            r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s+(?:dollars?|pounds?|miles?|hours?|minutes?|days?|weeks?|items?|bags?|people|students?|plants?|eggs?|toys?|books?|candies?|apples?)',\n",
    "            r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s+(?:is|are|will be|remain|left|total)',\n",
    "        ]\n",
    "        \n",
    "        # Search in last 30% of response (where answers typically appear)\n",
    "        last_portion = response[int(len(response) * 0.7):]\n",
    "        \n",
    "        for pattern in unit_patterns:\n",
    "            matches = list(re.finditer(pattern, last_portion, re.IGNORECASE))\n",
    "            if matches:\n",
    "                # Take the LAST match (most likely to be final answer)\n",
    "                answer = matches[-1].group(1).replace(',', '')\n",
    "                print(f\"[DEBUG MATH EXTRACT] ✓ Found answer with units: {answer}\")\n",
    "                return answer\n",
    "        \n",
    "        # Strategy 3: Look in CONCLUSION section specifically\n",
    "        conclusion_match = re.search(r'CONCLUSION:(.+?)(?:\\n\\n|$)', response, re.IGNORECASE | re.DOTALL)\n",
    "        if conclusion_match:\n",
    "            conclusion_text = conclusion_match.group(1)\n",
    "            # Find any number in conclusion\n",
    "            num_match = re.search(r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)', conclusion_text)\n",
    "            if num_match:\n",
    "                answer = num_match.group(1).replace(',', '')\n",
    "                print(f\"[DEBUG MATH EXTRACT] ✓ Found in conclusion: {answer}\")\n",
    "                return answer\n",
    "        \n",
    "        # Strategy 4: Extract from calculations (FALLBACK)\n",
    "        # Look for \"= NUMBER\" in last 5 lines\n",
    "        lines = response.split('\\n')\n",
    "        for line in reversed(lines[-5:]):\n",
    "            calc_match = re.search(r'=\\s*\\$?(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*$', line)\n",
    "            if calc_match:\n",
    "                answer = calc_match.group(1).replace(',', '')\n",
    "                print(f\"[DEBUG MATH EXTRACT] ✓ Extracted from calculation: {answer}\")\n",
    "                return answer\n",
    "        \n",
    "        # Strategy 5: Last number mentioned (LAST RESORT)\n",
    "        # Find all numbers in last 30% of response\n",
    "        numbers = re.findall(r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)', last_portion)\n",
    "        if numbers:\n",
    "            answer = numbers[-1].replace(',', '')\n",
    "            print(f\"[DEBUG MATH EXTRACT] ⚠ Using last number: {answer}\")\n",
    "            return answer\n",
    "        \n",
    "        print(f\"[DEBUG MATH EXTRACT] ✗ No numerical answer found\")\n",
    "        return None\n",
    "\n",
    "    def _extract_reasoning_steps(self, response, strategy, question_type) -> List['LogicalStep']:\n",
    "        \"\"\"\n",
    "        FIXED: Properly extracts reasoning steps from markdown-heavy responses.\n",
    "        Handles headers, bold text, and nested formatting.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n[DEBUG STEP EXTRACT] Strategy: {strategy}\")\n",
    "        \n",
    "        steps = []\n",
    "        reasoning_text = \"\"\n",
    "        \n",
    "        # Try to locate reasoning sections\n",
    "        section_patterns = [\n",
    "            (r'SOLUTION:(.*?)(?=ANSWER:|CONCLUSION:|VERIFICATION:|$)', 'SOLUTION'),\n",
    "            (r'ANALYSIS:(.*?)(?=ANSWER:|CONCLUSION:|$)', 'ANALYSIS'),\n",
    "            (r'REASONING:(.*?)(?=ANSWER:|CONCLUSION:|$)', 'REASONING'),\n",
    "        ]\n",
    "        \n",
    "        for pattern, section_name in section_patterns:\n",
    "            match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "            if match:\n",
    "                reasoning_text = match.group(1)\n",
    "                print(f\"[DEBUG STEP EXTRACT] Found {section_name} section\")\n",
    "                break\n",
    "        \n",
    "        # Fallback: use entire response\n",
    "        if not reasoning_text:\n",
    "            reasoning_text = response\n",
    "            print(f\"[DEBUG STEP EXTRACT] Using full response\")\n",
    "        \n",
    "        # Split into lines and clean\n",
    "        lines = [l.strip() for l in reasoning_text.split('\\n') if l.strip()]\n",
    "        \n",
    "        # Enhanced patterns that handle your actual output\n",
    "        patterns = [\n",
    "            # Markdown headers: ## Step 1: Content\n",
    "            (r'^##?\\s*[Ss]tep\\s+(\\d+)[:\\.\\-]?\\s*(.+)', 'Markdown Header'),\n",
    "            \n",
    "            # Bold headers: **Step 1:** Content\n",
    "            (r'^\\*\\*[Ss]tep\\s+(\\d+)[:\\.\\-]?\\*\\*\\s*(.+)', 'Bold Header'),\n",
    "            \n",
    "            # Simple: Step 1: Content\n",
    "            (r'^[Ss]tep\\s+(\\d+)[:\\.\\-]\\s*(.+)', 'Simple Step'),\n",
    "            \n",
    "            # Markdown subheaders: ### Content\n",
    "            (r'^###\\s+(.+)', 'Subheader'),\n",
    "            \n",
    "            # Numbered lists: 1. Content or 1) Content\n",
    "            (r'^(\\d+)[\\.\\)]\\s+(.+)', 'Numbered'),\n",
    "            \n",
    "            # Action verbs at start (your actual format)\n",
    "            (r'^(Analyze|Evaluate|Identify|Determine|Select|Compare|Calculate|Solve|Define|Explain)\\b\\s*[:\\-]?\\s*(.+)', 'Action Verb'),\n",
    "            \n",
    "            # Bold keywords: **Key Insight** or **Supporting Evidence**\n",
    "            (r'^\\*\\*([A-Z][a-zA-Z\\s]+)\\*\\*\\s*(.+)', 'Bold Keyword'),\n",
    "        ]\n",
    "        \n",
    "        step_count = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            # Skip very short lines\n",
    "            if len(line) < 15:\n",
    "                continue\n",
    "            \n",
    "            # Skip lines that are just headers without content\n",
    "            if line.startswith('##') and len(line) < 30:\n",
    "                continue\n",
    "            \n",
    "            matched = False\n",
    "            \n",
    "            for pattern, pattern_name in patterns:\n",
    "                match = re.match(pattern, line, re.IGNORECASE)\n",
    "                if match:\n",
    "                    groups = match.groups()\n",
    "                    \n",
    "                    # Extract content (varies by pattern)\n",
    "                    if pattern_name in ['Markdown Header', 'Bold Header', 'Simple Step', 'Numbered']:\n",
    "                        content = groups[-1].strip()  # Last group is content\n",
    "                    elif pattern_name == 'Subheader':\n",
    "                        content = groups[0].strip()\n",
    "                    elif pattern_name in ['Action Verb', 'Bold Keyword']:\n",
    "                        # Combine action/keyword with content\n",
    "                        content = f\"{groups[0]}: {groups[1]}\" if len(groups) > 1 else groups[0]\n",
    "                    else:\n",
    "                        content = groups[-1].strip()\n",
    "                    \n",
    "                    # Remove extra markdown\n",
    "                    content = re.sub(r'\\*\\*|__|`', '', content)\n",
    "                    \n",
    "                    # Must be substantial\n",
    "                    if len(content) < 20:\n",
    "                        continue\n",
    "                    \n",
    "                    # Detect math\n",
    "                    is_math = (\n",
    "                        '=' in content or\n",
    "                        any(op in content.lower() for op in\n",
    "                            ['divide', 'multiply', 'subtract', 'add', 'sum',\n",
    "                            'calculate', 'solve', 'substitute', 'factor',\n",
    "                            'simplify', 'expand', 'equation', 'formula'])\n",
    "                    )\n",
    "                    \n",
    "                    # Dynamic confidence\n",
    "                    confidence = self.confidence_assessor.assess_step_confidence(\n",
    "                        content, is_math, question_type\n",
    "                    )\n",
    "                    \n",
    "                    step = LogicalStep(\n",
    "                        id=f\"{strategy}_step_{step_count+1}\",\n",
    "                        operation=self._classify_operation(content),\n",
    "                        content=content,\n",
    "                        confidence=confidence,\n",
    "                        is_mathematical=is_math\n",
    "                    )\n",
    "                    \n",
    "                    # 🔥 MATH VERIFICATION (only for mathematical questions)\n",
    "                    if is_math and question_type == QuestionType.MATHEMATICAL:\n",
    "                        calculations = self.math_verifier.extract_calculations(content)\n",
    "                        if calculations:\n",
    "                            all_valid = True\n",
    "                            for calc in calculations:\n",
    "                                is_valid, feedback, correct_val = self.math_verifier.verify_calculation(calc)\n",
    "                                if not is_valid:\n",
    "                                    all_valid = False\n",
    "                                    step.validation_status = ValidationStatus.INVALID\n",
    "                                    step.validation_feedback = feedback\n",
    "                                    step.confidence *= 0.5  # Reduce confidence for failed calc\n",
    "                                    print(f\"[MATH VERIFY] ❌ {feedback}\")\n",
    "                                    break\n",
    "                            \n",
    "                            if all_valid:\n",
    "                                step.calculation_verified = True\n",
    "                                step.confidence = min(step.confidence * 1.1, 0.95)  # Boost verified steps\n",
    "                                print(f\"[MATH VERIFY] ✓ All calculations verified\")\n",
    "                    \n",
    "                    steps.append(step)  # ✅ ONLY APPEND ONCE!\n",
    "                    step_count += 1\n",
    "                    matched = True\n",
    "                    \n",
    "                    print(f\"[DEBUG STEP EXTRACT] ✓ {pattern_name}: {content[:50]}...\")\n",
    "                    break\n",
    "            \n",
    "            # If no pattern matched but line looks like reasoning content\n",
    "            if not matched and len(line) > 40:\n",
    "                # Check if it's a meaningful sentence (has verb)\n",
    "                if any(word in line.lower() for word in ['is', 'are', 'has', 'have', 'consists', 'includes', 'contains']):\n",
    "                    content = re.sub(r'\\*\\*|__|`', '', line)\n",
    "                    \n",
    "                    step = LogicalStep(\n",
    "                        id=f\"{strategy}_step_{step_count+1}\",\n",
    "                        operation=LogicalOperation.INFERENCE,\n",
    "                        content=content,\n",
    "                        confidence=0.70,\n",
    "                        is_mathematical=False\n",
    "                    )\n",
    "                    steps.append(step)\n",
    "                    step_count += 1\n",
    "                    print(f\"[DEBUG STEP EXTRACT] ✓ Freeform: {content[:50]}...\")\n",
    "        \n",
    "        print(f\"[DEBUG STEP EXTRACT] Total steps found: {len(steps)}\")\n",
    "        return steps\n",
    "\n",
    "    def _calculate_dynamic_confidence(self, steps: List['LogicalStep'], response: str, \n",
    "                                    answer: Optional[str] = None) -> float:\n",
    "        \"\"\"\n",
    "        FIXED: Better confidence calculation that doesn't undervalue good answers.\n",
    "        \"\"\"\n",
    "        \n",
    "        # If we have steps, use their confidence\n",
    "        if steps and len(steps) > 0:\n",
    "            avg_step_confidence = sum(s.confidence for s in steps) / len(steps)\n",
    "            \n",
    "            # Bonus for having an answer\n",
    "            if answer and answer not in [\"Unable to determine\", \"\"]:\n",
    "                avg_step_confidence += 0.10\n",
    "            \n",
    "            # Penalize uncertainty\n",
    "            uncertainty_count = sum(1 for word in ['might', 'maybe', 'possibly', 'unclear', 'uncertain']\n",
    "                                if word in response.lower())\n",
    "            avg_step_confidence -= (uncertainty_count * 0.05)\n",
    "            \n",
    "            return min(max(avg_step_confidence, 0.30), 0.95)\n",
    "        \n",
    "        # No steps but we have an answer - assess answer quality\n",
    "        if answer and answer not in [\"Unable to determine\", \"\"]:\n",
    "            return self._assess_answer_quality(answer, response)\n",
    "        \n",
    "        # No steps and no answer - but check if response is substantial\n",
    "        if len(response) > 200:\n",
    "            return 0.55  # Some reasoning present\n",
    "        \n",
    "        return 0.30  # Minimal confidence\n",
    "\n",
    "    def _assess_answer_quality(self, answer: str, response: str) -> float:\n",
    "        \"\"\"\n",
    "        FIXED: Better answer quality assessment.\n",
    "        \"\"\"\n",
    "        base_confidence = 0.60  # Start higher (was 0.5)\n",
    "        \n",
    "        # Length and detail\n",
    "        if len(answer) > 50:\n",
    "            base_confidence += 0.12\n",
    "        elif len(answer) > 20:\n",
    "            base_confidence += 0.08\n",
    "        \n",
    "        # Response length indicates reasoning depth\n",
    "        if len(response) > 800:\n",
    "            base_confidence += 0.10\n",
    "        elif len(response) > 400:\n",
    "            base_confidence += 0.05\n",
    "        \n",
    "        # Definitive language\n",
    "        definitive_words = ['therefore', 'thus', 'must', 'always', 'is', 'are',\n",
    "                        'the answer', 'conclusion', 'result', 'standard']\n",
    "        definitive_count = sum(1 for word in definitive_words \n",
    "                            if word in response.lower())\n",
    "        base_confidence += min(definitive_count * 0.03, 0.12)\n",
    "        \n",
    "        # Uncertainty penalty\n",
    "        uncertainty_words = ['might', 'possibly', 'approximately', 'roughly', \n",
    "                            'likely', 'probably', 'seems', 'appears']\n",
    "        uncertainty_count = sum(1 for word in uncertainty_words \n",
    "                            if word in response.lower())\n",
    "        base_confidence -= uncertainty_count * 0.04\n",
    "        \n",
    "        # Evidence markers\n",
    "        evidence_words = ['evidence', 'research', 'data', 'fact', 'proven', \n",
    "                        'verified', 'confirmed', 'universal', 'standard']\n",
    "        evidence_count = sum(1 for word in evidence_words \n",
    "                            if word in response.lower())\n",
    "        base_confidence += min(evidence_count * 0.025, 0.08)\n",
    "        \n",
    "        return min(max(base_confidence, 0.35), 0.90)\n",
    "\n",
    "    def _determine_consensus_verdict(self, verdicts: List[str], paths: List[ReasoningPath]) -> str:\n",
    "        \"\"\"\n",
    "        IMPROVED: Use confidence weighting + prefer most specific answer.\n",
    "        \"\"\"\n",
    "        if not verdicts:\n",
    "            return \"Unable to determine\"\n",
    "        \n",
    "        # Extract letters from verdicts (handle \"A: text\" format)\n",
    "        clean_verdicts = []\n",
    "        for v in verdicts:\n",
    "            letter_match = re.match(r'^([A-E])', v)\n",
    "            if letter_match:\n",
    "                clean_verdicts.append(letter_match.group(1))\n",
    "            else:\n",
    "                clean_verdicts.append(v)\n",
    "        \n",
    "        # Count with confidence weighting\n",
    "        weighted_votes = {}\n",
    "        for i, verdict in enumerate(clean_verdicts):\n",
    "            if i < len(paths):\n",
    "                weight = paths[i].confidence\n",
    "            else:\n",
    "                weight = 0.5\n",
    "            weighted_votes[verdict] = weighted_votes.get(verdict, 0) + weight\n",
    "        \n",
    "        # Get winner\n",
    "        winner = max(weighted_votes.items(), key=lambda x: x[1])\n",
    "        verdict_letter = winner[0]\n",
    "        total_weight = sum(weighted_votes.values())\n",
    "        \n",
    "        # Find full text from original verdicts\n",
    "        full_text = None\n",
    "        for v in verdicts:\n",
    "            if v.startswith(verdict_letter):\n",
    "                full_text = v\n",
    "                break\n",
    "        \n",
    "        if not full_text:\n",
    "            full_text = verdict_letter\n",
    "        \n",
    "        # Determine consensus level\n",
    "        if len(set(clean_verdicts)) == 1:\n",
    "            return f\"{full_text} (unanimous)\"\n",
    "        elif winner[1] / total_weight > 0.6:\n",
    "            return f\"{full_text} (strong consensus)\"\n",
    "        else:\n",
    "            return f\"{full_text} (weak consensus)\"\n",
    "    \n",
    "    def _classify_operation(self, content: str) -> LogicalOperation:\n",
    "        \"\"\"Classify the logical type of a reasoning step (premise, inference, etc.).\"\"\"\n",
    "        content_lower = content.lower()\n",
    "        if any(word in content_lower for word in ['evidence', 'research', 'data']):\n",
    "            return LogicalOperation.EVIDENCE\n",
    "        elif any(word in content_lower for word in ['therefore', 'thus', 'implies']):\n",
    "            return LogicalOperation.INFERENCE\n",
    "        elif any(word in content_lower for word in ['however', 'but', 'although']):\n",
    "            return LogicalOperation.COUNTERARGUMENT\n",
    "        else:\n",
    "            return LogicalOperation.PREMISE\n",
    "    \n",
    "    def _extract_conclusion(self, response: str, verdict_or_answer: Optional[str]) -> str:\n",
    "        \"\"\"Extract the conclusion section or fallback to verdict/answer.\"\"\"\n",
    "        conclusion_match = re.search(r'CONCLUSION:\\s*(.+)', response, re.IGNORECASE | re.DOTALL)\n",
    "        if conclusion_match:\n",
    "            conclusion = conclusion_match.group(1).strip().split('\\n')[0]\n",
    "            if verdict_or_answer:\n",
    "                return f\"{verdict_or_answer}. {conclusion}\"\n",
    "            return conclusion\n",
    "        \n",
    "        if verdict_or_answer:\n",
    "            return f\"Answer: {verdict_or_answer}\"\n",
    "        \n",
    "        return \"See reasoning above\"\n",
    "\n",
    "    def _create_fallback_path(self, query: str,\n",
    "                            classification: QuestionClassification = None) -> ReasoningPath:\n",
    "        \"\"\"Fallback path used when all generation attempts fail.\"\"\"\n",
    "        return ReasoningPath(\n",
    "            path_id=f\"fallback_{int(time.time())}\",\n",
    "            query=query,\n",
    "            verdict=\"UNCLEAR\" if classification and classification.question_type == QuestionType.BINARY else None,\n",
    "            answer=\"Unable to determine\",\n",
    "            steps=[],\n",
    "            conclusion=\"Generation failed\",\n",
    "            confidence=0.2,\n",
    "            generation_strategy=\"fallback\",\n",
    "            question_type=classification.question_type if classification else QuestionType.BINARY,\n",
    "            complexity_level=classification.complexity_level if classification else ComplexityLevel.MODERATE\n",
    "        )\n",
    "    \n",
    "    def get_total_cost(self) -> float:\n",
    "            \"\"\"Estimate total Claude API cost based on token usage.\"\"\"\n",
    "            input_cost = (self.total_tokens_input / 1_000_000) * 3.0\n",
    "            output_cost = (self.total_tokens_output / 1_000_000) * 15.0\n",
    "            return input_cost + output_cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfb88605",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SYNTHESIZER - WITH VALIDATION, REGENERATION, DIVERGENCE DETECTION & SPECIFICITY\n",
    "# ============================================================================\n",
    "# This class combines multiple reasoning paths into one synthesized final answer.\n",
    "# Features:\n",
    "# - Validation of reasoning steps with regeneration\n",
    "# - Numerical divergence detection and consensus building\n",
    "# - Specificity-based answer selection\n",
    "# - Confidence-based synthesis logic\n",
    "# ============================================================================\n",
    "\n",
    "import re\n",
    "import time\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import anthropic\n",
    "\n",
    "\n",
    "class SpecificityScorer:\n",
    "    \"\"\"\n",
    "    Score answer specificity to prefer concrete answers over abstract ones.\n",
    "    Example: \"singing\" (specific) > \"making music\" (generic)\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def score_specificity(answer_text: str, question: str) -> float:\n",
    "        \"\"\"\n",
    "        AGGRESSIVE specificity scoring for CommonsenseQA.\n",
    "        \n",
    "        Philosophy:\n",
    "        - SHORTER is usually MORE specific (\"singing\" > \"making music\")\n",
    "        - CONCRETE actions/nouns > abstract concepts\n",
    "        - DIRECT terms > generic categories\n",
    "        - Single words > phrases (usually)\n",
    "        \n",
    "        Higher score = more specific answer\n",
    "        \"\"\"\n",
    "        score = 0.5  # baseline\n",
    "        \n",
    "        answer_lower = answer_text.lower().strip()\n",
    "        answer_len = len(answer_text)\n",
    "        \n",
    "        # ========== Rule 1: LENGTH (CRITICAL) ==========\n",
    "        # Shorter answers are typically more specific in CommonsenseQA\n",
    "        if answer_len <= 8:  # Single short word (e.g., \"singing\", \"torn\")\n",
    "            score += 0.30\n",
    "        elif answer_len <= 15:  # Short phrase or compound word\n",
    "            score += 0.20\n",
    "        elif answer_len <= 25:  # Medium phrase\n",
    "            score += 0.10\n",
    "        elif answer_len > 40:  # Long explanatory answers\n",
    "            score -= 0.15\n",
    "        \n",
    "        # ========== Rule 2: CONCRETE ACTION VERBS (HIGH PRIORITY) ==========\n",
    "        # Specific actions are highly preferred\n",
    "        concrete_actions = {\n",
    "            # From your examples\n",
    "            'singing', 'torn', 'walk', 'disturb', 'attention',\n",
    "            # Common specific actions\n",
    "            'walking', 'running', 'reading', 'writing', 'eating',\n",
    "            'drinking', 'talking', 'listening', 'watching', 'playing',\n",
    "            'building', 'cooking', 'cleaning', 'studying', 'sleeping',\n",
    "            'dancing', 'swimming', 'driving', 'crying', 'laughing',\n",
    "            'screaming', 'whispering', 'jumping', 'sitting', 'standing'\n",
    "        }\n",
    "        if any(action in answer_lower for action in concrete_actions):\n",
    "            score += 0.35  # Very high bonus for concrete actions\n",
    "        \n",
    "        # ========== Rule 3: ABSTRACT/GENERIC TERMS (HEAVY PENALTY) ==========\n",
    "        # Generic verbs and phrases are red flags\n",
    "        abstract_terms = {\n",
    "            # Generic verbs\n",
    "            'making', 'doing', 'having', 'being', 'getting', 'going',\n",
    "            'coming', 'taking', 'giving', 'putting', 'becoming',\n",
    "            # Generic nouns\n",
    "            'thing', 'stuff', 'something', 'anything', 'everything',\n",
    "            # Generic descriptors\n",
    "            'general', 'various', 'some', 'any', 'all',\n",
    "            # Generic phrases from your data\n",
    "            'live in', 'work in', 'be in', 'go to'\n",
    "        }\n",
    "        abstract_count = sum(1 for term in abstract_terms if term in answer_lower)\n",
    "        if abstract_count > 0:\n",
    "            score -= 0.30 * abstract_count  # Compound penalty\n",
    "        \n",
    "        # ========== Rule 4: SPECIFIC NOUNS (BONUS) ==========\n",
    "        # Concrete, named things are more specific than categories\n",
    "        specific_nouns = {\n",
    "            # Locations\n",
    "            'bank', 'library', 'hospital', 'school', 'office',\n",
    "            'restaurant', 'store', 'park', 'theater', 'gym',\n",
    "            # Objects\n",
    "            'refrigerator', 'oven', 'desk', 'chair', 'door',\n",
    "            'window', 'table', 'bed', 'car', 'phone',\n",
    "            # Body parts (specific)\n",
    "            'hand', 'foot', 'eye', 'ear', 'mouth', 'nose'\n",
    "        }\n",
    "        if any(noun in answer_lower for noun in specific_nouns):\n",
    "            score += 0.25\n",
    "        \n",
    "        # ========== Rule 5: GENERIC CATEGORIES (PENALTY) ==========\n",
    "        # Category words indicate lack of specificity\n",
    "        generic_categories = {\n",
    "            'place', 'location', 'area', 'region', 'spot', 'site',\n",
    "            'thing', 'item', 'object', 'article',\n",
    "            'activity', 'action', 'process', 'method', 'way',\n",
    "            'type', 'kind', 'sort', 'form', 'category',\n",
    "            'person', 'people', 'individual', 'someone'\n",
    "        }\n",
    "        if any(cat in answer_lower for cat in generic_categories):\n",
    "            score -= 0.25\n",
    "        \n",
    "        # ========== Rule 6: \"-ING\" GERUNDS (CONTEXT-DEPENDENT) ==========\n",
    "        # \"singing\" is specific, but \"making music\" uses generic \"making\"\n",
    "        if answer_lower.endswith('ing'):\n",
    "            # Check if it's a concrete action (already scored above) or generic verb\n",
    "            if not any(action in answer_lower for action in concrete_actions):\n",
    "                # It's a generic gerund like \"making\", \"doing\"\n",
    "                score -= 0.10\n",
    "        \n",
    "        # ========== Rule 7: MULTI-WORD PHRASES (SUSPICIOUS) ==========\n",
    "        # More words often = less specific in CommonsenseQA\n",
    "        word_count = len(answer_text.split())\n",
    "        if word_count >= 3:\n",
    "            score -= 0.10 * (word_count - 2)  # Escalating penalty\n",
    "        \n",
    "        # ========== Rule 8: ARTICLES & PREPOSITIONS (OFTEN LESS SPECIFIC) ==========\n",
    "        # \"the bank\" might be less specific than just \"bank\" in some contexts\n",
    "        if any(answer_lower.startswith(article) for article in ['a ', 'an ', 'the ']):\n",
    "            score -= 0.05\n",
    "        \n",
    "        # ========== Rule 9: COMPOUND SPECIFICITY BOOST ==========\n",
    "        # Single concrete word = maximum specificity\n",
    "        if (word_count == 1 and \n",
    "            answer_len <= 10 and\n",
    "            any(action in answer_lower for action in concrete_actions | specific_nouns)):\n",
    "            score += 0.20  # Extra bonus for perfect specificity\n",
    "        \n",
    "        # ========== Rule 10: QUESTION CONTEXT (OPTIONAL) ==========\n",
    "        # If question asks \"where\", prefer specific locations\n",
    "        # If question asks \"what do\", prefer specific actions\n",
    "        if question:\n",
    "            q_lower = question.lower()\n",
    "            if 'where' in q_lower and any(noun in answer_lower for noun in specific_nouns):\n",
    "                score += 0.10\n",
    "            elif ('what do' in q_lower or 'what are' in q_lower) and any(action in answer_lower for action in concrete_actions):\n",
    "                score += 0.10\n",
    "        \n",
    "        # Clamp to valid range\n",
    "        return max(0.0, min(1.0, score))\n",
    "\n",
    "class AnswerSynthesizer:\n",
    "    def __init__(self, client: anthropic.Anthropic, model: str, \n",
    "                 strategy: Optional[ReasoningStrategy] = None):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.strategy = strategy  # NEWf\n",
    "        self.total_tokens_input = 0\n",
    "        self.total_tokens_output = 0\n",
    "        self.specificity_scorer = SpecificityScorer()\n",
    "    \n",
    "    def synthesize_final_answer(self, query: str, paths: List['ReasoningPath'],\n",
    "                            classification: 'QuestionClassification') -> 'SynthesizedAnswer':\n",
    "        \"\"\"\n",
    "        UPDATED: Strategy-based synthesis with backward compatibility.\n",
    "        Preserves all existing validation, divergence detection, and verification logic.\n",
    "        \"\"\"\n",
    "        \n",
    "        # ========================================================================\n",
    "        # STEP 1: VALIDATION (unchanged - runs regardless of strategy)\n",
    "        # ========================================================================\n",
    "        if classification.requires_validation:\n",
    "            print(f\"\\n[1.5] Validating paths (complexity: {classification.complexity_level.value})...\")\n",
    "            paths = self._validate_and_regenerate_paths(paths, classification)\n",
    "        \n",
    "        # ========================================================================\n",
    "        # STEP 2: DIVERGENCE DETECTION (unchanged)\n",
    "        # ========================================================================\n",
    "        print(f\"\\n[1.6] Checking for numerical divergence...\")\n",
    "        divergence_detected, divergent_pair = self._detect_numerical_divergence(paths)\n",
    "        \n",
    "        if divergence_detected and divergent_pair:\n",
    "            print(f\"\\n[DIVERGENCE] Detected: {divergent_pair[0].generation_strategy} vs {divergent_pair[1].generation_strategy}\")\n",
    "            consensus_path = self._regenerate_for_consensus(query, divergent_pair, classification)\n",
    "            if consensus_path:\n",
    "                paths.append(consensus_path)\n",
    "                print(f\"[CONSENSUS] Path added ({len(paths)} total paths now)\")\n",
    "        \n",
    "        # ========================================================================\n",
    "        # STEP 3: ANSWER EXTRACTION (for mathematical questions)\n",
    "        # ========================================================================\n",
    "        if classification.question_type == QuestionType.MATHEMATICAL:\n",
    "            print(f\"\\n[1.7] Extracting mathematical answers from paths...\")\n",
    "            \n",
    "            # Collect all answers from paths\n",
    "            extracted_answers = []\n",
    "            for i, path in enumerate(paths):\n",
    "                answer = None\n",
    "                \n",
    "                # Method 1: Use stored answer\n",
    "                if path.answer and path.answer != \"Unable to determine\":\n",
    "                    answer = path.answer\n",
    "                    print(f\"  Path {i+1}: Used stored answer: {answer}\")\n",
    "                \n",
    "                # Method 2: Re-extract from raw output\n",
    "                elif path.raw_output:\n",
    "                    answer = self._extract_answer_from_raw_output(path.raw_output)\n",
    "                    if answer:\n",
    "                        path.answer = answer  # Update the path\n",
    "                        print(f\"  Path {i+1}: Re-extracted: {answer}\")\n",
    "                \n",
    "                # Method 3: Extract from last step\n",
    "                elif path.steps and len(path.steps) > 0:\n",
    "                    last_step = path.steps[-1]\n",
    "                    num_match = re.search(r'=\\s*\\$?(\\d+(?:,\\d{3})*(?:\\.\\d+)?)', last_step.content)\n",
    "                    if num_match:\n",
    "                        answer = num_match.group(1).replace(',', '')\n",
    "                        path.answer = answer\n",
    "                        print(f\"  Path {i+1}: Extracted from last step: {answer}\")\n",
    "                \n",
    "                if answer:\n",
    "                    extracted_answers.append(answer)\n",
    "                else:\n",
    "                    print(f\"  Path {i+1}: ✗ No answer found\")\n",
    "            \n",
    "            # Update answers list for synthesis\n",
    "            if extracted_answers:\n",
    "                print(f\"\\n[1.8] Found {len(extracted_answers)} answers: {extracted_answers}\")\n",
    "        \n",
    "        # ========================================================================\n",
    "        # STEP 4: ANSWER SYNTHESIS (NEW: Strategy-based with fallback)\n",
    "        # ========================================================================\n",
    "        \n",
    "        # NEW: Use strategy if provided\n",
    "        if self.strategy is not None:\n",
    "            print(f\"\\n[SYNTHESIS] Using strategy: {self.strategy.__class__.__name__}\")\n",
    "            answers = [p.answer for p in paths if p.answer and p.answer != \"Unable to determine\"]\n",
    "            \n",
    "            if answers:\n",
    "                definitive_answer = self.strategy.select_final_answer(answers, paths)\n",
    "                answer_format = \"verdict\" if classification.question_type == QuestionType.BINARY else \"answer\"\n",
    "            else:\n",
    "                print(f\"[SYNTHESIS] No valid answers found, using fallback\")\n",
    "                definitive_answer = \"Unable to determine\"\n",
    "                answer_format = \"answer\"\n",
    "        \n",
    "        # OLD PATH: Original synthesis logic (backward compatible)\n",
    "        else:\n",
    "            print(f\"\\n[SYNTHESIS] Using legacy synthesis\")\n",
    "            \n",
    "            if classification.question_type == QuestionType.BINARY:\n",
    "                verdicts = [p.verdict for p in paths if p.verdict]\n",
    "                definitive_answer = self._determine_consensus_verdict(verdicts, paths)\n",
    "                answer_format = \"verdict\"\n",
    "            \n",
    "            elif classification.question_type == QuestionType.MATHEMATICAL:\n",
    "                # Use extracted answers from step 3\n",
    "                answers = [p.answer for p in paths if p.answer and p.answer != \"Unable to determine\"]\n",
    "                if answers:\n",
    "                    definitive_answer = self._select_best_numerical_answer(answers, paths)\n",
    "                else:\n",
    "                    definitive_answer = \"Unable to determine answer\"\n",
    "                answer_format = \"answer\"\n",
    "            \n",
    "            else:\n",
    "                # CommonsenseQA and others\n",
    "                answers = [p.answer for p in paths if p.answer]\n",
    "                if answers:\n",
    "                    definitive_answer = self._synthesize_answers_with_specificity(answers, paths)\n",
    "                else:\n",
    "                    definitive_answer = \"Unable to determine\"\n",
    "                answer_format = \"answer\"\n",
    "        \n",
    "        # ========================================================================\n",
    "        # STEP 5: MATHEMATICAL VERIFICATION (unchanged)\n",
    "        # ========================================================================\n",
    "        confidence_multiplier = 1.0  # Default: no adjustment\n",
    "        \n",
    "        if classification.question_type == QuestionType.MATHEMATICAL:\n",
    "            print(f\"\\n[VERIFICATION] Verifying mathematical answer...\")\n",
    "            \n",
    "            # Extract ALL calculations from ALL paths\n",
    "            math_verifier = MathematicalVerifier()\n",
    "            all_calculations = []\n",
    "            \n",
    "            for path in paths:\n",
    "                for step in path.steps:\n",
    "                    if step.is_mathematical:\n",
    "                        calcs = math_verifier.extract_calculations(step.content)\n",
    "                        all_calculations.extend(calcs)\n",
    "            \n",
    "            print(f\"[VERIFICATION] Found {len(all_calculations)} total calculation(s) across all paths\")\n",
    "            \n",
    "            # Verify each calculation\n",
    "            failed_calcs = []\n",
    "            verified_count = 0\n",
    "            \n",
    "            for calc in all_calculations:\n",
    "                is_valid, feedback, correct_val = math_verifier.verify_calculation(calc)\n",
    "                if not is_valid:\n",
    "                    failed_calcs.append(feedback)\n",
    "                else:\n",
    "                    verified_count += 1\n",
    "            \n",
    "            # Adjust confidence based on verification results\n",
    "            if failed_calcs:\n",
    "                print(f\"[VERIFICATION] Found {len(failed_calcs)} calculation error(s):\")\n",
    "                for error in failed_calcs[:3]:\n",
    "                    print(f\"  • {error}\")\n",
    "                \n",
    "                failure_rate = len(failed_calcs) / len(all_calculations) if all_calculations else 0\n",
    "                \n",
    "                if failure_rate > 0.5:\n",
    "                    confidence_multiplier = 0.5\n",
    "                elif failure_rate > 0.25:\n",
    "                    confidence_multiplier = 0.7\n",
    "                else:\n",
    "                    confidence_multiplier = 0.85\n",
    "                \n",
    "                print(f\"[VERIFICATION] Confidence reduced by {(1-confidence_multiplier)*100:.0f}%\")\n",
    "            else:\n",
    "                print(f\"[VERIFICATION] All {len(all_calculations)} calculation(s) verified\")\n",
    "                confidence_multiplier = 1.1  # Boost for perfection\n",
    "        \n",
    "        # ========================================================================\n",
    "        # STEP 6: SUPPORTING INFORMATION (unchanged)\n",
    "        # ========================================================================\n",
    "        supporting_reasoning = self._extract_key_points(paths)\n",
    "        conflicting_points = self._identify_conflicts(paths)\n",
    "        synthesis_explanation = self._generate_synthesis_explanation(\n",
    "            query, paths, definitive_answer, conflicting_points\n",
    "        )\n",
    "        \n",
    "        # ========================================================================\n",
    "        # STEP 7: CONFIDENCE CALCULATION (unchanged)\n",
    "        # ========================================================================\n",
    "        base_confidence = self._calculate_synthesis_confidence(paths, conflicting_points)\n",
    "        \n",
    "        if classification.question_type == QuestionType.MATHEMATICAL:\n",
    "            final_confidence = min(base_confidence * confidence_multiplier, 0.95)\n",
    "            print(f\"[VERIFICATION] Final confidence: {base_confidence:.2f} → {final_confidence:.2f}\")\n",
    "        else:\n",
    "            final_confidence = base_confidence\n",
    "        \n",
    "        # ========================================================================\n",
    "        # STEP 8: RETURN SYNTHESIZED ANSWER (unchanged)\n",
    "        # ========================================================================\n",
    "        return SynthesizedAnswer(\n",
    "            query=query,\n",
    "            definitive_answer=definitive_answer,\n",
    "            supporting_reasoning=supporting_reasoning,\n",
    "            conflicting_points=conflicting_points,\n",
    "            final_confidence=final_confidence,\n",
    "            synthesis_explanation=synthesis_explanation,\n",
    "            question_type=classification.question_type,\n",
    "            answer_format=answer_format\n",
    "        )\n",
    "    # =========================================================================\n",
    "    # VALIDATION & REGENERATION\n",
    "    # =========================================================================\n",
    "    def _validate_and_regenerate_paths(self, paths: List['ReasoningPath'],\n",
    "                                    classification: 'QuestionClassification') -> List['ReasoningPath']:\n",
    "        \"\"\"\n",
    "        FIXED: Force validation for mathematical questions regardless of confidence.\n",
    "        \"\"\"\n",
    "        validated_paths = []\n",
    "        total_validations = 0\n",
    "        total_regenerations = 0\n",
    "        \n",
    "        for path_idx, path in enumerate(paths):\n",
    "            print(f\"  Validating path {path_idx + 1}/{len(paths)} ({path.generation_strategy})...\")\n",
    "            \n",
    "            # Skip empty paths\n",
    "            if not path.steps:\n",
    "                validated_paths.append(path)\n",
    "                continue\n",
    "            \n",
    "            needs_regeneration = False\n",
    "            failed_step_idx = -1\n",
    "            \n",
    "            # NEW: Force validation for math questions\n",
    "            force_validation = (classification.question_type == QuestionType.MATHEMATICAL)\n",
    "            \n",
    "            # Validate each step within the path\n",
    "            for step_idx, step in enumerate(path.steps):\n",
    "                # MODIFIED: Skip confidence check if force_validation is True\n",
    "                if not force_validation and step.confidence >= classification.confidence_threshold:\n",
    "                    step.validation_status = ValidationStatus.VALID\n",
    "                    continue\n",
    "                \n",
    "                previous_steps = path.steps[:step_idx]\n",
    "                validation_result = self._validate_step(\n",
    "                    step=step,\n",
    "                    previous_steps=previous_steps,\n",
    "                    query=path.query,\n",
    "                    question_type=classification.question_type\n",
    "                )\n",
    "                \n",
    "                path.validation_passes += 1\n",
    "                total_validations += 1\n",
    "                \n",
    "                # Update step validation outcome\n",
    "                if validation_result['is_valid']:\n",
    "                    step.validation_status = ValidationStatus.VALID\n",
    "                    step.validation_feedback = validation_result['feedback']\n",
    "                    step.confidence = max(step.confidence, validation_result['confidence'])\n",
    "                else:\n",
    "                    # Mark step invalid and flag regeneration\n",
    "                    step.validation_status = ValidationStatus.INVALID\n",
    "                    step.validation_feedback = validation_result['feedback']\n",
    "                    needs_regeneration = True\n",
    "                    failed_step_idx = step_idx\n",
    "                    print(f\"    ✗ Step {step_idx + 1} failed validation\")\n",
    "                    break\n",
    "            \n",
    "            # Attempt regeneration if a failure occurred\n",
    "            if needs_regeneration and failed_step_idx >= 0:\n",
    "                print(f\"    ↻ Regenerating from step {failed_step_idx + 1}...\")\n",
    "                regenerated_path = self._regenerate_from_failed_step(\n",
    "                    path=path,\n",
    "                    failed_step_idx=failed_step_idx,\n",
    "                    classification=classification\n",
    "                )\n",
    "                \n",
    "                if regenerated_path:\n",
    "                    path = regenerated_path\n",
    "                    path.regeneration_count += 1\n",
    "                    total_regenerations += 1\n",
    "                    print(f\"    ✓ Regeneration successful\")\n",
    "                else:\n",
    "                    print(f\"    ✗ Regeneration failed, keeping original\")\n",
    "            \n",
    "            validated_paths.append(path)\n",
    "        \n",
    "        print(f\"  Validation complete: {total_validations} checks, {total_regenerations} regenerations\")\n",
    "        return validated_paths    \n",
    "\n",
    "    def _validate_step(self, step: 'LogicalStep', previous_steps: List['LogicalStep'],\n",
    "                      query: str, question_type: 'QuestionType') -> Dict:\n",
    "        \"\"\"Validate a single reasoning step using the model, with math verification.\"\"\"\n",
    "        \n",
    "        # 🔥 MATH VERIFICATION FIRST (before expensive LLM call)\n",
    "        if step.is_mathematical and question_type == QuestionType.MATHEMATICAL:\n",
    "            math_verifier = MathematicalVerifier()\n",
    "            calculations = math_verifier.extract_calculations(step.content)\n",
    "            \n",
    "            if calculations:\n",
    "                print(f\"[MATH VALIDATE] Found {len(calculations)} calculation(s) to verify\")\n",
    "                for calc in calculations:\n",
    "                    is_valid, feedback, correct_val = math_verifier.verify_calculation(calc)\n",
    "                    if not is_valid:\n",
    "                        print(f\"[MATH VALIDATE] ❌ {feedback}\")\n",
    "                        return {\n",
    "                            'is_valid': False,\n",
    "                            'confidence': 0.3,\n",
    "                            'feedback': f\"Math error: {feedback}\"\n",
    "                        }\n",
    "                \n",
    "                # All calculations valid\n",
    "                print(f\"[MATH VALIDATE] ✓ All {len(calculations)} calculation(s) verified\")\n",
    "                return {\n",
    "                    'is_valid': True,\n",
    "                    'confidence': 0.9,\n",
    "                    'feedback': \"✓ All mathematical calculations verified\"\n",
    "                }\n",
    "        \n",
    "        # 🔥 FALLBACK: LLM validation (for non-math or steps without extractable calculations)\n",
    "        context = f\"Original Question: {query}\\n\\n\"\n",
    "        \n",
    "        if previous_steps:\n",
    "            context += \"Previous reasoning steps:\\n\"\n",
    "            for i, prev_step in enumerate(previous_steps, 1):\n",
    "                context += f\"{i}. {prev_step.content}\\n\"\n",
    "            context += \"\\n\"\n",
    "        \n",
    "        context += f\"Step to validate:\\n{step.content}\"\n",
    "        \n",
    "        # Adjust validation prompt depending on whether the step is mathematical\n",
    "        if step.is_mathematical:\n",
    "            prompt = f\"\"\"{context}\n",
    "\n",
    "Validate this mathematical step. Check:\n",
    "1. Is the arithmetic/algebra correct?\n",
    "2. Does it follow logically from previous steps?\n",
    "3. Are there any calculation errors?\n",
    "\n",
    "Respond EXACTLY:\n",
    "VALID: YES or NO\n",
    "CONFIDENCE: 0.0 to 1.0\n",
    "FEEDBACK: Brief explanation\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"{context}\n",
    "\n",
    "Validate this reasoning step. Check:\n",
    "1. Does it logically follow from previous steps?\n",
    "2. Is it factually accurate?\n",
    "3. Is it relevant to answering the question?\n",
    "\n",
    "Respond EXACTLY:\n",
    "VALID: YES or NO\n",
    "CONFIDENCE: 0.0 to 1.0\n",
    "FEEDBACK: Brief explanation\"\"\"\n",
    "        \n",
    "        # Send validation prompt to the Claude model\n",
    "        try:\n",
    "            message = self.client.messages.create(\n",
    "                model=self.model,\n",
    "                max_tokens=300,\n",
    "                temperature=0.2,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            \n",
    "            response = message.content[0].text\n",
    "            self.total_tokens_input += message.usage.input_tokens\n",
    "            self.total_tokens_output += message.usage.output_tokens\n",
    "            \n",
    "            # Parse structured response\n",
    "            is_valid = 'VALID: YES' in response.upper()\n",
    "            confidence_match = re.search(r'CONFIDENCE:\\s*(0?\\.\\d+|1\\.0)', response)\n",
    "            confidence = float(confidence_match.group(1)) if confidence_match else 0.7\n",
    "            feedback_match = re.search(r'FEEDBACK:\\s*(.+?)(?:\\n|$)', response, re.DOTALL)\n",
    "            feedback = feedback_match.group(1).strip() if feedback_match else response[:100]\n",
    "            \n",
    "            return {'is_valid': is_valid, 'confidence': confidence, 'feedback': feedback}\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Fallback: mark as valid but low confidence if validation fails\n",
    "            return {'is_valid': True, 'confidence': 0.6, 'feedback': f\"Validation error: {str(e)[:50]}\"}\n",
    "    \n",
    "    def _regenerate_from_failed_step(self, path: 'ReasoningPath', failed_step_idx: int,\n",
    "                                    classification: 'QuestionClassification') -> Optional['ReasoningPath']:\n",
    "        \"\"\"Regenerate reasoning path after a failed step using alternative approach.\"\"\"\n",
    "        valid_steps = path.steps[:failed_step_idx]\n",
    "        failed_step = path.steps[failed_step_idx]\n",
    "        \n",
    "        # Build context including validated steps and failure reason\n",
    "        context = f\"Question: {path.query}\\n\\n\"\n",
    "        if valid_steps:\n",
    "            context += \"These steps are correct:\\n\"\n",
    "            for i, step in enumerate(valid_steps, 1):\n",
    "                context += f\"{i}. {step.content}\\n\"\n",
    "            context += \"\\n\"\n",
    "        \n",
    "        context += f\"This step FAILED validation:\\n{failed_step.content}\\n\"\n",
    "        context += f\"Reason: {failed_step.validation_feedback}\\n\\n\"\n",
    "        \n",
    "        # Adjust regeneration strategy based on question type\n",
    "        if classification.question_type == QuestionType.MATHEMATICAL:\n",
    "            if 'algebraic' in path.generation_strategy.lower():\n",
    "                approach = \"Try NUMERICAL approach instead. Calculate with actual numbers.\"\n",
    "            else:\n",
    "                approach = \"Try ALGEBRAIC approach instead. Use equations and variables.\"\n",
    "        else:\n",
    "            approach = \"Try a completely different reasoning approach.\"\n",
    "        \n",
    "        prompt = f\"\"\"{context}{approach}\n",
    "\n",
    "Continue solving from where valid steps ended. Show clear reasoning.\n",
    "\n",
    "Format:\n",
    "Step {failed_step_idx + 1}: [new step]\n",
    "Step {failed_step_idx + 2}: [next step]\n",
    "...\n",
    "ANSWER: [final answer]\"\"\"\n",
    "        \n",
    "        # Generate new reasoning continuation\n",
    "        try:\n",
    "            message = self.client.messages.create(\n",
    "                model=self.model,\n",
    "                max_tokens=1000,\n",
    "                temperature=0.7,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            \n",
    "            response = message.content[0].text\n",
    "            self.total_tokens_input += message.usage.input_tokens\n",
    "            self.total_tokens_output += message.usage.output_tokens\n",
    "            \n",
    "            # Extract new reasoning steps and answer\n",
    "            new_steps = self._extract_reasoning_steps_from_text(\n",
    "                response, f\"{path.generation_strategy}_regen\", classification.question_type\n",
    "            )\n",
    "            \n",
    "            if not new_steps:\n",
    "                return None\n",
    "            \n",
    "            combined_steps = valid_steps + new_steps\n",
    "            new_answer = self._extract_answer_from_text(response, classification.question_type)\n",
    "            \n",
    "            # Construct updated path object\n",
    "            return ReasoningPath(\n",
    "                path_id=path.path_id,\n",
    "                query=path.query,\n",
    "                verdict=None if classification.question_type != QuestionType.BINARY else new_answer,\n",
    "                answer=new_answer if classification.question_type != QuestionType.BINARY else None,\n",
    "                steps=combined_steps,\n",
    "                conclusion=f\"Regenerated: {new_answer}\" if new_answer else \"Regenerated\",\n",
    "                confidence=self._calculate_confidence_from_steps(new_answer, combined_steps, response),\n",
    "                generation_strategy=f\"{path.generation_strategy}_regenerated\",\n",
    "                raw_output=response,\n",
    "                question_type=path.question_type,\n",
    "                complexity_level=path.complexity_level,\n",
    "                validation_passes=path.validation_passes,\n",
    "                regeneration_count=path.regeneration_count\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ⚠ Regeneration error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # =========================================================================\n",
    "    # NUMERICAL DIVERGENCE DETECTION & CONSENSUS\n",
    "    # =========================================================================\n",
    "    \n",
    "    def _detect_numerical_divergence(self, paths: List['ReasoningPath']) -> Tuple[bool, Optional[Tuple['ReasoningPath', 'ReasoningPath', float]]]:\n",
    "        \"\"\"\n",
    "        Detect when paths have significant numerical disagreement (>10%).\n",
    "        \n",
    "        Returns:\n",
    "            (divergence_detected, (path1, path2, pct_diff)) or (False, None)\n",
    "        \"\"\"\n",
    "        numerical_answers = []\n",
    "        \n",
    "        for path in paths:\n",
    "            if path.answer:\n",
    "                nums = self._extract_numerical_values(path.answer.lower())\n",
    "                if nums and len(nums) > 0:\n",
    "                    # Take the primary (largest) number if multiple exist\n",
    "                    primary_num = max(nums) if nums else None\n",
    "                    if primary_num is not None:\n",
    "                        numerical_answers.append((path, primary_num))\n",
    "        \n",
    "        # Need at least 2 numerical answers to compare\n",
    "        if len(numerical_answers) < 2:\n",
    "            return False, None\n",
    "        \n",
    "        # Check all pairs for divergence\n",
    "        for i, (path1, num1) in enumerate(numerical_answers):\n",
    "            for path2, num2 in numerical_answers[i+1:]:\n",
    "                if num1 > 0 and num2 > 0:\n",
    "                    pct_diff = abs(num1 - num2) / max(num1, num2)\n",
    "                    \n",
    "                    # Threshold: >10% difference triggers regeneration\n",
    "                    if pct_diff > 0.10:\n",
    "                        print(f\"  ⚠️ DIVERGENCE DETECTED: {num1:.3f} vs {num2:.3f} ({pct_diff*100:.1f}% diff)\")\n",
    "                        return True, (path1, path2, pct_diff)\n",
    "        \n",
    "        return False, None\n",
    "\n",
    "    def _regenerate_for_consensus(self, \n",
    "                                query: str,\n",
    "                                divergent_paths: Tuple['ReasoningPath', 'ReasoningPath', float],\n",
    "                                classification: 'QuestionClassification') -> Optional['ReasoningPath']:\n",
    "        \"\"\"\n",
    "        When numerical answers diverge, generate a new path using a different approach.\n",
    "        \n",
    "        This uses alternative question framing or decomposition to find the truth.\n",
    "        \"\"\"\n",
    "        path1, path2, pct_diff = divergent_paths\n",
    "        \n",
    "        print(f\"\\n  [REGENERATION] Attempting consensus path...\")\n",
    "        print(f\"  Path 1: {path1.generation_strategy} → {path1.answer}\")\n",
    "        print(f\"  Path 2: {path2.generation_strategy} → {path2.answer}\")\n",
    "        \n",
    "        # Build a regeneration prompt that explicitly asks for verification\n",
    "        prompt = f\"\"\"Question: {query}\n",
    "\n",
    "Two independent approaches gave different numerical answers:\n",
    "- Approach 1 ({path1.generation_strategy}): {path1.answer}\n",
    "- Approach 2 ({path2.generation_strategy}): {path2.answer}\n",
    "\n",
    "These differ by {pct_diff*100:.1f}%. \n",
    "\n",
    "Please solve this problem from scratch using a THIRD distinct approach. \n",
    "Be extremely careful with all calculations. Show every step explicitly.\n",
    "\n",
    "Format your response as:\n",
    "\n",
    "APPROACH: [Name of third method]\n",
    "\n",
    "SOLUTION:\n",
    "Step 1: [First step]\n",
    "Step 2: [Second step]\n",
    "Step 3: [Continue...]\n",
    "\n",
    "ANSWER: [Final numerical answer with units]\n",
    "\n",
    "VERIFICATION: [Double-check your calculation]\"\"\"\n",
    "        \n",
    "        try:\n",
    "            message = self.client.messages.create(\n",
    "                model=self.model,\n",
    "                max_tokens=1500,\n",
    "                temperature=0.5,  # Lower temperature for consistency\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            \n",
    "            response_text = message.content[0].text\n",
    "            self.total_tokens_input += message.usage.input_tokens\n",
    "            self.total_tokens_output += message.usage.output_tokens\n",
    "            \n",
    "            # Extract answer from regenerated response\n",
    "            answer = self._extract_answer_from_regeneration(response_text)\n",
    "            \n",
    "            if answer:\n",
    "                # Create new path for this consensus attempt\n",
    "                new_path = ReasoningPath(\n",
    "                    path_id=f\"consensus_{int(time.time()*1000)}\",\n",
    "                    query=query,\n",
    "                    answer=answer,\n",
    "                    steps=[],  # Steps not extracted for brevity\n",
    "                    conclusion=f\"Consensus answer after divergence detection\",\n",
    "                    confidence=0.72,  # Moderate confidence for consensus attempts\n",
    "                    generation_strategy=\"consensus_verification\",\n",
    "                    raw_output=response_text,\n",
    "                    question_type=classification.question_type,\n",
    "                    complexity_level=classification.complexity_level,\n",
    "                )\n",
    "                \n",
    "                print(f\"  ✓ Consensus path generated: {answer}\")\n",
    "                return new_path\n",
    "            else:\n",
    "                print(f\"  ✗ Could not extract answer from consensus path\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Regeneration error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _extract_answer_from_regeneration(self, response: str) -> Optional[str]:\n",
    "        \"\"\"Extract answer from regeneration response.\"\"\"\n",
    "        # Try ANSWER: format first\n",
    "        answer_match = re.search(\n",
    "            r'ANSWER:\\s*\\*?\\*?(.+?)(?:\\n\\n|VERIFICATION|$)',\n",
    "            response, re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "        \n",
    "        if answer_match:\n",
    "            answer = answer_match.group(1).strip()\n",
    "            answer = re.sub(r'^\\*\\*|\\*\\*$', '', answer).strip()\n",
    "            if answer and len(answer) > 3:\n",
    "                return answer\n",
    "        \n",
    "        return None\n",
    "\n",
    "    # =========================================================================\n",
    "    # SYNTHESIS WITH SPECIFICITY SCORING\n",
    "    # =========================================================================\n",
    "    \n",
    "    def _synthesize_answers_with_specificity(self, answers: List[str], \n",
    "                                            paths: List['ReasoningPath']) -> str:\n",
    "        \"\"\"\n",
    "        IMPROVED: Better tie-breaking for close calls\n",
    "        \"\"\"\n",
    "        if not answers:\n",
    "            return \"Unable to determine answer\"\n",
    "        \n",
    "        answer_groups = self._group_equivalent_answers(answers)\n",
    "        \n",
    "        if len(answer_groups) == 1:\n",
    "            return f\"{answers[0]} (unanimous)\"\n",
    "        \n",
    "        # NEW: For CommonsenseQA, heavily weight specificity\n",
    "        best_answer = None\n",
    "        best_score = -1\n",
    "        \n",
    "        for i, path in enumerate(paths):\n",
    "            if not path.answer or i >= len(answers):\n",
    "                continue\n",
    "            \n",
    "            conf_score = path.confidence\n",
    "            spec_score = self.specificity_scorer.score_specificity(path.answer, path.query)\n",
    "            \n",
    "            # FIXED: Weight 70% confidence, 30% specificity for CommonsenseQA\n",
    "            if path.question_type == QuestionType.COMMONSENSE:\n",
    "                combined = 0.70 * conf_score + 0.30 * spec_score\n",
    "            else:\n",
    "                combined = 0.70 * conf_score + 0.30 * spec_score\n",
    "            \n",
    "            print(f\"  Path {i+1}: conf={conf_score:.2f}, spec={spec_score:.2f}, combined={combined:.2f}, answer='{path.answer[:40]}'\")\n",
    "            \n",
    "            if combined > best_score:\n",
    "                best_score = combined\n",
    "                best_answer = path.answer\n",
    "        \n",
    "        if best_answer:\n",
    "            return f\"{best_answer} (best, score={best_score:.2f})\"\n",
    "        \n",
    "        return f\"{paths[0].answer} (fallback)\"\n",
    "\n",
    "    def _answers_equivalent(self, ans1: str, ans2: str) -> bool:\n",
    "        \"\"\"Check if two answers are meaningfully equivalent.\n",
    "        \n",
    "        Tries multiple strategies in order:\n",
    "        1. Direct string match\n",
    "        2. Numerical comparison (with tolerance)\n",
    "        3. Math variable matching (x=5 style)\n",
    "        4. Substring matching for short answers\n",
    "        5. Word overlap for text answers\n",
    "        \"\"\"\n",
    "        # Strip markdown\n",
    "        clean1 = re.sub(r'\\*\\*|__|`|~~', '', ans1).lower().strip()\n",
    "        clean2 = re.sub(r'\\*\\*|__|`|~~', '', ans2).lower().strip()\n",
    "        \n",
    "        # Strategy 1: Direct match after cleaning\n",
    "        if clean1 == clean2:\n",
    "            return True\n",
    "        \n",
    "        # Strategy 2: Extract and compare numerical values (handles percentages)\n",
    "        numbers1 = self._extract_numerical_values(clean1)\n",
    "        numbers2 = self._extract_numerical_values(clean2)\n",
    "        \n",
    "        # If BOTH have numbers, use strict numerical comparison\n",
    "        if numbers1 and numbers2:\n",
    "            if self._numbers_equivalent(numbers1, numbers2):\n",
    "                return True\n",
    "            else:\n",
    "                # Numbers differ significantly → definitely not equivalent\n",
    "                return False\n",
    "        \n",
    "        # Strategy 3: Compare variable=value pairs in math expressions\n",
    "        solutions1 = set(re.findall(r'([a-z])\\s*=\\s*([-+]?\\d+\\.?\\d*)', clean1))\n",
    "        solutions2 = set(re.findall(r'([a-z])\\s*=\\s*([-+]?\\d+\\.?\\d*)', clean2))\n",
    "        if solutions1 and solutions2 and solutions1 == solutions2:\n",
    "            return True\n",
    "        \n",
    "        # Strategy 4: Substring check for short answers\n",
    "        # Handles \"2\" vs \"2 is the only...\"\n",
    "        if clean1 in clean2 or clean2 in clean1:\n",
    "            if len(clean1) < 20 or len(clean2) < 20:  # Short answer check\n",
    "                return True\n",
    "        \n",
    "        # Strategy 5: Word overlap ratio\n",
    "        # Only apply if NEITHER has strong numerical content\n",
    "        if not numbers1 and not numbers2:\n",
    "            words1 = set(clean1.split())\n",
    "            words2 = set(clean2.split())\n",
    "            if words1 and words2:\n",
    "                return len(words1 & words2) / len(words1 | words2) > 0.7\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def _group_equivalent_answers(self, answers: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Group answers that are semantically equivalent (even if text differs).\"\"\"\n",
    "        groups = []\n",
    "        used = set()\n",
    "        \n",
    "        for i, ans_i in enumerate(answers):\n",
    "            if i in used:\n",
    "                continue\n",
    "            \n",
    "            group = [ans_i]\n",
    "            used.add(i)\n",
    "            \n",
    "            for j, ans_j in enumerate(answers):\n",
    "                if j <= i or j in used:\n",
    "                    continue\n",
    "                \n",
    "                if self._answers_equivalent(ans_i, ans_j):\n",
    "                    group.append(ans_j)\n",
    "                    used.add(j)\n",
    "            \n",
    "            groups.append(group)\n",
    "        \n",
    "        return groups\n",
    "    \n",
    "    def _extract_numerical_values(self, text: str) -> list:\n",
    "        \"\"\"Extract all numerical values from text, including percentages.\"\"\"\n",
    "        numbers = []\n",
    "        \n",
    "        # Match percentages (88%, 71.2%)\n",
    "        percentage_matches = re.findall(r'([\\d.]+)\\s*%', text)\n",
    "        numbers.extend([float(p) / 100 for p in percentage_matches])\n",
    "        \n",
    "        # Match standalone numbers (including decimals)\n",
    "        # Avoid double-counting percentages already extracted\n",
    "        number_matches = re.findall(r'(?:^|\\s)([\\d.]+)(?:\\s|$|[^%\\d.])', text)\n",
    "        for n in number_matches:\n",
    "            if n not in percentage_matches:\n",
    "                try:\n",
    "                    numbers.append(float(n))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        \n",
    "        # Match fractions like \"99/100\"\n",
    "        fraction_matches = re.findall(r'(\\d+)/(\\d+)', text)\n",
    "        numbers.extend([float(n) / float(d) for n, d in fraction_matches])\n",
    "        \n",
    "        return sorted(set(numbers))\n",
    "\n",
    "    def _numbers_equivalent(self, nums1: list, nums2: list) -> bool:\n",
    "        \"\"\"Compare two lists of numbers with tolerance.\n",
    "        \n",
    "        Different number of values = different answers.\n",
    "        Large differences in values = not equivalent.\n",
    "        \"\"\"\n",
    "        if len(nums1) != len(nums2):\n",
    "            return False\n",
    "        \n",
    "        PERCENTAGE_TOLERANCE = 0.05  # 5 percentage points (88% vs 93% = different)\n",
    "        RELATIVE_TOLERANCE = 0.1      # 10% relative tolerance for other numbers\n",
    "        \n",
    "        for n1, n2 in zip(nums1, nums2):\n",
    "            # For values in 0-1 range (likely percentages)\n",
    "            if 0 <= n1 <= 1 and 0 <= n2 <= 1:\n",
    "                if abs(n1 - n2) > PERCENTAGE_TOLERANCE:\n",
    "                    return False\n",
    "            else:\n",
    "                # For larger numbers, use relative tolerance\n",
    "                max_allowed_diff = max(RELATIVE_TOLERANCE * max(abs(n1), abs(n2)), 0.01)\n",
    "                if abs(n1 - n2) > max_allowed_diff:\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    # =========================================================================\n",
    "    # EXTRACTION & CONFIDENCE HELPERS\n",
    "    # =========================================================================\n",
    "    \n",
    "    def _extract_mathematical_answer_enhanced(self, response: str, paths: List['ReasoningPath']) -> str:\n",
    "        \"\"\"\n",
    "        ENHANCED: Extract answers from complex math problems including:\n",
    "        - Coordinate pairs: (3, π/2)\n",
    "        - Symbolic expressions: p - q, 2q - 3p\n",
    "        - Fractions: 14/3\n",
    "        - Mixed notation: boxed{...}, $...$\n",
    "        \"\"\"\n",
    "        \n",
    "        # Strategy 1: LaTeX boxed answers (highest priority)\n",
    "        boxed_match = re.search(r'\\\\boxed\\{([^}]+)\\}', response)\n",
    "        if boxed_match:\n",
    "            answer = boxed_match.group(1).strip()\n",
    "            print(f\"[MATH EXTRACT] ✓ Found boxed answer: {answer}\")\n",
    "            return self._clean_math_notation(answer)\n",
    "        \n",
    "        # Strategy 2: Coordinate pairs (r, θ) or (x, y)\n",
    "        coord_patterns = [\n",
    "            r'\\((\\d+(?:\\.\\d+)?)\\s*,\\s*\\\\frac\\{\\\\pi\\}\\{(\\d+)\\}\\)',  # (3, π/2)\n",
    "            r'\\((\\d+(?:\\.\\d+)?)\\s*,\\s*\\\\pi/(\\d+)\\)',                # (3, π/2)\n",
    "            r'\\((\\d+(?:\\.\\d+)?)\\s*,\\s*([^\\)]+)\\)',                  # (3, 1.57)\n",
    "        ]\n",
    "        \n",
    "        for pattern in coord_patterns:\n",
    "            match = re.search(pattern, response)\n",
    "            if match:\n",
    "                answer = f\"({match.group(1)}, {match.group(2)})\"\n",
    "                print(f\"[MATH EXTRACT] ✓ Found coordinate: {answer}\")\n",
    "                return answer\n",
    "        \n",
    "        # Strategy 3: Symbolic expressions (p - q, 2q, etc.)\n",
    "        symbolic_patterns = [\n",
    "            r'(?:answer|result|equals?|is)\\s*(?:is|=)?\\s*([pq]\\s*[-+]\\s*[pq])',  # p - q\n",
    "            r'(?:answer|result|equals?|is)\\s*(?:is|=)?\\s*(\\d+[pq])',              # 2q\n",
    "            r'(?:answer|result|equals?|is)\\s*(?:is|=)?\\s*([pq]/\\d+)',             # q/2\n",
    "        ]\n",
    "        \n",
    "        for pattern in symbolic_patterns:\n",
    "            match = re.search(pattern, response, re.IGNORECASE)\n",
    "            if match:\n",
    "                answer = match.group(1).strip()\n",
    "                print(f\"[MATH EXTRACT] ✓ Found symbolic: {answer}\")\n",
    "                return answer\n",
    "        \n",
    "        # Strategy 4: Fractions (14/3, 5/2)\n",
    "        fraction_pattern = r'(?:answer|result|equals?|is)\\s*(?:is|=)?\\s*(\\d+/\\d+)'\n",
    "        fraction_match = re.search(fraction_pattern, response, re.IGNORECASE)\n",
    "        if fraction_match:\n",
    "            answer = fraction_match.group(1)\n",
    "            print(f\"[MATH EXTRACT] ✓ Found fraction: {answer}\")\n",
    "            return answer\n",
    "        \n",
    "        # Strategy 5: Extract from CONCLUSION section\n",
    "        conclusion_match = re.search(r'CONCLUSION:(.+?)(?:\\n\\n|$)', response, re.IGNORECASE | re.DOTALL)\n",
    "        if conclusion_match:\n",
    "            conclusion_text = conclusion_match.group(1)\n",
    "            \n",
    "            # Try all extraction methods on conclusion\n",
    "            for pattern in coord_patterns + symbolic_patterns + [fraction_pattern]:\n",
    "                match = re.search(pattern, conclusion_text)\n",
    "                if match:\n",
    "                    answer = match.group(1) if len(match.groups()) == 1 else match.group(0)\n",
    "                    print(f\"[MATH EXTRACT] ✓ Found in conclusion: {answer}\")\n",
    "                    return self._clean_math_notation(answer)\n",
    "        \n",
    "        # Strategy 6: Fallback to calculations in reasoning steps\n",
    "        print(f\"[MATH EXTRACT] ⚠️ Using fallback: extracting from reasoning steps\")\n",
    "        return self._extract_from_reasoning_calculations(paths)\n",
    "\n",
    "    def _clean_math_notation(self, text: str) -> str:\n",
    "        \"\"\"Clean LaTeX and math notation from extracted answers\"\"\"\n",
    "        # Remove LaTeX commands\n",
    "        text = re.sub(r'\\\\(frac|pi|theta|cdot|times|left|right)', '', text)\n",
    "        # \n",
    "        # Remove extra spaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        # Remove dollar signs\n",
    "        text = text.replace('$', '')\n",
    "        return text\n",
    "\n",
    "    def _extract_from_reasoning_calculations(self, paths: List['ReasoningPath']) -> str:\n",
    "        \"\"\"\n",
    "        Fallback: Extract answer from verified calculations in reasoning steps\n",
    "        \"\"\"\n",
    "        for path in paths:\n",
    "            for step in path.steps:\n",
    "                if step.calculation_verified and step.is_mathematical:\n",
    "                    # Look for \"= result\" in the step content\n",
    "                    equals_match = re.search(r'=\\s*([^\\s,]+)(?:\\s|$)', step.content)\n",
    "                    if equals_match:\n",
    "                        result = equals_match.group(1)\n",
    "                        print(f\"[MATH EXTRACT] ✓ Extracted from verified step: {result}\")\n",
    "                        return result\n",
    "        \n",
    "        return \"Unable to determine answer\"\n",
    "    \n",
    "    def _extract_reasoning_steps_from_text(self, text: str, strategy: str,\n",
    "                                        question_type: 'QuestionType') -> List['LogicalStep']:\n",
    "        \"\"\"Extract structured reasoning steps from model-generated text.\"\"\"\n",
    "        steps = []\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            step_match = re.match(r'^(?:step\\s+)?(\\d+)[\\.\\):\\-]\\s*(.+)', line, re.IGNORECASE)\n",
    "            if step_match and len(step_match.group(2)) > 10:\n",
    "                content = step_match.group(2).strip()\n",
    "                is_math = '=' in content or any(op in content.lower()\n",
    "                                            for op in ['calculate', 'solve', 'divide'])\n",
    "                \n",
    "                step = LogicalStep(\n",
    "                    id=f\"{strategy}_step_{len(steps)+1}\",\n",
    "                    operation=LogicalOperation.INFERENCE,\n",
    "                    content=content,\n",
    "                    confidence=0.75,\n",
    "                    is_mathematical=is_math\n",
    "                )\n",
    "                steps.append(step)\n",
    "        \n",
    "        return steps\n",
    "    \n",
    "    def _extract_answer_from_text(self, text: str, question_type: 'QuestionType') -> Optional[str]:\n",
    "        \"\"\"Extract the final answer from regenerated text.\"\"\"\n",
    "        answer_match = re.search(r'ANSWER:\\s*(.+?)(?:\\n|$)', text, re.IGNORECASE)\n",
    "        if answer_match:\n",
    "            return answer_match.group(1).strip()\n",
    "        \n",
    "        # Fallback: last line may contain a mathematical result\n",
    "        lines = [l.strip() for l in text.split('\\n') if l.strip()]\n",
    "        if lines:\n",
    "            last_line = lines[-1]\n",
    "            if '=' in last_line:\n",
    "                return last_line\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _calculate_confidence_from_steps(self, answer: Optional[str],\n",
    "                                        steps: List['LogicalStep'], text: str) -> float:\n",
    "        \"\"\"Estimate path confidence based on number of valid steps and uncertainty.\"\"\"\n",
    "        base = 0.7\n",
    "        if answer:\n",
    "            base += 0.1\n",
    "        if len(steps) >= 3:\n",
    "            base += 0.05\n",
    "        uncertainty = sum(1 for word in ['might', 'maybe', 'possibly']\n",
    "                        if word in text.lower())\n",
    "        base -= uncertainty * 0.03\n",
    "        return min(max(base, 0.1), 0.95)\n",
    "    \n",
    "    def _determine_consensus_verdict(self, verdicts: List[str], paths: List['ReasoningPath']) -> str:\n",
    "        \"\"\"Aggregate binary verdicts into a consensus (unanimous, majority, or contested).\"\"\"\n",
    "        if not verdicts:\n",
    "            return \"Unable to determine\"\n",
    "        \n",
    "        verdict_counts = {}\n",
    "        for v in verdicts:\n",
    "            verdict_counts[v] = verdict_counts.get(v, 0) + 1\n",
    "        \n",
    "        verdict, count = max(verdict_counts.items(), key=lambda x: x[1])\n",
    "        \n",
    "        if count == len(verdicts):\n",
    "            return f\"{verdict} (unanimous)\"\n",
    "        elif count > len(verdicts) / 2:\n",
    "            return f\"{verdict} (majority {count}/{len(verdicts)})\"\n",
    "        else:\n",
    "            return f\"{verdict} (contested {count}/{len(verdicts)})\"\n",
    "    \n",
    "    def _extract_key_points(self, paths: List['ReasoningPath']) -> List[str]:\n",
    "        \"\"\"Collect key reasoning evidence and inference steps for synthesis summary.\"\"\"\n",
    "        key_points = []\n",
    "        for path in paths:\n",
    "            relevant = [s for s in path.steps if s.operation in [LogicalOperation.EVIDENCE, LogicalOperation.INFERENCE]]\n",
    "            for step in relevant[:2]:\n",
    "                key_points.append(f\"[{path.generation_strategy}] {step.content}\")\n",
    "        return key_points[:5]\n",
    "        \n",
    "    def _identify_conflicts(self, paths: List['ReasoningPath']) -> List[str]:\n",
    "        \"\"\"Identify disagreements between reasoning paths.\"\"\"\n",
    "        conflicts = []\n",
    "        \n",
    "        # For verdicts\n",
    "        verdicts = [p.verdict for p in paths if p.verdict]\n",
    "        if len(set(verdicts)) > 1:\n",
    "            conflicts.append(f\"Verdict disagreement: {', '.join(set(verdicts))}\")\n",
    "        \n",
    "        # For answers - USE PROPER NORMALIZATION\n",
    "        answers = [p.answer for p in paths if p.answer]\n",
    "        if len(answers) > 1:\n",
    "            answer_groups = self._group_equivalent_answers(answers)\n",
    "            if len(answer_groups) > 1:\n",
    "                conflicts.append(\"Answer disagreement\")\n",
    "        \n",
    "        return conflicts\n",
    "    \n",
    "    def _generate_synthesis_explanation(self, query: str, paths: List['ReasoningPath'],\n",
    "                                    answer: str, conflicts: List[str]) -> str:\n",
    "        \"\"\"Generate a readable summary explaining synthesis outcomes.\"\"\"\n",
    "        explanation = f\"Analyzed {len(paths)} independent reasoning approaches.\\n\"\n",
    "        if not conflicts:\n",
    "            explanation += \"All paths converged with consistent reasoning.\"\n",
    "        else:\n",
    "            explanation += f\"Found {len(conflicts)} disagreement(s).\"\n",
    "        return explanation\n",
    "        \n",
    "    def _extract_answer_from_raw_output(self, raw_output: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        NEW: Extract numerical answer from raw model output.\n",
    "        Used when path.answer is not populated.\n",
    "        \"\"\"\n",
    "        # Pattern 1: ANSWER: 42\n",
    "        answer_match = re.search(\n",
    "            r'ANSWER\\s*[:\\=]\\s*\\$?(\\d+(?:,\\d{3})*(?:\\.\\d+)?)',\n",
    "            raw_output, re.IGNORECASE\n",
    "        )\n",
    "        if answer_match:\n",
    "            return answer_match.group(1).replace(',', '')\n",
    "        \n",
    "        # Pattern 2: \\boxed{42}\n",
    "        boxed_match = re.search(r'\\\\boxed\\{(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\}', raw_output)\n",
    "        if boxed_match:\n",
    "            return boxed_match.group(1).replace(',', '')\n",
    "        \n",
    "        # Pattern 3: Last \"= NUMBER\" in response\n",
    "        lines = raw_output.split('\\n')\n",
    "        for line in reversed(lines[-5:]):\n",
    "            calc_match = re.search(r'=\\s*\\$?(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*$', line)\n",
    "            if calc_match:\n",
    "                return calc_match.group(1).replace(',', '')\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _select_best_numerical_answer(self, answers: List[str], \n",
    "                                 paths: List['ReasoningPath']) -> str:\n",
    "        \"\"\"\n",
    "        NEW: Select best numerical answer using majority vote + confidence weighting.\n",
    "        \"\"\"\n",
    "        if not answers:\n",
    "            return \"Unable to determine answer\"\n",
    "        \n",
    "        # Convert to numbers\n",
    "        numerical_answers = []\n",
    "        for i, ans in enumerate(answers):\n",
    "            # Extract number from answer string\n",
    "            num_match = re.search(r'(\\d+(?:\\.\\d+)?)', ans.replace(',', ''))\n",
    "            if num_match:\n",
    "                try:\n",
    "                    num = float(num_match.group(1))\n",
    "                    confidence = paths[i].confidence if i < len(paths) else 0.5\n",
    "                    numerical_answers.append((num, confidence, ans))\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        if not numerical_answers:\n",
    "            return answers[0]  # Fallback to first answer\n",
    "        \n",
    "        # Count occurrences with confidence weighting\n",
    "        vote_scores = {}\n",
    "        for num, conf, original in numerical_answers:\n",
    "            if num not in vote_scores:\n",
    "                vote_scores[num] = {'score': 0, 'count': 0, 'original': original}\n",
    "            vote_scores[num]['score'] += conf\n",
    "            vote_scores[num]['count'] += 1\n",
    "        \n",
    "        # Select answer with highest weighted score\n",
    "        best_num = max(vote_scores.items(), key=lambda x: (x[1]['count'], x[1]['score']))\n",
    "        result = best_num[1]['original']\n",
    "        \n",
    "        # Format result\n",
    "        if best_num[1]['count'] == len(numerical_answers):\n",
    "            return f\"{result} (unanimous)\"\n",
    "        elif best_num[1]['count'] > len(numerical_answers) / 2:\n",
    "            return f\"{result} (consensus {best_num[1]['count']}/{len(numerical_answers)})\"\n",
    "        else:\n",
    "            return f\"{result} (best of {best_num[1]['count']}/{len(numerical_answers)})\"\n",
    "            \n",
    "    def _calculate_synthesis_confidence(self, paths: List['ReasoningPath'], conflicts: List[str]) -> float:\n",
    "        \"\"\"Compute overall synthesis confidence, reducing it for conflicts.\"\"\"\n",
    "        avg_confidence = sum(p.confidence for p in paths) / len(paths) if paths else 0.5\n",
    "        conflict_penalty = len(conflicts) * 0.1\n",
    "        return min(max(avg_confidence - conflict_penalty, 0.15), 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48dc0f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MAIN SYSTEM\n",
    "# ============================================================================\n",
    "# This class ties together the entire reasoning pipeline:\n",
    "#   1. Classifies a question.\n",
    "#   2. Generates multiple reasoning paths using Claude.\n",
    "#   3. Validates and (if needed) regenerates reasoning steps.\n",
    "#   4. Synthesizes a final answer with confidence scoring.\n",
    "#   5. Displays results, including performance and validation metrics.\n",
    "# ============================================================================\n",
    "\n",
    "class ClaudeDynamicReasoningSystem:\n",
    "    \"\"\"High-level orchestrator for dynamic multi-path reasoning and synthesis using Claude.\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize the system with API key and required components.\"\"\"\n",
    "        print(\"Initializing Claude Dynamic Reasoning System...\")\n",
    "        self.generator = ClaudeReasoningGenerator(api_key)  # Generates reasoning paths\n",
    "        self.synthesizer = AnswerSynthesizer(self.generator.client, self.generator.model)  # Handles synthesis + validation\n",
    "        print(\"System ready - Claude 3.5 Sonnet with ALL 5 FIXES\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # MAIN ENTRYPOINT: Run full reasoning → synthesis pipeline\n",
    "    # ------------------------------------------------------------------------\n",
    "    def reason_with_synthesis(self, query: str, num_paths: int = 3) -> NegotiationResult:\n",
    "        \"\"\"\n",
    "        Run full reasoning workflow for a given query.\n",
    "\n",
    "        Steps:\n",
    "        1. Classify the question type and complexity.\n",
    "        2. Generate multiple reasoning paths (in parallel for speed).\n",
    "        3. Validate and synthesize a final answer.\n",
    "        4. Aggregate all results, including timing and cost stats.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"QUERY: {query}\")\n",
    "        print('='*70)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # --- STEP 0: Classify the question ---\n",
    "        print(\"\\n[0] Classifying question...\")\n",
    "        classification = self.generator.classifier.classify(query)\n",
    "        print(f\"Type: {classification.question_type.value}, Complexity: {classification.complexity_level.value}\")\n",
    "        print(f\"Validation: {classification.requires_validation}, Num paths: {classification.num_paths}\")\n",
    "\n",
    "        # --- STEP 1: Generate reasoning paths ---\n",
    "        print(f\"\\n[1] Generating reasoning paths...\")\n",
    "        paths, speedup = self.generator.generate_multiple_paths_parallel(query, num_paths, classification)\n",
    "\n",
    "        print(f\"Generated {len(paths)} path(s) ({speedup:.2f}x speedup):\")\n",
    "        for path in paths:\n",
    "            # Summarize each reasoning path\n",
    "            if path.verdict:\n",
    "                answer_str = f\"verdict={path.verdict}\"\n",
    "            elif path.answer:\n",
    "                answer_str = f\"answer={path.answer[:40]}...\"\n",
    "            else:\n",
    "                answer_str = \"no answer\"\n",
    "            print(f\"  • {path.generation_strategy}: {len(path.steps)} steps, {answer_str}, conf={path.confidence:.2f}\")\n",
    "\n",
    "        # --- STEP 2: Synthesize the final answer ---\n",
    "        print(\"\\n[2] Synthesizing final answer...\")\n",
    "        synthesized = self.synthesizer.synthesize_final_answer(query, paths, classification)\n",
    "\n",
    "        # --- STEP 3: Compute performance metrics ---\n",
    "        total_time = time.time() - start_time\n",
    "        total_cost = self.generator.get_total_cost()\n",
    "\n",
    "        print(f\"\\n[3] Complete in {total_time:.2f}s\")\n",
    "        print(f\"Answer: {synthesized.definitive_answer}\")\n",
    "        print(f\"Confidence: {synthesized.final_confidence:.2f}\")\n",
    "        print(f\"API Cost: ${total_cost:.4f}\")\n",
    "\n",
    "        # Collect validation + regeneration metrics\n",
    "        total_validations = sum(p.validation_passes for p in paths)\n",
    "        total_regenerations = sum(p.regeneration_count for p in paths)\n",
    "\n",
    "        # --- STEP 4: Bundle everything into a NegotiationResult ---\n",
    "        return NegotiationResult(\n",
    "            original_paths=paths,\n",
    "            synthesized_answer=synthesized,\n",
    "            total_time=total_time,\n",
    "            parallel_speedup=speedup,\n",
    "            total_cost=total_cost,\n",
    "            total_validations=total_validations,\n",
    "            total_regenerations=total_regenerations,\n",
    "            classification=classification\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # DISPLAY: Nicely print the full reasoning and synthesis results\n",
    "    # ------------------------------------------------------------------------\n",
    "    def display_result(self, result: NegotiationResult):\n",
    "        \"\"\"\n",
    "        Display the entire reasoning and synthesis output in a readable format,\n",
    "        including reasoning paths, final answer, and performance statistics.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"FINAL RESULT\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        # --- Question classification overview ---\n",
    "        if result.classification:\n",
    "            print(f\"\\nQuestion Classification:\")\n",
    "            print(f\"  Type: {result.classification.question_type.value}\")\n",
    "            print(f\"  Complexity: {result.classification.complexity_level.value}\")\n",
    "\n",
    "        # --- Synthesized answer summary ---\n",
    "        print(\"\\n\" + \"▶\"*35)\n",
    "        print(\"SYNTHESIZED ANSWER\")\n",
    "        print(\"▶\"*35)\n",
    "        print(f\"\\nQuery: {result.synthesized_answer.query}\")\n",
    "        print(f\"\\nAnswer: {result.synthesized_answer.definitive_answer}\")\n",
    "        print(f\"Confidence: {result.synthesized_answer.final_confidence:.2f}\")\n",
    "\n",
    "        # --- Supporting and conflicting reasoning ---\n",
    "        if result.synthesized_answer.supporting_reasoning:\n",
    "            print(\"\\nKey Supporting Points:\")\n",
    "            for i, point in enumerate(result.synthesized_answer.supporting_reasoning, 1):\n",
    "                print(f\"  {i}. {point}\")\n",
    "\n",
    "        if result.synthesized_answer.conflicting_points:\n",
    "            print(\"\\nConflicting Aspects:\")\n",
    "            for conflict in result.synthesized_answer.conflicting_points:\n",
    "                print(f\"  ⚠ {conflict}\")\n",
    "\n",
    "        print(\"\\nSynthesis Process:\")\n",
    "        print(result.synthesized_answer.synthesis_explanation)\n",
    "\n",
    "        # --- Individual reasoning chains ---\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(\"INDIVIDUAL REASONING PATHS\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        for path in result.original_paths:\n",
    "            print(f\"\\n{path.to_readable_chain()}\")\n",
    "\n",
    "        # --- Performance & API usage ---\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(\"PERFORMANCE & COST\")\n",
    "        print(\"-\"*70)\n",
    "        print(f\"Total time: {result.total_time:.2f}s\")\n",
    "        print(f\"Parallel speedup: {result.parallel_speedup:.2f}x\")\n",
    "        print(f\"Total API cost: ${result.total_cost:.4f}\")\n",
    "\n",
    "        # --- Validation statistics ---\n",
    "        if result.total_validations > 0:\n",
    "            print(f\"\\nValidation Statistics:\")\n",
    "            print(f\"  Validation passes: {result.total_validations}\")\n",
    "            print(f\"  Regenerations: {result.total_regenerations}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33d2109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_root_.cache_huggingface_datasets_commonsense_qa_default_0.0.0_94630fe30dad47192a8546eb75f094926d47e155.lock', '_root_.cache_huggingface_datasets_gsm8k_main_0.0.0_e53f048856ff4f594e959d75785d2c2d37b678ee.lock', '_root_.cache_huggingface_datasets_nlile___hendrycks-math-benchmark_default_0.0.0_465bcdb36f5962aa3512891498966df785fc3c18.lock', '_root_.cache_huggingface_datasets_tau___commonsense_qa_default_0.0.0_94630fe30dad47192a8546eb75f094926d47e155.lock', 'commonsense_qa', 'gsm8k', 'nlile___hendrycks-math-benchmark', 'tau___commonsense_qa']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "ds = load_dataset(\"nlile/hendrycks-MATH-benchmark\")\n",
    "\n",
    "datasets_dir = Path.home() / \".cache\" / \"huggingface\" / \"datasets\"\n",
    "print(sorted(os.listdir(datasets_dir)))\n",
    "\n",
    "#['_root_.cache_huggingface_datasets_commonsense_qa_default_0.0.0_94630fe30dad47192a8546eb75f094926d47e155.lock', '_root_.cache_huggingface_datasets_gsm8k_main_0.0.0_e53f048856ff4f594e959d75785d2c2d37b678ee.lock', '_root_.cache_huggingface_datasets_nlile___hendrycks-math-benchmark_default_0.0.0_465bcdb36f5962aa3512891498966df785fc3c18.lock', '_root_.cache_huggingface_datasets_tau___commonsense_qa_default_0.0.0_94630fe30dad47192a8546eb75f094926d47e155.lock', 'commonsense_qa', 'gsm8k', 'nlile___hendrycks-math-benchmark', 'tau___commonsense_qa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02de8066",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Claude Dynamic Reasoning System...\n",
      "System ready - Claude 3.5 Sonnet with ALL 5 FIXES\n",
      "\n",
      "Test Harness Initialized\n",
      "  Model: claude-3-5-haiku-20241022\n",
      "  Max Tokens: 800\n",
      "\n",
      "Applying system optimizations...\n",
      "  ✓ Model: claude-3-5-haiku-20241022\n",
      "  ✓ Token limit: 800\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RUNNING: COMMONSENSE_QA\n",
      "================================================================================\n",
      "\n",
      "Loading CommonsenseQA...\n",
      "Dataset: CommonsenseQA: Multiple choice commonsense reasoning\n",
      "Total questions available: 1221\n",
      "Will answer up to: 10\n",
      "\n",
      "\n",
      "================================================================================\n",
      "QUESTION #1/10\n",
      "================================================================================\n",
      "Q: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\n",
      "\n",
      "A: bank\n",
      "B: library\n",
      "C: department store\n",
      "D: mall\n",
      "E: new york\n",
      "\n",
      "Please select ONLY ONE answer ...\n",
      "Ground Truth: A\n",
      "\n",
      "======================================================================\n",
      "QUERY: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\n",
      "\n",
      "A: bank\n",
      "B: library\n",
      "C: department store\n",
      "D: mall\n",
      "E: new york\n",
      "\n",
      "Please select ONLY ONE answer (A-E).\n",
      "======================================================================\n",
      "\n",
      "[0] Classifying question...\n",
      "[CLASSIFIER] Extracted question: ...\n",
      "[CLASSIFIER] DEFAULT → COMMONSENSE\n",
      "[CLASSIFIER] COMMONSENSE → Forcing 3 paths for consensus\n",
      "[CLASSIFIER] → Type: commonsense, Complexity: moderate, Paths: 3\n",
      "Type: commonsense, Complexity: moderate\n",
      "Validation: False, Num paths: 3\n",
      "\n",
      "[1] Generating reasoning paths...\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let me solve this systematically:\n",
      "\n",
      "Step 1: Parse the question\n",
      "- A revolving door provides controlled entry/exit\n",
      "- It's specifically noted as a SECURITY MEASURE\n",
      "- So we're looking for a location where controlled access is critical\n",
      "\n",
      "Step 2: Evaluate options\n",
      "A: Bank ✓ - High security requirement\n",
      "   - C\n",
      "[DEBUG ANSWER EXTRACT] ✗ No answer found\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: skeptical\n",
      "[DEBUG STEP EXTRACT] Found REASONING section\n",
      "[DEBUG STEP EXTRACT] Total steps found: 0\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let's solve this systematically:\n",
      "\n",
      "ANALYSIS:\n",
      "Step 1: Context of Revolving Door as Security Measure\n",
      "- Revolving doors control entry/exit\n",
      "- Implies need for controlled, secure access\n",
      "- Suggests environment requiring strict entry monitoring\n",
      "\n",
      "Step 2: Specificity Test\n",
      "- Bank (A): HIGH SECURITY environment\n",
      "[DEBUG ANSWER EXTRACT] ✗ No answer found\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: evidence_based\n",
      "[DEBUG STEP EXTRACT] Found ANALYSIS section\n",
      "[DEBUG STEP EXTRACT] ✓ Simple Step: Context of Revolving Door as Security Measure...\n",
      "[DEBUG STEP EXTRACT] ✓ Simple Step: Most Specific Answer...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - BANK stands out as the MOST PRECISE security con...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Monitors/limits access to sensitive areas...\n",
      "[DEBUG STEP EXTRACT] Total steps found: 4\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let's analyze this systematically:\n",
      "\n",
      "Step 1: Question Focus\n",
      "- The question is about a revolving door's security function\n",
      "- We need to identify the most logical place where security is critical\n",
      "\n",
      "Step 2: Option Evaluation\n",
      "A: Bank ✓ \n",
      "- High-security environment\n",
      "- Needs controlled entry/exit\n",
      "- Protects c\n",
      "[DEBUG ANSWER EXTRACT] ✗ No answer found\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: analytical\n",
      "[DEBUG STEP EXTRACT] Found REASONING section\n",
      "[DEBUG STEP EXTRACT] Total steps found: 0\n",
      "Generated 1 path(s) (1.00x speedup):\n",
      "  • evidence_based: 4 steps, no answer, conf=0.73\n",
      "\n",
      "[2] Synthesizing final answer...\n",
      "\n",
      "[1.6] Checking for numerical divergence...\n",
      "\n",
      "[SYNTHESIS] Using legacy synthesis\n",
      "\n",
      "[3] Complete in 6.56s\n",
      "Answer: Unable to determine\n",
      "Confidence: 0.73\n",
      "API Cost: $0.0139\n",
      "\n",
      "Model Answer: Unable to determine\n",
      "Correct: False\n",
      "Confidence: 0.73\n",
      "Time: 6.56s\n",
      "\n",
      "================================================================================\n",
      "QUESTION #2/10\n",
      "================================================================================\n",
      "Q: What do people aim to do at work?\n",
      "\n",
      "A: complete job\n",
      "B: learn from each other\n",
      "C: kill animals\n",
      "D: wear hats\n",
      "E: talk to each other\n",
      "\n",
      "Please select ONLY ONE answer (A-E)....\n",
      "Ground Truth: A\n",
      "\n",
      "======================================================================\n",
      "QUERY: What do people aim to do at work?\n",
      "\n",
      "A: complete job\n",
      "B: learn from each other\n",
      "C: kill animals\n",
      "D: wear hats\n",
      "E: talk to each other\n",
      "\n",
      "Please select ONLY ONE answer (A-E).\n",
      "======================================================================\n",
      "\n",
      "[0] Classifying question...\n",
      "[CLASSIFIER] Extracted question: ...\n",
      "[CLASSIFIER] DEFAULT → COMMONSENSE\n",
      "[CLASSIFIER] COMMONSENSE → Forcing 3 paths for consensus\n",
      "[CLASSIFIER] → Type: commonsense, Complexity: moderate, Paths: 3\n",
      "Type: commonsense, Complexity: moderate\n",
      "Validation: False, Num paths: 3\n",
      "\n",
      "[1] Generating reasoning paths...\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let's solve this step by step using everyday commonsense reasoning:\n",
      "\n",
      "Step 1: What is the question REALLY asking?\n",
      "- The question is about the primary aim or purpose of people's actions at work\n",
      "- Focus on the most fundamental, practical goal people have when working\n",
      "\n",
      "Step 2: Evaluate each option\n",
      "A: co\n",
      "[DEBUG ANSWER EXTRACT] ✗ No answer found\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: analytical\n",
      "[DEBUG STEP EXTRACT] Found REASONING section\n",
      "[DEBUG STEP EXTRACT] ✓ Simple Step: What is the question REALLY asking?...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - The question is about the primary aim or purpose...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Focus on the most fundamental, practical goal pe...\n",
      "[DEBUG STEP EXTRACT] ✓ Simple Step: Evaluate each option...\n",
      "[DEBUG STEP EXTRACT] ✓ Simple Step: Select the MOST SPECIFIC answer...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - \"Complete job\" is the most specific and direct d...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - It captures the core purpose: accomplishing assi...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Other options are either too vague, irrelevant, ...\n",
      "[DEBUG STEP EXTRACT] Total steps found: 8\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let's analyze this systematically:\n",
      "\n",
      "Step 1: Parse the question\n",
      "- The question asks about people's PRIMARY AIM at work\n",
      "\n",
      "Step 2: Evaluate each option\n",
      "A: \"Complete job\" - This is VERY SPECIFIC and DIRECTLY relates to work\n",
      "B: \"Learn from each other\" - Possible, but secondary to main work goal\n",
      "C: \"Kill a\n",
      "[DEBUG ANSWER EXTRACT] ✓ Found via pattern: A: complete job...\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: skeptical\n",
      "[DEBUG STEP EXTRACT] Using full response\n",
      "[DEBUG STEP EXTRACT] ✓ Simple Step: Evaluate each option...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: A: \"Complete job\" - This is VERY SPECIFIC and DIRE...\n",
      "[DEBUG STEP EXTRACT] ✓ Simple Step: Select most specific, sensible answer...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - The core purpose of work is to COMPLETE TASKS/JO...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - People are fundamentally hired to accomplish spe...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: The answer is A because completing assigned work t...\n",
      "[DEBUG STEP EXTRACT] Total steps found: 6\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let's analyze this systematically:\n",
      "\n",
      "Step 1: Context of Work\n",
      "- People go to work to perform specific tasks and responsibilities\n",
      "- The core purpose is typically to complete assigned work/tasks\n",
      "\n",
      "Step 2: Specificity Test\n",
      "A: \"complete job\" ✓ MOST SPECIFIC\n",
      "- Directly describes the primary workplace object\n",
      "[DEBUG ANSWER EXTRACT] ✗ No answer found\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: evidence_based\n",
      "[DEBUG STEP EXTRACT] Found REASONING section\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Other options are either auxiliary activities or...\n",
      "[DEBUG STEP EXTRACT] Total steps found: 1\n",
      "Generated 3 path(s) (2.86x speedup):\n",
      "  • analytical: 8 steps, no answer, conf=0.72\n",
      "  • skeptical: 6 steps, answer=A: complete job..., conf=0.82\n",
      "  • evidence_based: 1 steps, no answer, conf=0.70\n",
      "\n",
      "[2] Synthesizing final answer...\n",
      "\n",
      "[1.6] Checking for numerical divergence...\n",
      "\n",
      "[SYNTHESIS] Using legacy synthesis\n",
      "\n",
      "[3] Complete in 5.80s\n",
      "Answer: A: complete job (unanimous)\n",
      "Confidence: 0.75\n",
      "API Cost: $0.0273\n",
      "\n",
      "Model Answer: A: complete job (unanimous)\n",
      "Correct: True\n",
      "Confidence: 0.75\n",
      "Time: 5.80s\n",
      "\n",
      "================================================================================\n",
      "QUESTION #3/10\n",
      "================================================================================\n",
      "Q: Where would you find magazines along side many other printed works?\n",
      "\n",
      "A: doctor\n",
      "B: bookstore\n",
      "C: market\n",
      "D: train station\n",
      "E: mortuary\n",
      "\n",
      "Please select ONLY ONE answer (A-E)....\n",
      "Ground Truth: B\n",
      "\n",
      "======================================================================\n",
      "QUERY: Where would you find magazines along side many other printed works?\n",
      "\n",
      "A: doctor\n",
      "B: bookstore\n",
      "C: market\n",
      "D: train station\n",
      "E: mortuary\n",
      "\n",
      "Please select ONLY ONE answer (A-E).\n",
      "======================================================================\n",
      "\n",
      "[0] Classifying question...\n",
      "[CLASSIFIER] Extracted question: ...\n",
      "[CLASSIFIER] DEFAULT → COMMONSENSE\n",
      "[CLASSIFIER] COMMONSENSE → Forcing 3 paths for consensus\n",
      "[CLASSIFIER] → Type: commonsense, Complexity: moderate, Paths: 3\n",
      "Type: commonsense, Complexity: moderate\n",
      "Validation: False, Num paths: 3\n",
      "\n",
      "[1] Generating reasoning paths...\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let me solve this systematically:\n",
      "\n",
      "Step 1: Parse the question\n",
      "- Looking for a place where magazines and printed works are commonly found and sold/displayed\n",
      "\n",
      "Step 2: Evaluate options\n",
      "A: Doctor's office - Typically has medical journals, not a broad magazine selection\n",
      "B: Bookstore - SPECIFICALLY design\n",
      "[DEBUG ANSWER EXTRACT] ✓ Found via pattern: B: bookstore...\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: skeptical\n",
      "[DEBUG STEP EXTRACT] Found REASONING section\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Bookstores are dedicated spaces for printed medi...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Matches the question's intent precisely...\n",
      "[DEBUG STEP EXTRACT] Total steps found: 2\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let's solve this systematically:\n",
      "\n",
      "Step 1: Question Analysis\n",
      "- Looking for a place where magazines and printed works are typically found\n",
      "- Need a location that specializes in printed materials\n",
      "\n",
      "Step 2: Option Evaluation\n",
      "A: Doctor's office - Might have some magazines in waiting room, but not a primary\n",
      "[DEBUG ANSWER EXTRACT] ✗ No answer found\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: analytical\n",
      "[DEBUG STEP EXTRACT] Found REASONING section\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: A bookstore is the most logical, concrete, and spe...\n",
      "[DEBUG STEP EXTRACT] Total steps found: 1\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let's solve this systematically:\n",
      "\n",
      "ANALYSIS:\n",
      "Step 1: Context - Finding magazines alongside other printed works\n",
      "- Looking for a place specifically dedicated to printed media\n",
      "- Need a location where reading materials are the primary focus\n",
      "\n",
      "Step 2: Specificity Test\n",
      "A. Doctor's office - Might have some m\n",
      "[DEBUG ANSWER EXTRACT] ✗ No answer found\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: evidence_based\n",
      "[DEBUG STEP EXTRACT] Found ANALYSIS section\n",
      "[DEBUG STEP EXTRACT] ✓ Simple Step: Context - Finding magazines alongside other printe...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Need a location where reading materials are the ...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: A. Doctor's office - Might have some magazines, bu...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: C. Market - Might have some magazines, but not the...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: D. Train station - Could have some magazines, but ...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: E. Mortuary - Extremely unlikely to have magazine ...\n",
      "[DEBUG STEP EXTRACT] ✓ Simple Step: Most Specific Answer...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Bookstore is SPECIFICALLY designed for printed m...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Has wide variety of magazines, newspapers, books...\n",
      "[DEBUG STEP EXTRACT] Total steps found: 9\n",
      "Generated 3 path(s) (2.74x speedup):\n",
      "  • skeptical: 2 steps, answer=B: bookstore..., conf=0.75\n",
      "  • analytical: 1 steps, no answer, conf=0.65\n",
      "  • evidence_based: 9 steps, no answer, conf=0.66\n",
      "\n",
      "[2] Synthesizing final answer...\n",
      "\n",
      "[1.6] Checking for numerical divergence...\n",
      "\n",
      "[SYNTHESIS] Using legacy synthesis\n",
      "\n",
      "[3] Complete in 6.30s\n",
      "Answer: B: bookstore (unanimous)\n",
      "Confidence: 0.69\n",
      "API Cost: $0.0401\n",
      "\n",
      "Model Answer: B: bookstore (unanimous)\n",
      "Correct: True\n",
      "Confidence: 0.69\n",
      "Time: 6.30s\n",
      "\n",
      "================================================================================\n",
      "QUESTION #4/10\n",
      "================================================================================\n",
      "Q: Where are  you likely to find a hamburger?\n",
      "\n",
      "A: fast food restaurant\n",
      "B: pizza\n",
      "C: ground up dead cows\n",
      "D: mouth\n",
      "E: cow carcus\n",
      "\n",
      "Please select ONLY ONE answer (A-E)....\n",
      "Ground Truth: A\n",
      "\n",
      "======================================================================\n",
      "QUERY: Where are  you likely to find a hamburger?\n",
      "\n",
      "A: fast food restaurant\n",
      "B: pizza\n",
      "C: ground up dead cows\n",
      "D: mouth\n",
      "E: cow carcus\n",
      "\n",
      "Please select ONLY ONE answer (A-E).\n",
      "======================================================================\n",
      "\n",
      "[0] Classifying question...\n",
      "[CLASSIFIER] Extracted question: ...\n",
      "[CLASSIFIER] DEFAULT → COMMONSENSE\n",
      "[CLASSIFIER] COMMONSENSE → Forcing 3 paths for consensus\n",
      "[CLASSIFIER] → Type: commonsense, Complexity: moderate, Paths: 3\n",
      "Type: commonsense, Complexity: moderate\n",
      "Validation: False, Num paths: 3\n",
      "\n",
      "[1] Generating reasoning paths...\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let's solve this step-by-step using everyday commonsense reasoning:\n",
      "\n",
      "Step 1: What is the question REALLY asking?\n",
      "- Where are you most likely to find a hamburger in a practical, real-world context?\n",
      "\n",
      "Step 2: Evaluate each option\n",
      "A: fast food restaurant ✓ - Most specific, common place to find hamburger\n",
      "[DEBUG ANSWER EXTRACT] ✗ No answer found\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: analytical\n",
      "[DEBUG STEP EXTRACT] Found REASONING section\n",
      "[DEBUG STEP EXTRACT] ✓ Simple Step: What is the question REALLY asking?...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Where are you most likely to find a hamburger in...\n",
      "[DEBUG STEP EXTRACT] ✓ Simple Step: Evaluate each option...\n",
      "[DEBUG STEP EXTRACT] ✓ Simple Step: Select the MOST SPECIFIC answer...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - \"Fast food restaurant\" is the most precise, comm...\n",
      "[DEBUG STEP EXTRACT] Total steps found: 5\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let's analyze this systematically:\n",
      "\n",
      "Step 1: Context of \"hamburger\"\n",
      "- A hamburger is a food item people typically consume\n",
      "- We're looking for where it is MOST LIKELY to be found\n",
      "\n",
      "Step 2: Specificity Test\n",
      "A: Fast food restaurant ✓ SPECIFIC location where hamburgers are commonly served\n",
      "B: Pizza (unrela\n",
      "[DEBUG ANSWER EXTRACT] ✓ Found via pattern: A: fast food restaurant...\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: evidence_based\n",
      "[DEBUG STEP EXTRACT] Found REASONING section\n",
      "[DEBUG STEP EXTRACT] Total steps found: 0\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let me carefully analyze this step-by-step:\n",
      "\n",
      "Step 1: Parse the question\n",
      "- Question is asking \"Where are you LIKELY to find a hamburger?\"\n",
      "- This means a TYPICAL location where hamburgers are served/found\n",
      "\n",
      "Step 2: Evaluate specificity of options\n",
      "A: fast food restaurant ✓ SPECIFIC location where hambur\n",
      "[DEBUG ANSWER EXTRACT] ✓ Found via pattern: A: fast food restaurant...\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: skeptical\n",
      "[DEBUG STEP EXTRACT] Found REASONING section\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Directly answers \"where you are likely to find\" ...\n",
      "[DEBUG STEP EXTRACT] Total steps found: 1\n",
      "Generated 3 path(s) (2.91x speedup):\n",
      "  • analytical: 5 steps, no answer, conf=0.73\n",
      "  • evidence_based: 0 steps, answer=A: fast food restaurant..., conf=0.80\n",
      "  • skeptical: 1 steps, answer=A: fast food restaurant..., conf=0.80\n",
      "\n",
      "[2] Synthesizing final answer...\n",
      "\n",
      "[1.6] Checking for numerical divergence...\n",
      "\n",
      "[SYNTHESIS] Using legacy synthesis\n",
      "\n",
      "[3] Complete in 5.70s\n",
      "Answer: A: fast food restaurant (unanimous)\n",
      "Confidence: 0.78\n",
      "API Cost: $0.0535\n",
      "\n",
      "Model Answer: A: fast food restaurant (unanimous)\n",
      "Correct: True\n",
      "Confidence: 0.78\n",
      "Time: 5.70s\n",
      "\n",
      "================================================================================\n",
      "QUESTION #5/10\n",
      "================================================================================\n",
      "Q: James was looking for a good place to buy farmland.  Where might he look?\n",
      "\n",
      "A: midwest\n",
      "B: countryside\n",
      "C: estate\n",
      "D: farming areas\n",
      "E: illinois\n",
      "\n",
      "Please select ONLY ONE answer (A-E)....\n",
      "Ground Truth: A\n",
      "\n",
      "======================================================================\n",
      "QUERY: James was looking for a good place to buy farmland.  Where might he look?\n",
      "\n",
      "A: midwest\n",
      "B: countryside\n",
      "C: estate\n",
      "D: farming areas\n",
      "E: illinois\n",
      "\n",
      "Please select ONLY ONE answer (A-E).\n",
      "======================================================================\n",
      "\n",
      "[0] Classifying question...\n",
      "[CLASSIFIER] Extracted question: ...\n",
      "[CLASSIFIER] DEFAULT → COMMONSENSE\n",
      "[CLASSIFIER] COMMONSENSE → Forcing 3 paths for consensus\n",
      "[CLASSIFIER] → Type: commonsense, Complexity: moderate, Paths: 3\n",
      "Type: commonsense, Complexity: moderate\n",
      "Validation: False, Num paths: 3\n",
      "\n",
      "[1] Generating reasoning paths...\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let's solve this systematically:\n",
      "\n",
      "ANALYSIS:\n",
      "Step 1: Parse the question\n",
      "- James is looking to BUY FARMLAND\n",
      "- We need to identify the BEST LOCATION for purchasing farmland\n",
      "\n",
      "Step 2: Evaluate specificity\n",
      "- A: midwest ✓ VERY specific, known for extensive farmland\n",
      "- B: countryside (generic, could be anywh\n",
      "[DEBUG ANSWER EXTRACT] ✗ No answer found\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: skeptical\n",
      "[DEBUG STEP EXTRACT] Found ANALYSIS section\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - We need to identify the BEST LOCATION for purcha...\n",
      "[DEBUG STEP EXTRACT] ✓ Simple Step: Evaluate specificity...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - E: illinois (specific state, but less comprehens...\n",
      "[DEBUG STEP EXTRACT] ✓ Simple Step: Select most specific answer...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - \"Midwest\" is the most precise, targeted location...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Midwest states like Iowa, Kansas, Nebraska are r...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Historically and economically associated with ag...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Represents a large, agriculturally rich area...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Provides the best context for land purchasing...\n",
      "[DEBUG STEP EXTRACT] Total steps found: 9\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let's analyze this systematically:\n",
      "\n",
      "Step 1: Context\n",
      "- James is looking to buy farmland\n",
      "- This requires a location known for agricultural land and farming\n",
      "\n",
      "Step 2: Specificity Test\n",
      "A: Midwest ✓ MOST SPECIFIC\n",
      "- Known for extensive, high-quality farmland\n",
      "- States like Iowa, Illinois, Indiana have prime\n",
      "[DEBUG ANSWER EXTRACT] ✓ Found via pattern: A: Midwest...\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: evidence_based\n",
      "[DEBUG STEP EXTRACT] Found REASONING section\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: The Midwest is renowned for its extensive, fertile...\n",
      "[DEBUG STEP EXTRACT] Total steps found: 1\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let's solve this step by step:\n",
      "\n",
      "Step 1: The question is about finding a good place to BUY FARMLAND\n",
      "- This requires a location with agricultural potential\n",
      "- Need somewhere with good farming conditions\n",
      "\n",
      "Step 2: Evaluating options:\n",
      "A: Midwest - ✓ HIGHLY SPECIFIC\n",
      "   - Known for extensive, high-quality f\n",
      "[DEBUG ANSWER EXTRACT] ✗ No answer found\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: analytical\n",
      "[DEBUG STEP EXTRACT] Using full response\n",
      "[DEBUG STEP EXTRACT] ✓ Simple Step: The question is about finding a good place to BUY ...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - This requires a location with agricultural poten...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - States like Iowa, Nebraska, Illinois have prime ...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Could mean many different types of rural areas...\n",
      "[DEBUG STEP EXTRACT] ✓ Simple Step: Most Specific Answer...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - \"Midwest\" is the MOST SPECIFIC and PRACTICAL loc...\n",
      "[DEBUG STEP EXTRACT] Total steps found: 6\n",
      "Generated 3 path(s) (2.90x speedup):\n",
      "  • skeptical: 9 steps, no answer, conf=0.71\n",
      "  • evidence_based: 1 steps, answer=A: Midwest..., conf=0.80\n",
      "  • analytical: 6 steps, no answer, conf=0.72\n",
      "\n",
      "[2] Synthesizing final answer...\n",
      "\n",
      "[1.6] Checking for numerical divergence...\n",
      "\n",
      "[SYNTHESIS] Using legacy synthesis\n",
      "\n",
      "[3] Complete in 6.50s\n",
      "Answer: A: Midwest (unanimous)\n",
      "Confidence: 0.74\n",
      "API Cost: $0.0676\n",
      "\n",
      "Model Answer: A: Midwest (unanimous)\n",
      "Correct: True\n",
      "Confidence: 0.74\n",
      "Time: 6.50s\n",
      "\n",
      "================================================================================\n",
      "QUESTION #6/10\n",
      "================================================================================\n",
      "Q: What island country is ferret popular?\n",
      "\n",
      "A: own home\n",
      "B: north carolina\n",
      "C: great britain\n",
      "D: hutch\n",
      "E: outdoors\n",
      "\n",
      "Please select ONLY ONE answer (A-E)....\n",
      "Ground Truth: C\n",
      "\n",
      "======================================================================\n",
      "QUERY: What island country is ferret popular?\n",
      "\n",
      "A: own home\n",
      "B: north carolina\n",
      "C: great britain\n",
      "D: hutch\n",
      "E: outdoors\n",
      "\n",
      "Please select ONLY ONE answer (A-E).\n",
      "======================================================================\n",
      "\n",
      "[0] Classifying question...\n",
      "[CLASSIFIER] Extracted question: ...\n",
      "[CLASSIFIER] DEFAULT → COMMONSENSE\n",
      "[CLASSIFIER] COMMONSENSE → Forcing 3 paths for consensus\n",
      "[CLASSIFIER] → Type: commonsense, Complexity: moderate, Paths: 3\n",
      "Type: commonsense, Complexity: moderate\n",
      "Validation: False, Num paths: 3\n",
      "\n",
      "[1] Generating reasoning paths...\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let me carefully analyze this:\n",
      "\n",
      "Step 1: Question means \"In which island country are ferrets popular as pets or commonly kept?\"\n",
      "\n",
      "Step 2: Evaluating options:\n",
      "- A: \"own home\" - Not a country\n",
      "- B: North Carolina - A state, not an island country\n",
      "- C: Great Britain - An island country ✓\n",
      "- D: \"hutch\" - Not\n",
      "[DEBUG ANSWER EXTRACT] ✗ No answer found\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: skeptical\n",
      "[DEBUG STEP EXTRACT] Found REASONING section\n",
      "[DEBUG STEP EXTRACT] Total steps found: 0\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let's solve this systematically:\n",
      "\n",
      "Step 1: The question is asking about an island country where ferrets are popular as pets/companions.\n",
      "\n",
      "Step 2: Evaluating options:\n",
      "A: Own home - Not an island country\n",
      "B: North Carolina - A state, not an island country\n",
      "C: Great Britain - An island country ✓\n",
      "D: Hutch -\n",
      "[DEBUG ANSWER EXTRACT] ✗ No answer found\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: analytical\n",
      "[DEBUG STEP EXTRACT] Found REASONING section\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: Great Britain is an island country with a document...\n",
      "[DEBUG STEP EXTRACT] Total steps found: 1\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let me carefully analyze this:\n",
      "\n",
      "ANALYSIS:\n",
      "Step 1: Context is about ferret popularity in an island country\n",
      "- Need to identify where ferrets are most commonly kept/owned\n",
      "\n",
      "Step 2: Examining options:\n",
      "A: \"own home\" - Very generic, not a country\n",
      "B: North Carolina - A state, not an island country\n",
      "C: Great \n",
      "[DEBUG ANSWER EXTRACT] ✗ No answer found\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: evidence_based\n",
      "[DEBUG STEP EXTRACT] Found ANALYSIS section\n",
      "[DEBUG STEP EXTRACT] ✓ Simple Step: Context is about ferret popularity in an island co...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Need to identify where ferrets are most commonly...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: B: North Carolina - A state, not an island country...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Great Britain is a clear island country...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Ferrets have historical popularity in UK culture...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Ferret ownership is relatively common in Britain...\n",
      "[DEBUG STEP EXTRACT] Total steps found: 6\n",
      "Generated 2 path(s) (1.92x speedup):\n",
      "  • analytical: 1 steps, no answer, conf=0.70\n",
      "  • evidence_based: 6 steps, no answer, conf=0.71\n",
      "\n",
      "[2] Synthesizing final answer...\n",
      "\n",
      "[1.6] Checking for numerical divergence...\n",
      "\n",
      "[SYNTHESIS] Using legacy synthesis\n",
      "\n",
      "[3] Complete in 5.60s\n",
      "Answer: Unable to determine\n",
      "Confidence: 0.70\n",
      "API Cost: $0.0794\n",
      "\n",
      "Model Answer: Unable to determine\n",
      "Correct: False\n",
      "Confidence: 0.70\n",
      "Time: 5.60s\n",
      "\n",
      "================================================================================\n",
      "QUESTION #7/10\n",
      "================================================================================\n",
      "Q: In what Spanish speaking North American country can you get a great cup of coffee?\n",
      "\n",
      "A: mildred's coffee shop\n",
      "B: mexico\n",
      "C: diner\n",
      "D: kitchen\n",
      "E: canteen\n",
      "\n",
      "Please select ONLY ONE answer (A-E)....\n",
      "Ground Truth: B\n",
      "\n",
      "======================================================================\n",
      "QUERY: In what Spanish speaking North American country can you get a great cup of coffee?\n",
      "\n",
      "A: mildred's coffee shop\n",
      "B: mexico\n",
      "C: diner\n",
      "D: kitchen\n",
      "E: canteen\n",
      "\n",
      "Please select ONLY ONE answer (A-E).\n",
      "======================================================================\n",
      "\n",
      "[0] Classifying question...\n",
      "[CLASSIFIER] Extracted question: ...\n",
      "[CLASSIFIER] DEFAULT → COMMONSENSE\n",
      "[CLASSIFIER] COMMONSENSE → Forcing 3 paths for consensus\n",
      "[CLASSIFIER] → Type: commonsense, Complexity: moderate, Paths: 3\n",
      "Type: commonsense, Complexity: moderate\n",
      "Validation: False, Num paths: 3\n",
      "\n",
      "[1] Generating reasoning paths...\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let me solve this systematically:\n",
      "\n",
      "Step 1: Parse the question\n",
      "- Looking for a Spanish-speaking North American country known for good coffee\n",
      "\n",
      "Step 2: Evaluate options\n",
      "A: \"Mildred's coffee shop\" - Too specific, not a country\n",
      "B: Mexico - A Spanish-speaking North American country\n",
      "C: \"Diner\" - Not a coun\n",
      "[DEBUG ANSWER EXTRACT] ✓ Found via pattern: B: Mexico...\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: skeptical\n",
      "[DEBUG STEP EXTRACT] Found REASONING section\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Meets the \"Spanish-speaking\" requirement ✓...\n",
      "[DEBUG STEP EXTRACT] Total steps found: 1\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let's solve this systematically:\n",
      "\n",
      "Step 1: Context Analysis\n",
      "- The question asks about a Spanish-speaking North American country with great coffee\n",
      "- Looking for a specific, practical coffee destination\n",
      "\n",
      "Step 2: Option Specificity Test\n",
      "A: Mildred's coffee shop - Too specific, single location\n",
      "B: Mexico \n",
      "[DEBUG ANSWER EXTRACT] ✗ No answer found\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: evidence_based\n",
      "[DEBUG STEP EXTRACT] Found REASONING section\n",
      "[DEBUG STEP EXTRACT] Total steps found: 0\n",
      "\n",
      "[DEBUG ANSWER EXTRACT] Question type: QuestionType.COMMONSENSE\n",
      "[DEBUG ANSWER EXTRACT] Response preview: Let's solve this systematically:\n",
      "\n",
      "ANALYSIS:\n",
      "Step 1: The question asks about a Spanish-speaking North American country known for good coffee.\n",
      "\n",
      "Step 2: Evaluating options:\n",
      "A: Mildred's coffee shop - Too specific, not a country\n",
      "B: Mexico - A Spanish-speaking North American country\n",
      "C: Diner - Generic lo\n",
      "[DEBUG ANSWER EXTRACT] ✗ No answer found\n",
      "\n",
      "[DEBUG STEP EXTRACT] Strategy: analytical\n",
      "[DEBUG STEP EXTRACT] Found ANALYSIS section\n",
      "[DEBUG STEP EXTRACT] ✓ Simple Step: The question asks about a Spanish-speaking North A...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: B: Mexico - A Spanish-speaking North American coun...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Mexico is the ONLY Spanish-speaking North Americ...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Mexico is well-known for producing high-quality ...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Mexico is geographically in North America...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Mexico has a robust coffee-growing culture, espe...\n",
      "[DEBUG STEP EXTRACT] ✓ Freeform: - Mexican coffee is internationally recognized for...\n",
      "[DEBUG STEP EXTRACT] Total steps found: 7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 557\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# MAIN EXECUTION\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;66;03m# Option 1: Interactive mode\u001b[39;00m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m#results = run_test_harness_interactive()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    555\u001b[0m \n\u001b[1;32m    556\u001b[0m     \u001b[38;5;66;03m# CommonsenseQA dataset - 10 questions\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_test_programmatic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;66;03m# Math dataset - 10 questions\u001b[39;00m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;66;03m#results = run_test_programmatic(dataset=2, questions=10)\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \n\u001b[1;32m    562\u001b[0m     \u001b[38;5;66;03m# All datasets - 10 questions each\u001b[39;00m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;66;03m#results = run_test_programmatic(dataset=4, questions=10)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 538\u001b[0m, in \u001b[0;36mrun_test_programmatic\u001b[0;34m(dataset, questions)\u001b[0m\n\u001b[1;32m    530\u001b[0m system \u001b[38;5;241m=\u001b[39m ClaudeDynamicReasoningSystem(api_key\u001b[38;5;241m=\u001b[39mAPI_KEY)\n\u001b[1;32m    532\u001b[0m harness \u001b[38;5;241m=\u001b[39m UniversalTestHarness(\n\u001b[1;32m    533\u001b[0m     system\u001b[38;5;241m=\u001b[39msystem,\n\u001b[1;32m    534\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaude-3-5-haiku-20241022\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    535\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m\n\u001b[1;32m    536\u001b[0m )\n\u001b[0;32m--> 538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mharness\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_questions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\n\u001b[1;32m    542\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 106\u001b[0m, in \u001b[0;36mUniversalTestHarness.run_test\u001b[0;34m(self, dataset_choice, num_questions, save_interval)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRUNNING: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_type\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_single_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_questions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_questions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_interval\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     all_results[dataset_type\u001b[38;5;241m.\u001b[39mvalue] \u001b[38;5;241m=\u001b[39m results\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_results\n",
      "Cell \u001b[0;32mIn[12], line 279\u001b[0m, in \u001b[0;36mUniversalTestHarness._run_single_dataset\u001b[0;34m(self, dataset_type, num_questions, save_interval)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGround Truth: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# Run reasoning\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreason_with_synthesis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     answer \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39msynthesized_answer\u001b[38;5;241m.\u001b[39mdefinitive_answer\n\u001b[1;32m    282\u001b[0m     confidence \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39msynthesized_answer\u001b[38;5;241m.\u001b[39mfinal_confidence\n",
      "Cell \u001b[0;32mIn[10], line 49\u001b[0m, in \u001b[0;36mClaudeDynamicReasoningSystem.reason_with_synthesis\u001b[0;34m(self, query, num_paths)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# --- STEP 1: Generate reasoning paths ---\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[1] Generating reasoning paths...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m paths, speedup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_multiple_paths_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassification\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(paths)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m path(s) (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspeedup\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx speedup):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# Summarize each reasoning path\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 76\u001b[0m, in \u001b[0;36mClaudeReasoningGenerator.generate_multiple_paths_parallel\u001b[0;34m(self, query, num_paths, classification)\u001b[0m\n\u001b[1;32m     68\u001b[0m future_to_strategy \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     69\u001b[0m     executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_path, query, strategy_name, instruction, classification\n\u001b[1;32m     71\u001b[0m     ): strategy_name\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m strategy_name, instruction \u001b[38;5;129;01min\u001b[39;00m strategies\n\u001b[1;32m     73\u001b[0m }\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Collect completed results as they finish\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m as_completed(future_to_strategy):\n\u001b[1;32m     77\u001b[0m     strategy \u001b[38;5;241m=\u001b[39m future_to_strategy[future]\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:245\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wait_timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    243\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> 245\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n\u001b[1;32m    248\u001b[0m     finished \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39mfinished_futures\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# UNIVERSAL TEST HARNESS CLASS\n",
    "# ============================================================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "from enum import Enum\n",
    "\n",
    "class DatasetType(Enum):\n",
    "    \"\"\"Supported dataset types\"\"\"\n",
    "    GSM8K = \"gsm8k\"\n",
    "    COMMONSENSE_QA = \"commonsense_qa\"\n",
    "    MATH_COMPETITION = \"math_competition\"\n",
    "\n",
    "class UniversalTestHarness:\n",
    "    \"\"\"\n",
    "    Unified test harness for multiple reasoning datasets.\n",
    "    \n",
    "    Features:\n",
    "    - Single interface for all datasets\n",
    "    - Configurable parameters (model, tokens, questions)\n",
    "    - Consistent output format\n",
    "    - No cost tracking (removed)\n",
    "    - Progress tracking and intermediate saves\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, system: 'ClaudeDynamicReasoningSystem', \n",
    "                 model: str = \"claude-3-5-haiku-20241022\",\n",
    "                 max_tokens: int = 800):\n",
    "        \"\"\"\n",
    "        Initialize test harness.\n",
    "        \n",
    "        Args:\n",
    "            system: Your reasoning system instance\n",
    "            model: Claude model to use\n",
    "            max_tokens: Token limit per request\n",
    "        \"\"\"\n",
    "        self.system = system\n",
    "        self.model = model\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "        # Dataset loaders\n",
    "        self.dataset_loaders = {\n",
    "            DatasetType.GSM8K: self._load_gsm8k,\n",
    "            DatasetType.COMMONSENSE_QA: self._load_commonsense_qa,\n",
    "            DatasetType.MATH_COMPETITION: self._load_math_competition\n",
    "        }\n",
    "        \n",
    "        # Answer comparators\n",
    "        self.answer_comparators = {\n",
    "            DatasetType.GSM8K: self._compare_numerical,\n",
    "            DatasetType.COMMONSENSE_QA: self._compare_letter_choice,\n",
    "            DatasetType.MATH_COMPETITION: self._compare_math_expression\n",
    "        }\n",
    "        \n",
    "        print(f\"Test Harness Initialized\")\n",
    "        print(f\"  Model: {model}\")\n",
    "        print(f\"  Max Tokens: {max_tokens}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # MAIN RUN METHOD\n",
    "    # ========================================================================\n",
    "    \n",
    "    def run_test(self, \n",
    "                 dataset_choice: int,\n",
    "                 num_questions: int = 100,\n",
    "                 save_interval: int = 25) -> Dict:\n",
    "        \"\"\"\n",
    "        Run test on selected dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset_choice: 1=GSM8K, 2=CommonsenseQA, 3=MATH, 4=All\n",
    "            num_questions: Number of questions to test\n",
    "            save_interval: Save results every N questions\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with results and metadata\n",
    "        \"\"\"\n",
    "        \n",
    "        # Map choice to datasets\n",
    "        dataset_map = {\n",
    "            1: [DatasetType.GSM8K],\n",
    "            2: [DatasetType.COMMONSENSE_QA],\n",
    "            3: [DatasetType.MATH_COMPETITION],\n",
    "            4: [DatasetType.GSM8K, DatasetType.COMMONSENSE_QA, DatasetType.MATH_COMPETITION]\n",
    "        }\n",
    "        \n",
    "        datasets_to_run = dataset_map.get(dataset_choice)\n",
    "        if not datasets_to_run:\n",
    "            raise ValueError(f\"Invalid dataset choice: {dataset_choice}\")\n",
    "        \n",
    "        # Apply system optimizations\n",
    "        self._optimize_system()\n",
    "        \n",
    "        # Run each dataset\n",
    "        all_results = {}\n",
    "        for dataset_type in datasets_to_run:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"RUNNING: {dataset_type.value.upper()}\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "            \n",
    "            results = self._run_single_dataset(\n",
    "                dataset_type=dataset_type,\n",
    "                num_questions=num_questions,\n",
    "                save_interval=save_interval\n",
    "            )\n",
    "            \n",
    "            all_results[dataset_type.value] = results\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    # ========================================================================\n",
    "    # DATASET LOADERS\n",
    "    # ========================================================================\n",
    "    \n",
    "    def _load_gsm8k(self) -> Tuple[List[Dict], str]:\n",
    "        \"\"\"Load GSM8K dataset\"\"\"\n",
    "        print(\"Loading GSM8K (Grade School Math)...\")\n",
    "        dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "        questions = dataset['test']\n",
    "        \n",
    "        processed = []\n",
    "        for idx, item in enumerate(questions):\n",
    "            processed.append({\n",
    "                'idx': idx,\n",
    "                'question': item['question'],\n",
    "                'ground_truth': item['answer'].split('####')[-1].strip(),\n",
    "                'full_answer': item['answer']\n",
    "            })\n",
    "        \n",
    "        return processed, \"GSM8K: Grade school math word problems\"\n",
    "    \n",
    "    def _load_commonsense_qa(self) -> Tuple[List[Dict], str]:\n",
    "        \"\"\"Load CommonsenseQA dataset\"\"\"\n",
    "        print(\"Loading CommonsenseQA...\")\n",
    "        dataset = load_dataset(\"tau/commonsense_qa\")\n",
    "        questions = dataset['validation']  # Use validation split\n",
    "        \n",
    "        processed = []\n",
    "        for idx, item in enumerate(questions):\n",
    "            # Format question with choices\n",
    "            question_text = item['question']\n",
    "            choices = item['choices']\n",
    "            \n",
    "            formatted_q = f\"{question_text}\\n\\n\"\n",
    "            for i, choice in enumerate(choices['text']):\n",
    "                formatted_q += f\"{choices['label'][i]}: {choice}\\n\"\n",
    "            formatted_q += \"\\nPlease select ONLY ONE answer (A-E).\"\n",
    "            \n",
    "            processed.append({\n",
    "                'idx': idx,\n",
    "                'question': formatted_q,\n",
    "                'ground_truth': item['answerKey'],\n",
    "                'choices': choices\n",
    "            })\n",
    "        \n",
    "        return processed, \"CommonsenseQA: Multiple choice commonsense reasoning\"\n",
    "    \n",
    "    def _load_math_competition(self) -> Tuple[List[Dict], str]:\n",
    "        \"\"\"Load MATH competition dataset\"\"\"\n",
    "        print(\"Loading MATH Competition dataset...\")\n",
    "        dataset = load_dataset(\"nlile/hendrycks-math-benchmark\", \"default\")\n",
    "        questions = dataset['test']\n",
    "        processed = []\n",
    "        for idx, item in enumerate(questions):\n",
    "            processed.append({\n",
    "                'idx': idx,\n",
    "                'question': item['problem'],\n",
    "                'ground_truth': item['solution'],\n",
    "                'level': item.get('level', 'unknown'),\n",
    "                'category': item.get('category', item.get('type', 'unknown'))\n",
    "            })\n",
    "        \n",
    "        return processed, \"MATH: Competition-level mathematics\"\n",
    "    \n",
    "    # ========================================================================\n",
    "    # ANSWER COMPARATORS\n",
    "    # ========================================================================\n",
    "    \n",
    "    def _compare_numerical(self, predicted: str, ground_truth: str) -> bool:\n",
    "        \"\"\"Compare numerical answers (GSM8K)\"\"\"\n",
    "        pred_num = self._extract_number(predicted)\n",
    "        truth_num = self._extract_number(ground_truth)\n",
    "        \n",
    "        if pred_num is None or truth_num is None:\n",
    "            return predicted.strip().lower() == ground_truth.strip().lower()\n",
    "        \n",
    "        if truth_num == 0:\n",
    "            return abs(pred_num) < 0.01\n",
    "        \n",
    "        return abs(pred_num - truth_num) / abs(truth_num) < 0.01\n",
    "    \n",
    "    def _compare_letter_choice(self, predicted: str, ground_truth: str) -> bool:\n",
    "        \"\"\"Compare letter choices (CommonsenseQA)\"\"\"\n",
    "        # Extract letter from predicted (handles \"A: text\" format)\n",
    "        pred_match = re.match(r'^([A-E])', predicted.strip())\n",
    "        pred_letter = pred_match.group(1) if pred_match else predicted.strip()\n",
    "        \n",
    "        return pred_letter.upper() == ground_truth.upper()\n",
    "    \n",
    "    def _compare_math_expression(self, predicted: str, ground_truth: str) -> bool:\n",
    "        \"\"\"Compare mathematical expressions (MATH)\"\"\"\n",
    "        # Try numerical comparison first\n",
    "        if self._compare_numerical(predicted, ground_truth):\n",
    "            return True\n",
    "        \n",
    "        # Try exact string match\n",
    "        clean_pred = predicted.strip().lower().replace(' ', '')\n",
    "        clean_truth = ground_truth.strip().lower().replace(' ', '')\n",
    "        \n",
    "        return clean_pred == clean_truth\n",
    "    \n",
    "    def _extract_number(self, text: str) -> Optional[float]:\n",
    "        \"\"\"Extract numerical value from text\"\"\"\n",
    "        if not text:\n",
    "            return None\n",
    "        \n",
    "        text = text.lower().strip()\n",
    "        text = re.sub(r'^(the answer is|answer:|final answer:)\\s*', '', text)\n",
    "        \n",
    "        # Try currency\n",
    "        currency_match = re.search(r'\\$?\\s*([\\d,]+\\.?\\d*)', text)\n",
    "        if currency_match:\n",
    "            try:\n",
    "                return float(currency_match.group(1).replace(',', ''))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Try plain number\n",
    "        num_match = re.search(r'([-+]?\\d*\\.?\\d+)', text)\n",
    "        if num_match:\n",
    "            try:\n",
    "                return float(num_match.group(1))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CORE TEST EXECUTION\n",
    "    # ========================================================================\n",
    "    \n",
    "    def _run_single_dataset(self, \n",
    "                           dataset_type: DatasetType,\n",
    "                           num_questions: int,\n",
    "                           save_interval: int) -> Dict:\n",
    "        \"\"\"Run test on a single dataset\"\"\"\n",
    "        \n",
    "        # Load dataset\n",
    "        loader = self.dataset_loaders[dataset_type]\n",
    "        questions, description = loader()\n",
    "        \n",
    "        print(f\"Dataset: {description}\")\n",
    "        print(f\"Total questions available: {len(questions)}\")\n",
    "        print(f\"Will answer up to: {num_questions}\\n\")\n",
    "        \n",
    "        # Get comparator\n",
    "        comparator = self.answer_comparators[dataset_type]\n",
    "        \n",
    "        # Run questions\n",
    "        results = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, q_data in enumerate(questions[:num_questions]):\n",
    "            q_num = i + 1\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"QUESTION #{q_num}/{num_questions}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"Q: {q_data['question'][:200]}...\")\n",
    "            print(f\"Ground Truth: {q_data['ground_truth']}\")\n",
    "            \n",
    "            try:\n",
    "                # Run reasoning\n",
    "                result = self.system.reason_with_synthesis(q_data['question'])\n",
    "                \n",
    "                answer = result.synthesized_answer.definitive_answer\n",
    "                confidence = result.synthesized_answer.final_confidence\n",
    "                \n",
    "                # Check correctness\n",
    "                is_correct = comparator(answer, q_data['ground_truth'])\n",
    "                \n",
    "                print(f\"\\nModel Answer: {answer}\")\n",
    "                print(f\"Correct: {is_correct}\")\n",
    "                print(f\"Confidence: {confidence:.2f}\")\n",
    "                print(f\"Time: {result.total_time:.2f}s\")\n",
    "                \n",
    "                # Store result\n",
    "                result_entry = {\n",
    "                    'question_num': q_num,\n",
    "                    'dataset_idx': q_data['idx'],\n",
    "                    'question': q_data['question'],\n",
    "                    'ground_truth': q_data['ground_truth'],\n",
    "                    'model_answer': answer,\n",
    "                    'is_correct': is_correct,\n",
    "                    'confidence': confidence,\n",
    "                    'time': result.total_time,\n",
    "                    'paths_generated': len(result.original_paths),\n",
    "                    'validations': result.total_validations,\n",
    "                    'regenerations': result.total_regenerations\n",
    "                }\n",
    "                \n",
    "                # Add dataset-specific fields\n",
    "                if dataset_type == DatasetType.MATH_COMPETITION:\n",
    "                    result_entry['level'] = q_data.get('level')\n",
    "                    result_entry['category'] = q_data.get('category')\n",
    "                \n",
    "                results.append(result_entry)\n",
    "                \n",
    "                # Periodic save\n",
    "                if q_num % save_interval == 0:\n",
    "                    self._save_results(\n",
    "                        dataset_type=dataset_type,\n",
    "                        results=results,\n",
    "                        final=False\n",
    "                    )\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nERROR: {e}\")\n",
    "                print(\"Skipping question...\")\n",
    "                continue\n",
    "        \n",
    "        # Final save\n",
    "        total_time = time.time() - start_time\n",
    "        self._save_results(\n",
    "            dataset_type=dataset_type,\n",
    "            results=results,\n",
    "            final=True,\n",
    "            total_time=total_time\n",
    "        )\n",
    "        \n",
    "        # Calculate stats\n",
    "        stats = self._calculate_statistics(results, total_time)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"COMPLETED: {dataset_type.value.upper()}\")\n",
    "        print(f\"Accuracy: {stats['accuracy']:.1f}% ({stats['correct']}/{stats['total']})\")\n",
    "        print(f\"Time: {stats['total_time']:.2f}s\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        return {\n",
    "            'results': results,\n",
    "            'statistics': stats,\n",
    "            'dataset_type': dataset_type.value\n",
    "        }\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SAVING & STATISTICS\n",
    "    # ========================================================================\n",
    "    \n",
    "    def _save_results(self,\n",
    "                     dataset_type: DatasetType,\n",
    "                     results: List[Dict],\n",
    "                     final: bool = True,\n",
    "                     total_time: Optional[float] = None):\n",
    "        \"\"\"Save results to JSON file\"\"\"\n",
    "        \n",
    "        os.makedirs('Results', exist_ok=True)\n",
    "        \n",
    "        suffix = \"_FINAL\" if final else f\"_{len(results)}q\"\n",
    "        filename = f\"{dataset_type.value}_results{suffix}.json\"\n",
    "        output_path = os.path.join('Results', filename)\n",
    "        \n",
    "        stats = self._calculate_statistics(results, total_time)\n",
    "        \n",
    "        output = {\n",
    "            'metadata': {\n",
    "                'dataset': dataset_type.value,\n",
    "                'model': self.model,\n",
    "                'max_tokens': self.max_tokens,\n",
    "                'questions_answered': len(results),\n",
    "                'accuracy': stats['accuracy'],\n",
    "                'correct_count': stats['correct'],\n",
    "                'total_time_seconds': total_time,\n",
    "                'avg_time_per_question': stats['avg_time'],\n",
    "                'total_validations': stats['total_validations'],\n",
    "                'total_regenerations': stats['total_regenerations'],\n",
    "                'completed_at': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            },\n",
    "            'results': results\n",
    "        }\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(output, f, indent=2)\n",
    "        \n",
    "        status = \"FINAL\" if final else \"INTERMEDIATE\"\n",
    "        print(f\"\\n{status} RESULTS SAVED: {output_path}\")\n",
    "        print(f\"  Questions: {len(results)}\")\n",
    "        print(f\"  Accuracy: {stats['accuracy']:.1f}%\")\n",
    "    \n",
    "    def _calculate_statistics(self, results: List[Dict], \n",
    "                             total_time: Optional[float] = None) -> Dict:\n",
    "        \"\"\"Calculate statistics from results\"\"\"\n",
    "        if not results:\n",
    "            return {\n",
    "                'total': 0,\n",
    "                'correct': 0,\n",
    "                'accuracy': 0.0,\n",
    "                'avg_time': 0.0,\n",
    "                'total_validations': 0,\n",
    "                'total_regenerations': 0,\n",
    "                'total_time': 0.0\n",
    "            }\n",
    "        \n",
    "        correct = sum(1 for r in results if r['is_correct'])\n",
    "        accuracy = (correct / len(results)) * 100\n",
    "        \n",
    "        avg_time = (sum(r['time'] for r in results) / len(results)) if results else 0\n",
    "        total_validations = sum(r['validations'] for r in results)\n",
    "        total_regenerations = sum(r['regenerations'] for r in results)\n",
    "        \n",
    "        return {\n",
    "            'total': len(results),\n",
    "            'correct': correct,\n",
    "            'accuracy': accuracy,\n",
    "            'avg_time': avg_time,\n",
    "            'total_validations': total_validations,\n",
    "            'total_regenerations': total_regenerations,\n",
    "            'total_time': total_time or sum(r['time'] for r in results)\n",
    "        }\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SYSTEM OPTIMIZATION\n",
    "    # ========================================================================\n",
    "    \n",
    "    def _optimize_system(self):\n",
    "        \"\"\"Apply optimizations to the reasoning system\"\"\"\n",
    "        print(\"\\nApplying system optimizations...\")\n",
    "        \n",
    "        # Set model\n",
    "        self.system.generator.model = self.model\n",
    "        self.system.synthesizer.model = self.model\n",
    "        \n",
    "        # Limit tokens\n",
    "        original_create_gen = self.system.generator.client.messages.create\n",
    "        original_create_synth = self.system.synthesizer.client.messages.create\n",
    "        \n",
    "        def limited_create(original_method):\n",
    "            def wrapper(*args, **kwargs):\n",
    "                kwargs['max_tokens'] = min(kwargs.get('max_tokens', 1500), self.max_tokens)\n",
    "                return original_method(*args, **kwargs)\n",
    "            return wrapper\n",
    "        \n",
    "        self.system.generator.client.messages.create = limited_create(original_create_gen)\n",
    "        self.system.synthesizer.client.messages.create = limited_create(original_create_synth)\n",
    "        \n",
    "        print(f\"  ✓ Model: {self.model}\")\n",
    "        print(f\"  ✓ Token limit: {self.max_tokens}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE INTERFACE\n",
    "# ============================================================================\n",
    "\n",
    "def run_test_harness_interactive():\n",
    "    \"\"\"Interactive test harness runner\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"UNIVERSAL REASONING TEST HARNESS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nSelect Dataset:\")\n",
    "    print(\"  1. GSM8K (Grade School Math)\")\n",
    "    print(\"  2. CommonsenseQA (Multiple Choice)\")\n",
    "    print(\"  3. MATH Competition (Advanced Math)\")\n",
    "    print(\"  4. All Datasets\")\n",
    "    print()\n",
    "    \n",
    "    # Get user input\n",
    "    try:\n",
    "        dataset_choice = int(input(\"Enter choice (1-4): \"))\n",
    "        num_questions = int(input(\"Number of questions to test: \"))\n",
    "        \n",
    "        # Optional: Get model and tokens\n",
    "        use_defaults = input(\"Use default settings? (y/n): \").lower() == 'y'\n",
    "        \n",
    "        if use_defaults:\n",
    "            model = \"claude-3-5-haiku-20241022\"\n",
    "            max_tokens = 1500\n",
    "        else:\n",
    "            model = input(\"Model (default: claude-3-5-haiku-20241022): \") or \"claude-3-5-haiku-20241022\"\n",
    "            max_tokens = int(input(\"Max tokens (default: 800): \") or \"800\")\n",
    "        \n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Using defaults.\")\n",
    "        dataset_choice = 1\n",
    "        num_questions = 10\n",
    "        model = \"claude-3-5-haiku-20241022\"\n",
    "        max_tokens = 800\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CONFIGURATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Dataset: {dataset_choice}\")\n",
    "    print(f\"  Questions: {num_questions}\")\n",
    "    print(f\"  Model: {model}\")\n",
    "    print(f\"  Max Tokens: {max_tokens}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Initialize system\n",
    "    system = ClaudeDynamicReasoningSystem(api_key=API_KEY)\n",
    "    \n",
    "    # Create and run harness\n",
    "    harness = UniversalTestHarness(\n",
    "        system=system,\n",
    "        model=model,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    results = harness.run_test(\n",
    "        dataset_choice=dataset_choice,\n",
    "        num_questions=num_questions,\n",
    "        save_interval=25\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PROGRAMMATIC USAGE\n",
    "# ============================================================================\n",
    "\n",
    "def run_test_programmatic(dataset: int = 1, questions: int = 100):\n",
    "    \"\"\"Programmatic test runner (for scripts)\"\"\"\n",
    "    \n",
    "    system = ClaudeDynamicReasoningSystem(api_key=API_KEY)\n",
    "    \n",
    "    harness = UniversalTestHarness(\n",
    "        system=system,\n",
    "        model=\"claude-3-5-haiku-20241022\",\n",
    "        max_tokens=800\n",
    "    )\n",
    "    \n",
    "    return harness.run_test(\n",
    "        dataset_choice=dataset,\n",
    "        num_questions=questions,\n",
    "        save_interval=25\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Option 1: Interactive mode\n",
    "    #results = run_test_harness_interactive()\n",
    "    \n",
    "    # GSM8K (Grade School Math) dataset - 10 questions each\n",
    "    #results = run_test_programmatic(dataset=1, questions=10)\n",
    "\n",
    "    # CommonsenseQA dataset - 10 questions\n",
    "    results = run_test_programmatic(dataset=2, questions=10)\n",
    "\n",
    "    # Math dataset - 10 questions\n",
    "    #results = run_test_programmatic(dataset=2, questions=10)\n",
    "\n",
    "    # All datasets - 10 questions each\n",
    "    #results = run_test_programmatic(dataset=4, questions=10)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
